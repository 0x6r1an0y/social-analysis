#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Embeddings è¦†è“‹ç‡åˆ†æå·¥å…·
"""

from sqlalchemy import create_engine, text
import pandas as pd
import json
import os
import logging

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_embedding_coverage(db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash"):
    """åˆ†æ Embeddings è¦†è“‹ç‡"""
    try:
        engine = create_engine(db_url)
        
        # 1. æª¢æŸ¥ posts_deduplicated è¡¨ç¸½æ•¸
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT COUNT(*) 
                FROM posts_deduplicated
                WHERE content IS NOT NULL AND content != ''
            """))
            total_posts = result.scalar()
            
        # 2. æª¢æŸ¥ embedding_processed è¡¨æ•¸é‡
        with engine.connect() as conn:
            result = conn.execute(text("SELECT COUNT(*) FROM embedding_processed"))
            processed_posts = result.scalar()
            
        # 3. æª¢æŸ¥ embeddings ç´¢å¼•æª”æ¡ˆ
        embeddings_dir = "embeddings_data"
        index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        
        if os.path.exists(index_file):
            with open(index_file, 'r', encoding='utf-8') as f:
                pos_tid_to_index = json.load(f)
            index_count = len(pos_tid_to_index)
        else:
            index_count = 0
            
        # 4. æª¢æŸ¥æ²’æœ‰ embeddings çš„è²¼æ–‡
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT COUNT(*) 
                FROM posts_deduplicated p
                LEFT JOIN embedding_processed ep ON p.pos_tid = ep.pos_tid
                WHERE p.content IS NOT NULL 
                AND p.content != ''
                AND ep.pos_tid IS NULL
            """))
            missing_embeddings = result.scalar()
            
        # 5. æª¢æŸ¥é‡è¤‡çš„ pos_tid
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT pos_tid, COUNT(*) as count
                FROM embedding_processed
                GROUP BY pos_tid
                HAVING COUNT(*) > 1
                LIMIT 10
            """))
            duplicate_processed = result.fetchall()
            
        # 6. æª¢æŸ¥ç´¢å¼•æª”æ¡ˆä¸­ä½†ä¸åœ¨ embedding_processed è¡¨ä¸­çš„è¨˜éŒ„
        if index_count > 0:
            # éš¨æ©Ÿé¸å–ä¸€äº›ç´¢å¼•ä¸­çš„ pos_tid ä¾†æª¢æŸ¥
            sample_pos_tids = list(pos_tid_to_index.keys())[:100]
            placeholders = ','.join([f"'{pid}'" for pid in sample_pos_tids])
            
            with engine.connect() as conn:
                result = conn.execute(text(f"""
                    SELECT COUNT(*) 
                    FROM embedding_processed 
                    WHERE pos_tid IN ({placeholders})
                """))
                sample_in_db = result.scalar()
        else:
            sample_in_db = 0
            
        # 7. æª¢æŸ¥å…§å®¹é•·åº¦åˆ†å¸ƒ
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN LENGTH(content) < 5 THEN 1 END) as very_short,
                    COUNT(CASE WHEN LENGTH(content) >= 5 AND LENGTH(content) < 10 THEN 1 END) as short,
                    COUNT(CASE WHEN LENGTH(content) >= 10 AND LENGTH(content) < 50 THEN 1 END) as medium,
                    COUNT(CASE WHEN LENGTH(content) >= 50 THEN 1 END) as long
                FROM posts_deduplicated 
                WHERE content IS NOT NULL AND content != ''
            """))
            content_stats = result.fetchone()
            
        # 8. æª¢æŸ¥æ²’æœ‰ embeddings çš„è²¼æ–‡å…§å®¹æ¨£æœ¬
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT p.pos_tid, p.content, LENGTH(p.content) as content_length
                FROM posts_deduplicated p
                LEFT JOIN embedding_processed ep ON p.pos_tid = ep.pos_tid
                WHERE p.content IS NOT NULL 
                AND p.content != ''
                AND ep.pos_tid IS NULL
                ORDER BY LENGTH(p.content) DESC
                LIMIT 10
            """))
            missing_samples = result.fetchall()
            
        logger.info("=== Embeddings è¦†è“‹ç‡åˆ†æ ===")
        logger.info(f"posts_deduplicated è¡¨ç¸½æ•¸: {total_posts:,}")
        logger.info(f"embedding_processed è¡¨æ•¸é‡: {processed_posts:,}")
        logger.info(f"embeddings ç´¢å¼•æª”æ¡ˆæ•¸é‡: {index_count:,}")
        logger.info(f"ç¼ºå°‘ embeddings çš„è²¼æ–‡: {missing_embeddings:,}")
        
        if total_posts > 0:
            coverage_rate = (processed_posts / total_posts) * 100
            logger.info(f"è¦†è“‹ç‡: {coverage_rate:.2f}%")
            
            if missing_embeddings > 0:
                missing_rate = (missing_embeddings / total_posts) * 100
                logger.info(f"ç¼ºå¤±ç‡: {missing_rate:.2f}%")
                
        logger.info("\n=== å…§å®¹é•·åº¦åˆ†å¸ƒ ===")
        logger.info(f"æ¥µçŸ­å…§å®¹ (<5 å­—): {content_stats.very_short:,}")
        logger.info(f"çŸ­å…§å®¹ (5-10 å­—): {content_stats.short:,}")
        logger.info(f"ä¸­ç­‰å…§å®¹ (10-50 å­—): {content_stats.medium:,}")
        logger.info(f"é•·å…§å®¹ (>=50 å­—): {content_stats.long:,}")
        
        if duplicate_processed:
            logger.info(f"\n=== ç™¼ç¾ {len(duplicate_processed)} å€‹é‡è¤‡çš„è™•ç†è¨˜éŒ„ ===")
            for pos_tid, count in duplicate_processed:
                logger.info(f"pos_tid: {pos_tid}, é‡è¤‡æ¬¡æ•¸: {count}")
        else:
            logger.info("\nâœ… æ²’æœ‰ç™¼ç¾é‡è¤‡çš„è™•ç†è¨˜éŒ„")
            
        if index_count > 0:
            sample_coverage = (sample_in_db / 100) * 100
            logger.info(f"\nç´¢å¼•æª”æ¡ˆèˆ‡è³‡æ–™åº«åŒæ­¥ç‡ (æ¨£æœ¬): {sample_coverage:.1f}%")
            
        if missing_samples:
            logger.info(f"\n=== ç¼ºå°‘ Embeddings çš„è²¼æ–‡æ¨£æœ¬ (å‰ 10 ç­†) ===")
            for pos_tid, content, length in missing_samples:
                logger.info(f"pos_tid: {pos_tid}, é•·åº¦: {length}, å…§å®¹: {content[:50]}...")
                
        # åˆ†æå¯èƒ½çš„åŸå› 
        logger.info("\n=== å¯èƒ½çš„åŸå› åˆ†æ ===")
        
        if missing_embeddings > 0:
            if missing_embeddings < 1000:
                logger.info("ğŸ” å°‘é‡è²¼æ–‡ç¼ºå°‘ embeddingsï¼Œå¯èƒ½æ˜¯è™•ç†éç¨‹ä¸­çš„å°éŒ¯èª¤")
            elif missing_embeddings < total_posts * 0.1:
                logger.info("ğŸ” ç´„ 10% çš„è²¼æ–‡ç¼ºå°‘ embeddingsï¼Œå¯èƒ½æ˜¯æ‰¹æ¬¡è™•ç†ä¸­çš„å•é¡Œ")
            else:
                logger.info("ğŸ” å¤§é‡è²¼æ–‡ç¼ºå°‘ embeddingsï¼Œå¯èƒ½æ˜¯è™•ç†éç¨‹ä¸­çš„åš´é‡å•é¡Œ")
                
            # æª¢æŸ¥æ˜¯å¦æœ‰ç‰¹å®šæ¨¡å¼çš„è²¼æ–‡æ²’æœ‰ embeddings
            if content_stats.very_short > 0:
                logger.info("ğŸ” ç™¼ç¾æ¥µçŸ­å…§å®¹çš„è²¼æ–‡ï¼Œå¯èƒ½æ˜¯éæ¿¾æ¢ä»¶éæ–¼åš´æ ¼")
                
        if processed_posts != index_count:
            logger.info("ğŸ” embedding_processed è¡¨èˆ‡ç´¢å¼•æª”æ¡ˆæ•¸é‡ä¸ä¸€è‡´")
            logger.info("   é€™å¯èƒ½è¡¨ç¤ºè™•ç†ç‹€æ…‹è¡¨èˆ‡ç´¢å¼•æª”æ¡ˆä¸åŒæ­¥")
            
        return {
            'total_posts': total_posts,
            'processed_posts': processed_posts,
            'index_count': index_count,
            'missing_embeddings': missing_embeddings,
            'coverage_rate': (processed_posts / total_posts) * 100 if total_posts > 0 else 0
        }
        
    except Exception as e:
        logger.error(f"åˆ†æ Embeddings è¦†è“‹ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise

def check_processing_status():
    """æª¢æŸ¥è™•ç†ç‹€æ…‹"""
    try:
        engine = create_engine("postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash")
        
        # æª¢æŸ¥æœ€è¿‘çš„è™•ç†è¨˜éŒ„
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT 
                    DATE(processed_at) as process_date,
                    COUNT(*) as count
                FROM embedding_processed
                GROUP BY DATE(processed_at)
                ORDER BY process_date DESC
                LIMIT 10
            """))
            daily_stats = result.fetchall()
            
        logger.info("=== è™•ç†ç‹€æ…‹æª¢æŸ¥ ===")
        logger.info("æœ€è¿‘ 10 å¤©çš„è™•ç†çµ±è¨ˆ:")
        for date, count in daily_stats:
            logger.info(f"  {date}: {count:,} ç­†")
            
        # æª¢æŸ¥æ˜¯å¦æœ‰è™•ç†ä¸­çš„è¨˜éŒ„
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT COUNT(*) 
                FROM embedding_processed 
                WHERE processed_at > NOW() - INTERVAL '1 hour'
            """))
            recent_count = result.scalar()
            
        logger.info(f"æœ€è¿‘ 1 å°æ™‚è™•ç†çš„è¨˜éŒ„: {recent_count:,} ç­†")
        
    except Exception as e:
        logger.error(f"æª¢æŸ¥è™•ç†ç‹€æ…‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise

def main():
    """ä¸»è¦åˆ†æå‡½æ•¸"""
    logger.info("é–‹å§‹ Embeddings è¦†è“‹ç‡åˆ†æ...")
    
    # 1. åˆ†æè¦†è“‹ç‡
    logger.info("\n=== è¦†è“‹ç‡åˆ†æ ===")
    coverage_stats = analyze_embedding_coverage()
    
    # 2. æª¢æŸ¥è™•ç†ç‹€æ…‹
    logger.info("\n=== è™•ç†ç‹€æ…‹æª¢æŸ¥ ===")
    check_processing_status()
    
    # 3. å»ºè­°
    logger.info("\n=== å»ºè­° ===")
    if coverage_stats['missing_embeddings'] > 0:
        logger.info("ğŸ”§ å»ºè­°é‡æ–°é‹è¡Œ embedding ç”Ÿæˆå™¨:")
        logger.info("   python embedding_generator_memmap.py")
        
        if coverage_stats['missing_embeddings'] < 10000:
            logger.info("ğŸ”§ æˆ–è€…æ‰‹å‹•è™•ç†å‰©é¤˜çš„è²¼æ–‡")
        else:
            logger.info("ğŸ”§ å»ºè­°æª¢æŸ¥è™•ç†éç¨‹ä¸­çš„éŒ¯èª¤æ—¥èªŒ")
    else:
        logger.info("âœ… Embeddings è¦†è“‹ç‡å·²é” 100%")
        
    logger.info("åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main() 