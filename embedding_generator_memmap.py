from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import logging
import time
import os
import json
from typing import Optional, Tuple
import pickle

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EmbeddingGeneratorMemmap:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 1000,
                 embeddings_dir: str = "embeddings_data"):
        """
        åˆå§‹åŒ– Embedding ç”Ÿæˆå™¨ (ä½¿ç”¨ memmap å­˜å„²)
        
        Args:
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²
            model_name: ä½¿ç”¨çš„ sentence-transformers æ¨¡å‹
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
            embeddings_dir: embeddings å­˜å„²ç›®éŒ„
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.embeddings_dir = embeddings_dir
        self.engine = None
        self.model = None
        
        # å‰µå»ºå­˜å„²ç›®éŒ„
        os.makedirs(embeddings_dir, exist_ok=True)
        
        # å®šç¾©æª”æ¡ˆè·¯å¾‘
        self.embeddings_file = os.path.join(embeddings_dir, "embeddings.dat")
        self.index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        self.metadata_file = os.path.join(embeddings_dir, "metadata.json")
        
        # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        self._init_db_connection()
        
        # è¼‰å…¥æ¨¡å‹
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œembedding ç¶­åº¦: {self.embedding_dim}")
        
        # è¼‰å…¥æˆ–åˆå§‹åŒ–ç´¢å¼•
        self.pos_tid_to_index = self._load_index()
        self.next_index = len(self.pos_tid_to_index)
        
    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(self.db_url)
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _load_index(self) -> dict:
        """è¼‰å…¥ pos_tid åˆ° index çš„æ˜ å°„"""
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
                logger.info(f"è¼‰å…¥ç¾æœ‰ç´¢å¼•ï¼Œå…± {len(index_data)} ç­†è¨˜éŒ„")
                return index_data
        else:
            logger.info("å‰µå»ºæ–°çš„ç´¢å¼•æª”æ¡ˆ")
            return {}
            
    def _save_index(self):
        """å„²å­˜ç´¢å¼•æª”æ¡ˆ"""
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.pos_tid_to_index, f, ensure_ascii=False, indent=2)
            
    def _save_metadata(self):
        """å„²å­˜ metadata"""
        metadata = {
            'total_embeddings': len(self.pos_tid_to_index),
            'embedding_dim': self.embedding_dim,
            'model_name': self.model.model_name,
            'last_updated': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
    def _get_total_posts_count(self) -> int:
        """ç²å–éœ€è¦è™•ç†çš„è²¼æ–‡ç¸½æ•¸"""
        try:
            # ç²å–æ‰€æœ‰é‚„æ²’è™•ç†çš„ pos_tid
            processed_ids = set(self.pos_tid_to_index.keys())
            
            with self.engine.connect() as conn:
                if processed_ids:
                    # æ§‹å»º NOT IN å­å¥
                    placeholders = ','.join([f"'{pid}'" for pid in processed_ids])
                    sql = f"""
                        SELECT COUNT(*) 
                        FROM posts 
                        WHERE content IS NOT NULL 
                        AND content != ''
                        AND pos_tid NOT IN ({placeholders})
                    """
                else:
                    sql = """
                        SELECT COUNT(*) 
                        FROM posts 
                        WHERE content IS NOT NULL 
                        AND content != ''
                    """
                
                result = conn.execute(text(sql))
                return result.scalar()
                
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡ç¸½æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_posts_without_embeddings(self, offset: int = 0) -> pd.DataFrame:
        """ç²å–é‚„æ²’æœ‰ embedding çš„è²¼æ–‡"""
        try:
            processed_ids = set(self.pos_tid_to_index.keys())
            
            if processed_ids:
                # æ§‹å»º NOT IN å­å¥ 
                placeholders = ','.join([f"'{pid}'" for pid in processed_ids])
                sql = f"""
                    SELECT pos_tid, content 
                    FROM posts 
                    WHERE content IS NOT NULL 
                    AND content != ''
                    AND pos_tid NOT IN ({placeholders})
                    ORDER BY pos_tid
                    LIMIT {self.batch_size} OFFSET {offset}
                """
            else:
                sql = f"""
                    SELECT pos_tid, content 
                    FROM posts 
                    WHERE content IS NOT NULL 
                    AND content != ''
                    ORDER BY pos_tid
                    LIMIT {self.batch_size} OFFSET {offset}
                """
            
            df = pd.read_sql_query(text(sql), self.engine)
            return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _generate_embeddings(self, contents: list) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬çš„ embeddings"""
        try:
            # éæ¿¾ç©ºç™½å…§å®¹
            valid_contents = [content if content and content.strip() else " " for content in contents]
            embeddings = self.model.encode(valid_contents, convert_to_tensor=False, show_progress_bar=False)
            return embeddings.astype(np.float32)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_memmap_array(self, total_size: int) -> np.ndarray:
        """ç²å–æˆ–å‰µå»º memmap é™£åˆ—"""
        if os.path.exists(self.embeddings_file):
            # è¼‰å…¥ç¾æœ‰çš„ memmap
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='r+',
                shape=(total_size, self.embedding_dim)
            )
        else:
            # å‰µå»ºæ–°çš„ memmap
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='w+',
                shape=(total_size, self.embedding_dim)
            )
            
        return embeddings_array
        
    def _estimate_total_records(self) -> int:
        """ä¼°ç®—ç¸½è¨˜éŒ„æ•¸"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT COUNT(*) 
                    FROM posts 
                    WHERE content IS NOT NULL 
                    AND content != ''
                """))
                return result.scalar()
        except Exception as e:
            logger.error(f"ä¼°ç®—ç¸½è¨˜éŒ„æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _save_embeddings_batch(self, pos_tids: list, embeddings: np.ndarray, embeddings_array: np.ndarray):
        """æ‰¹æ¬¡å„²å­˜ embeddings åˆ° memmap"""
        try:
            indices = []
            for pos_tid in pos_tids:
                if pos_tid not in self.pos_tid_to_index:
                    self.pos_tid_to_index[pos_tid] = self.next_index
                    indices.append(self.next_index)
                    self.next_index += 1
                else:
                    indices.append(self.pos_tid_to_index[pos_tid])
                    
            # å¯«å…¥ embeddings
            for i, (idx, embedding) in enumerate(zip(indices, embeddings)):
                embeddings_array[idx] = embedding
                
            # å¼·åˆ¶å¯«å…¥ç£ç¢Ÿ
            embeddings_array.flush()
            
        except Exception as e:
            logger.error(f"å„²å­˜ embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def process_all_posts(self):
        """è™•ç†æ‰€æœ‰è²¼æ–‡ï¼Œç”Ÿæˆä¸¦å„²å­˜ embeddings"""
        try:
            # ä¼°ç®—ç¸½è¨˜éŒ„æ•¸ä»¥å‰µå»º memmap
            estimated_total = self._estimate_total_records()
            logger.info(f"ä¼°ç®—ç¸½è¨˜éŒ„æ•¸: {estimated_total}")
            
            # å‰µå»º memmap é™£åˆ—
            embeddings_array = self._get_memmap_array(estimated_total)
            
            # ç²å–éœ€è¦è™•ç†çš„æ•¸é‡
            remaining_posts = self._get_total_posts_count()
            logger.info(f"éœ€è¦è™•ç†çš„è²¼æ–‡æ•¸é‡: {remaining_posts}")
            
            if remaining_posts == 0:
                logger.info("æ‰€æœ‰è²¼æ–‡éƒ½å·²ç¶“æœ‰ embeddings äº†")
                return
                
            processed = 0
            offset = 0
            
            while processed < remaining_posts:
                start_time = time.time()
                
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_without_embeddings(offset)
                
                if df.empty:
                    logger.info("æ²’æœ‰æ›´å¤šè³‡æ–™éœ€è¦è™•ç†")
                    break
                    
                logger.info(f"æ­£åœ¨è™•ç†ç¬¬ {processed + 1} åˆ° {processed + len(df)} ç­†è³‡æ–™...")
                
                # ç”Ÿæˆ embeddings
                embeddings = self._generate_embeddings(df['content'].tolist())
                
                # å„²å­˜åˆ° memmap
                self._save_embeddings_batch(df['pos_tid'].tolist(), embeddings, embeddings_array)
                
                processed += len(df)
                
                # è¨ˆç®—è™•ç†æ™‚é–“å’Œé€²åº¦
                elapsed_time = time.time() - start_time
                progress = (processed / remaining_posts) * 100
                
                logger.info(f"âœ… å·²å®Œæˆ {processed}/{remaining_posts} ({progress:.2f}%) - æœ¬æ‰¹æ¬¡è€—æ™‚: {elapsed_time:.2f}ç§’")
                
                # å®šæœŸä¿å­˜ç´¢å¼•
                if processed % (self.batch_size * 10) == 0:
                    self._save_index()
                    self._save_metadata()
                    logger.info("ğŸ“ å·²ä¿å­˜ç´¢å¼•å’Œ metadata")
                
                # é¿å…éåº¦è² è¼‰
                time.sleep(0.1)
                
            # æœ€çµ‚ä¿å­˜
            self._save_index()
            self._save_metadata()
            
            logger.info("ğŸ‰ æ‰€æœ‰è²¼æ–‡çš„ embeddings è™•ç†å®Œæˆï¼")
            logger.info(f"ğŸ“ Embeddings æª”æ¡ˆ: {self.embeddings_file}")
            logger.info(f"ğŸ“‹ ç´¢å¼•æª”æ¡ˆ: {self.index_file}")
            logger.info(f"ğŸ“Š Metadata æª”æ¡ˆ: {self.metadata_file}")
            
        except Exception as e:
            logger.error(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # å³ä½¿å‡ºéŒ¯ä¹Ÿè¦ä¿å­˜é€²åº¦
            self._save_index()
            self._save_metadata()
            raise
            
    def get_embedding(self, pos_tid: str) -> Optional[np.ndarray]:
        """ç²å–æŒ‡å®š pos_tid çš„ embedding"""
        if pos_tid not in self.pos_tid_to_index:
            return None
            
        try:
            index = self.pos_tid_to_index[pos_tid]
            estimated_total = self._estimate_total_records()
            
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='r',
                shape=(estimated_total, self.embedding_dim)
            )
            
            return embeddings_array[index].copy()
            
        except Exception as e:
            logger.error(f"ç²å– embedding æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
            
    def get_embeddings_batch(self, pos_tids: list) -> dict:
        """æ‰¹æ¬¡ç²å– embeddings"""
        try:
            estimated_total = self._estimate_total_records()
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='r',
                shape=(estimated_total, self.embedding_dim)
            )
            
            result = {}
            for pos_tid in pos_tids:
                if pos_tid in self.pos_tid_to_index:
                    index = self.pos_tid_to_index[pos_tid]
                    result[pos_tid] = embeddings_array[index].copy()
                    
            return result
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ç²å– embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {}
            
    def test_connection(self):
        """æ¸¬è©¦è³‡æ–™åº«é€£æ¥å’Œæ¨¡å‹è¼‰å…¥"""
        try:
            # æ¸¬è©¦è³‡æ–™åº«
            with self.engine.connect() as conn:
                result = conn.execute(text("SELECT COUNT(*) FROM posts"))
                total_posts = result.scalar()
                logger.info(f"è³‡æ–™åº«é€£æ¥æ­£å¸¸ï¼Œposts è¡¨æ ¼å…±æœ‰ {total_posts} ç­†è³‡æ–™")
                
            # æ¸¬è©¦æ¨¡å‹
            test_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬"
            test_embedding = self.model.encode(test_text)
            logger.info(f"æ¨¡å‹æ¸¬è©¦æ­£å¸¸ï¼Œembedding ç¶­åº¦: {test_embedding.shape}")
            
            # æ¸¬è©¦æª”æ¡ˆç³»çµ±
            logger.info(f"Embeddings ç›®éŒ„: {self.embeddings_dir}")
            logger.info(f"å·²è™•ç†çš„è¨˜éŒ„æ•¸: {len(self.pos_tid_to_index)}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¸¬è©¦å¤±æ•—: {str(e)}")
            return False
            
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_processed': len(self.pos_tid_to_index),
            'embedding_dimension': self.embedding_dim,
            'embeddings_file_size': os.path.getsize(self.embeddings_file) if os.path.exists(self.embeddings_file) else 0,
            'index_file_size': os.path.getsize(self.index_file) if os.path.exists(self.index_file) else 0
        }
        
        if stats['embeddings_file_size'] > 0:
            stats['embeddings_file_size_mb'] = stats['embeddings_file_size'] / 1024 / 1024
            
        return stats

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    try:
        # å‰µå»º embedding ç”Ÿæˆå™¨
        generator = EmbeddingGeneratorMemmap(
            batch_size=1000,  # å¯ä»¥è¨­ç½®æ›´å¤§çš„æ‰¹æ¬¡å¤§å°
            embeddings_dir="embeddings_data"
        )
        
        # æ¸¬è©¦é€£æ¥
        if not generator.test_connection():
            logger.error("é€£æ¥æ¸¬è©¦å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
            return
            
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = generator.get_statistics()
        logger.info(f"ç•¶å‰çµ±è¨ˆ: {stats}")
        
        # é–‹å§‹è™•ç†
        logger.info("é–‹å§‹ç”Ÿæˆå’Œå„²å­˜ embeddings...")
        generator.process_all_posts()
        
        # æœ€çµ‚çµ±è¨ˆ
        final_stats = generator.get_statistics()
        logger.info(f"è™•ç†å®Œæˆçµ±è¨ˆ: {final_stats}")
        
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == "__main__":
    main() 