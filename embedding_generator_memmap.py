from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import logging
import time
import os
import json
from typing import Optional
import torch

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EmbeddingGeneratorMemmap:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 1000,
                 embeddings_dir: str = "embeddings_data",
                 source_table: str = "posts_deduplicated",
                 device: str = "auto"):
        """
        åˆå§‹åŒ– Embedding ç”Ÿæˆå™¨ (ä½¿ç”¨ memmap å­˜å„²)
        
        Args:
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²
            model_name: ä½¿ç”¨çš„ sentence-transformers æ¨¡å‹
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
            embeddings_dir: embeddings å­˜å„²ç›®éŒ„
            source_table: ä¾†æºè³‡æ–™è¡¨åç¨±
            device: è¨ˆç®—è£ç½® ('auto', 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.)
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.embeddings_dir = embeddings_dir
        self.source_table = source_table
        self.engine = None
        self.model = None
        self.model_name = model_name  # ä¿å­˜æ¨¡å‹åç¨±
        
        # è¨­å®šè¨ˆç®—è£ç½®
        self.device = self._setup_device(device)
        logger.info(f"å°‡ä½¿ç”¨è£ç½®: {self.device}")
        
        # å‰µå»ºå­˜å„²ç›®éŒ„
        os.makedirs(embeddings_dir, exist_ok=True)
        
        # å®šç¾©æª”æ¡ˆè·¯å¾‘
        self.embeddings_file = os.path.join(embeddings_dir, "embeddings.dat")
        self.index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        self.metadata_file = os.path.join(embeddings_dir, "metadata.json")
        
        # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        self._init_db_connection()
        
        # å‰µå»ºè™•ç†ç‹€æ…‹è¡¨
        self._create_processed_table()
        
        # è¼‰å…¥æ¨¡å‹
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name, device=self.device)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        logger.info(f"æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œembedding ç¶­åº¦: {self.embedding_dim}")
        
        # å¦‚æœä½¿ç”¨ GPUï¼Œé¡¯ç¤º GPU è³‡è¨Š
        if self.device.startswith('cuda'):
            self._log_gpu_info()
        
        # è¼‰å…¥æˆ–åˆå§‹åŒ–ç´¢å¼•
        self.pos_tid_to_index = self._load_index()
        self.next_index = len(self.pos_tid_to_index)
        
        # åŒæ­¥è™•ç†ç‹€æ…‹è¡¨
        self._sync_processed_table()
        
    def _setup_device(self, device: str) -> str:
        """è¨­å®šè¨ˆç®—è£ç½®"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                logger.info("âœ… æª¢æ¸¬åˆ° CUDAï¼Œå°‡ä½¿ç”¨ GPU åŠ é€Ÿ")
            else:
                device = "cpu"
                logger.info("âš ï¸ æœªæª¢æ¸¬åˆ° CUDAï¼Œå°‡ä½¿ç”¨ CPU")
        elif device.startswith("cuda"):
            if not torch.cuda.is_available():
                logger.warning("âš ï¸ æŒ‡å®šä½¿ç”¨ CUDA ä½†ç³»çµ±ä¸æ”¯æ´ï¼Œæ”¹ç”¨ CPU")
                device = "cpu"
            else:
                logger.info(f"âœ… å°‡ä½¿ç”¨æŒ‡å®šçš„ GPU: {device}")
        else:
            logger.info(f"âœ… å°‡ä½¿ç”¨æŒ‡å®šçš„è£ç½®: {device}")
            
        return device
        
    def _log_gpu_info(self):
        """è¨˜éŒ„ GPU è³‡è¨Š"""
        try:
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                gpu_name = torch.cuda.get_device_name(current_device)
                gpu_memory = torch.cuda.get_device_properties(current_device).total_memory / 1024**3
                
                logger.info(f"ğŸ® GPU è³‡è¨Š:")
                logger.info(f"   - å¯ç”¨ GPU æ•¸é‡: {gpu_count}")
                logger.info(f"   - ç•¶å‰ä½¿ç”¨ GPU: {current_device}")
                logger.info(f"   - GPU åç¨±: {gpu_name}")
                logger.info(f"   - GPU è¨˜æ†¶é«”: {gpu_memory:.1f} GB")
                
        except Exception as e:
            logger.warning(f"ç„¡æ³•ç²å– GPU è³‡è¨Š: {str(e)}")

    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(self.db_url)
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _create_processed_table(self):
        """å‰µå»ºè™•ç†ç‹€æ…‹è¡¨"""
        try:
            with self.engine.begin() as conn:
                conn.execute(text("""
                    CREATE TABLE IF NOT EXISTS embedding_processed (
                        pos_tid TEXT PRIMARY KEY,
                        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """))
                
                # å‰µå»ºç´¢å¼•ä»¥æé«˜æŸ¥è©¢æ•ˆèƒ½
                conn.execute(text("""
                    CREATE INDEX IF NOT EXISTS idx_embedding_processed_pos_tid 
                    ON embedding_processed(pos_tid)
                """))
                
        except Exception as e:
            logger.error(f"å‰µå»ºè™•ç†ç‹€æ…‹è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _sync_processed_table(self):
        """åŒæ­¥è™•ç†ç‹€æ…‹è¡¨èˆ‡ç´¢å¼•æª”æ¡ˆ"""
        try:
            processed_ids = list(self.pos_tid_to_index.keys())
            
            if processed_ids:
                with self.engine.begin() as conn:
                    # æ‰¹æ¬¡æ’å…¥æ–°çš„å·²è™•ç†è¨˜éŒ„
                    batch_size = 1000
                    for i in range(0, len(processed_ids), batch_size):
                        batch = processed_ids[i:i + batch_size]
                        values = ','.join([f"('{pid}')" for pid in batch])
                        conn.execute(text(f"""
                            INSERT INTO embedding_processed (pos_tid) 
                            VALUES {values}
                            ON CONFLICT (pos_tid) DO NOTHING
                        """))
        except Exception as e:
            logger.error(f"åŒæ­¥è™•ç†ç‹€æ…‹è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            
    def _load_index(self) -> dict:
        """è¼‰å…¥ pos_tid åˆ° index çš„æ˜ å°„"""
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
                logger.info(f"è¼‰å…¥ç¾æœ‰ç´¢å¼•ï¼Œå…± {len(index_data)} ç­†è¨˜éŒ„")
                return index_data
        else:
            logger.info("å‰µå»ºæ–°çš„ç´¢å¼•æª”æ¡ˆ")
            return {}
            
    def _save_index(self):
        """å„²å­˜ç´¢å¼•æª”æ¡ˆ"""
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(self.pos_tid_to_index, f, ensure_ascii=False, indent=2)
            
    def _save_metadata(self):
        """å„²å­˜ metadata"""
        metadata = {
            'total_embeddings': len(self.pos_tid_to_index),
            'embedding_dim': self.embedding_dim,
            'model_name': self.model_name,
            'device': self.device,
            'last_updated': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # å¦‚æœä½¿ç”¨ GPUï¼Œæ·»åŠ  GPU è³‡è¨Š
        if self.device.startswith('cuda') and torch.cuda.is_available():
            try:
                metadata['gpu_info'] = {
                    'gpu_name': torch.cuda.get_device_name(),
                    'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3,
                    'cuda_version': torch.version.cuda
                }
            except:
                pass
                
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
    def _get_total_posts_count(self) -> int:
        """ç²å–éœ€è¦è™•ç†çš„è²¼æ–‡ç¸½æ•¸"""
        try:
            with self.engine.connect() as conn:
                sql = f"""
                    SELECT COUNT(*) 
                    FROM {self.source_table} p
                    LEFT JOIN embedding_processed ep ON p.pos_tid = ep.pos_tid
                    WHERE p.content IS NOT NULL 
                    AND p.content != ''
                    AND ep.pos_tid IS NULL
                """
                
                result = conn.execute(text(sql))
                return result.scalar()
                
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡ç¸½æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_posts_without_embeddings(self, offset: int = 0) -> pd.DataFrame:
        """ç²å–é‚„æ²’æœ‰ embedding çš„è²¼æ–‡ (ä½¿ç”¨è™•ç†ç‹€æ…‹è¡¨)"""
        try:
            with self.engine.connect() as conn:
                sql = f"""
                    SELECT p.pos_tid, p.content 
                    FROM {self.source_table} p
                    LEFT JOIN embedding_processed ep ON p.pos_tid = ep.pos_tid
                    WHERE p.content IS NOT NULL 
                    AND p.content != ''
                    AND ep.pos_tid IS NULL
                    ORDER BY p.pos_tid
                    LIMIT {self.batch_size} OFFSET {offset}
                """
                
                df = pd.read_sql_query(text(sql), conn)
                return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _generate_embeddings(self, contents: list) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬çš„ embeddings (æ”¯æ´ GPU åŠ é€Ÿ)"""
        try:
            # éæ¿¾ç©ºç™½å…§å®¹
            valid_contents = [content if content and content.strip() else " " for content in contents]
            
            # å¦‚æœä½¿ç”¨ GPUï¼Œè¨˜éŒ„è™•ç†å‰çš„è¨˜æ†¶é«”ç‹€æ…‹
            if self.device.startswith('cuda') and torch.cuda.is_available():
                torch.cuda.empty_cache()  # æ¸…ç†æœªä½¿ç”¨çš„è¨˜æ†¶é«”
                memory_before = torch.cuda.memory_allocated() / 1024**2
                
            # ä½¿ç”¨ GPU åŠ é€Ÿç”Ÿæˆ embeddings - èª¿é«˜batch sizeä»¥æ›´å¥½åˆ©ç”¨è³‡æº
            embeddings = self.model.encode(
                valid_contents, 
                convert_to_tensor=False, 
                show_progress_bar=False,  
                batch_size=min(len(valid_contents), 1024 if self.device.startswith('cuda') else 256),  # èª¿é«˜åˆ°1024å……åˆ†åˆ©ç”¨GPU
                device=self.device
            )
            
            # å¦‚æœä½¿ç”¨ GPUï¼Œè¨˜éŒ„è™•ç†å¾Œçš„è¨˜æ†¶é«”ç‹€æ…‹
            if self.device.startswith('cuda') and torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated() / 1024**2
                memory_reserved = torch.cuda.memory_reserved() / 1024**2
                memory_peak = torch.cuda.max_memory_allocated() / 1024**2
                
                # æ¯éš”ä¸€æ®µæ™‚é–“è¨˜éŒ„ä¸€æ¬¡è©³ç´°çš„GPUç‹€æ…‹
                if hasattr(self, '_last_gpu_log_time'):
                    if time.time() - self._last_gpu_log_time > 30:  # æ¯30ç§’è¨˜éŒ„ä¸€æ¬¡
                        logger.info(f"ğŸ® GPU è¨˜æ†¶é«”ç‹€æ…‹: ä½¿ç”¨ä¸­ {memory_after:.1f}MB, å·²ä¿ç•™ {memory_reserved:.1f}MB, å³°å€¼ {memory_peak:.1f}MB")
                        self._last_gpu_log_time = time.time()
                else:
                    self._last_gpu_log_time = time.time()
                    logger.info(f"ğŸ® GPU è¨˜æ†¶é«”ç‹€æ…‹: ä½¿ç”¨ä¸­ {memory_after:.1f}MB, å·²ä¿ç•™ {memory_reserved:.1f}MB")
            
            return embeddings.astype(np.float32)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆ embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # å¦‚æœ GPU å‡ºéŒ¯ï¼Œå˜—è©¦å›é€€åˆ° CPU
            if self.device.startswith('cuda'):
                logger.warning("GPU è™•ç†å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ CPU...")
                try:
                    self.model = self.model.to('cpu')
                    self.device = 'cpu'
                    embeddings = self.model.encode(
                        valid_contents, 
                        convert_to_tensor=False, 
                        show_progress_bar=False,
                        device='cpu'
                    )
                    return embeddings.astype(np.float32)
                except:
                    pass
            raise
            
    def _get_memmap_array(self, total_size: int) -> np.ndarray:
        """ç²å–æˆ–å‰µå»º memmap é™£åˆ—"""
        if os.path.exists(self.embeddings_file):
            # è¼‰å…¥ç¾æœ‰çš„ memmap
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='r+',
                shape=(total_size, self.embedding_dim)
            )
        else:
            # å‰µå»ºæ–°çš„ memmap
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='w+',
                shape=(total_size, self.embedding_dim)
            )
            
        return embeddings_array
        
    def _estimate_total_records(self) -> int:
        """ä¼°ç®—ç¸½è¨˜éŒ„æ•¸"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"""
                    SELECT COUNT(*) 
                    FROM {self.source_table}
                    WHERE content IS NOT NULL 
                    AND content != ''
                """))
                return result.scalar()
        except Exception as e:
            logger.error(f"ä¼°ç®—ç¸½è¨˜éŒ„æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _save_embeddings_batch(self, pos_tids: list, embeddings: np.ndarray, embeddings_array: np.ndarray):
        """æ‰¹æ¬¡å„²å­˜ embeddings åˆ° memmap ä¸¦æ›´æ–°è™•ç†ç‹€æ…‹è¡¨"""
        try:
            indices = []
            for pos_tid in pos_tids:
                if pos_tid not in self.pos_tid_to_index:
                    self.pos_tid_to_index[pos_tid] = self.next_index
                    indices.append(self.next_index)
                    self.next_index += 1
                else:
                    indices.append(self.pos_tid_to_index[pos_tid])
                    
            # å¯«å…¥ embeddings
            for i, (idx, embedding) in enumerate(zip(indices, embeddings)):
                embeddings_array[idx] = embedding
                
            # å¼·åˆ¶å¯«å…¥ç£ç¢Ÿ
            embeddings_array.flush()
            
            # æ›´æ–°è™•ç†ç‹€æ…‹è¡¨
            with self.engine.begin() as conn:
                values = ','.join([f"('{pid}')" for pid in pos_tids])
                conn.execute(text(f"""
                    INSERT INTO embedding_processed (pos_tid) 
                    VALUES {values}
                    ON CONFLICT (pos_tid) DO NOTHING
                """))
            
        except Exception as e:
            logger.error(f"å„²å­˜ embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def process_all_posts(self):
        """è™•ç†æ‰€æœ‰è²¼æ–‡ï¼Œç”Ÿæˆä¸¦å„²å­˜ embeddings"""
        try:
            # ä¼°ç®—ç¸½è¨˜éŒ„æ•¸ä»¥å‰µå»º memmap
            estimated_total = self._estimate_total_records()
            logger.info(f"ä¼°ç®—ç¸½è¨˜éŒ„æ•¸: {estimated_total}")
            
            # å‰µå»º memmap é™£åˆ—
            embeddings_array = self._get_memmap_array(estimated_total)
            
            # ç²å–éœ€è¦è™•ç†çš„æ•¸é‡
            remaining_posts = self._get_total_posts_count()
            logger.info(f"éœ€è¦è™•ç†çš„è²¼æ–‡æ•¸é‡: {remaining_posts}")
            
            if remaining_posts == 0:
                logger.info("æ‰€æœ‰è²¼æ–‡éƒ½å·²ç¶“æœ‰ embeddings äº†")
                return
                
            processed = 0
            offset = 0
            
            # æ€§èƒ½çµ±è¨ˆ
            total_db_time = 0
            total_embedding_time = 0
            total_save_time = 0
            
            while processed < remaining_posts:
                batch_start_time = time.time()
                
                # 1. è³‡æ–™åº«æŸ¥è©¢æ™‚é–“
                db_start = time.time()
                df = self._get_posts_without_embeddings(offset)
                db_time = time.time() - db_start
                total_db_time += db_time
                
                if df.empty:
                    logger.info("æ²’æœ‰æ›´å¤šè³‡æ–™éœ€è¦è™•ç†")
                    break
                    
                # 2. Embedding ç”Ÿæˆæ™‚é–“
                embedding_start = time.time()
                embeddings = self._generate_embeddings(df['content'].tolist())
                embedding_time = time.time() - embedding_start
                total_embedding_time += embedding_time
                
                # 3. å„²å­˜æ™‚é–“
                save_start = time.time()
                self._save_embeddings_batch(df['pos_tid'].tolist(), embeddings, embeddings_array)
                save_time = time.time() - save_start
                total_save_time += save_time
                
                processed += len(df)
                
                # è¨ˆç®—è™•ç†æ™‚é–“å’Œé€²åº¦
                total_batch_time = time.time() - batch_start_time
                progress = (processed / remaining_posts) * 100
                
                # è©³ç´°æ€§èƒ½å ±å‘Š
                logger.info(f"âœ… å·²å®Œæˆ {processed}/{remaining_posts} ({progress:.2f}%) - ç¸½è€—æ™‚: {total_batch_time:.2f}ç§’")
                logger.info(f"   ğŸ“Š æ€§èƒ½åˆ†æ: DBæŸ¥è©¢ {db_time:.2f}s ({db_time/total_batch_time*100:.1f}%) | "
                          f"Embeddingç”Ÿæˆ {embedding_time:.2f}s ({embedding_time/total_batch_time*100:.1f}%) | "
                          f"è³‡æ–™å„²å­˜ {save_time:.2f}s ({save_time/total_batch_time*100:.1f}%)")
                
                # å®šæœŸä¿å­˜ç´¢å¼•
                if processed % (self.batch_size * 3) == 0:  # æ¯ 3 å€‹æ‰¹æ¬¡å„²å­˜ä¸€æ¬¡
                    self._save_index()
                    self._save_metadata()
                    logger.info("ğŸ“ å·²ä¿å­˜ç´¢å¼•å’Œ metadata")
                    
                    # é¡¯ç¤ºç´¯ç©æ€§èƒ½çµ±è¨ˆ
                    batches_processed = processed // self.batch_size
                    if batches_processed > 0:
                        avg_db_time = total_db_time / batches_processed
                        avg_embedding_time = total_embedding_time / batches_processed
                        avg_save_time = total_save_time / batches_processed
                        
                        logger.info(f"ğŸ“ˆ ç´¯ç©æ€§èƒ½çµ±è¨ˆ (å¹³å‡æ¯æ‰¹æ¬¡):")
                        logger.info(f"   DBæŸ¥è©¢: {avg_db_time:.2f}s | Embedding: {avg_embedding_time:.2f}s | å„²å­˜: {avg_save_time:.2f}s")
                
                # é¿å…éåº¦è² è¼‰
                time.sleep(0.1)
                
            # æœ€çµ‚ä¿å­˜
            self._save_index()
            self._save_metadata()
            
            # æœ€çµ‚æ€§èƒ½å ±å‘Š
            batches_total = processed // self.batch_size
            if batches_total > 0:
                logger.info("ğŸ‰ æ‰€æœ‰è²¼æ–‡çš„ embeddings è™•ç†å®Œæˆï¼")
                logger.info(f"ğŸ“Š æœ€çµ‚æ€§èƒ½çµ±è¨ˆ:")
                logger.info(f"   å¹³å‡DBæŸ¥è©¢æ™‚é–“: {total_db_time/batches_total:.2f}s ({total_db_time/(total_db_time+total_embedding_time+total_save_time)*100:.1f}%)")
                logger.info(f"   å¹³å‡Embeddingæ™‚é–“: {total_embedding_time/batches_total:.2f}s ({total_embedding_time/(total_db_time+total_embedding_time+total_save_time)*100:.1f}%)")
                logger.info(f"   å¹³å‡å„²å­˜æ™‚é–“: {total_save_time/batches_total:.2f}s ({total_save_time/(total_db_time+total_embedding_time+total_save_time)*100:.1f}%)")
            
            logger.info(f"ğŸ“ Embeddings æª”æ¡ˆ: {self.embeddings_file}")
            logger.info(f"ğŸ“‹ ç´¢å¼•æª”æ¡ˆ: {self.index_file}")
            logger.info(f"ğŸ“Š Metadata æª”æ¡ˆ: {self.metadata_file}")
            
        except Exception as e:
            logger.error(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            # å³ä½¿å‡ºéŒ¯ä¹Ÿè¦ä¿å­˜é€²åº¦
            self._save_index()
            self._save_metadata()
            raise
            
    def get_embedding(self, pos_tid: str) -> Optional[np.ndarray]:
        """ç²å–æŒ‡å®š pos_tid çš„ embedding"""
        if pos_tid not in self.pos_tid_to_index:
            return None
            
        try:
            index = self.pos_tid_to_index[pos_tid]
            estimated_total = self._estimate_total_records()
            
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='r',
                shape=(estimated_total, self.embedding_dim)
            )
            
            return embeddings_array[index].copy()
            
        except Exception as e:
            logger.error(f"ç²å– embedding æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
            
    def get_embeddings_batch(self, pos_tids: list) -> dict:
        """æ‰¹æ¬¡ç²å– embeddings"""
        try:
            estimated_total = self._estimate_total_records()
            embeddings_array = np.memmap(
                self.embeddings_file, 
                dtype=np.float32, 
                mode='r',
                shape=(estimated_total, self.embedding_dim)
            )
            
            result = {}
            for pos_tid in pos_tids:
                if pos_tid in self.pos_tid_to_index:
                    index = self.pos_tid_to_index[pos_tid]
                    result[pos_tid] = embeddings_array[index].copy()
                    
            return result
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ç²å– embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {}
            
    def test_connection(self):
        """æ¸¬è©¦è³‡æ–™åº«é€£æ¥å’Œæ¨¡å‹è¼‰å…¥"""
        try:
            # æ¸¬è©¦è³‡æ–™åº«
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {self.source_table}"))
                total_posts = result.scalar()
                logger.info(f"è³‡æ–™åº«é€£æ¥æ­£å¸¸ï¼Œ{self.source_table} è¡¨æ ¼å…±æœ‰ {total_posts} ç­†è³‡æ–™")
                
            # æ¸¬è©¦æ¨¡å‹å’Œè£ç½®
            test_text = "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬"
            start_time = time.time()
            test_embedding = self.model.encode(test_text)
            encode_time = time.time() - start_time
            
            logger.info(f"æ¨¡å‹æ¸¬è©¦æ­£å¸¸ï¼Œembedding ç¶­åº¦: {test_embedding.shape}")
            logger.info(f"ç•¶å‰ä½¿ç”¨è£ç½®: {self.device}")
            logger.info(f"å–®å€‹æ–‡æœ¬ç·¨ç¢¼è€—æ™‚: {encode_time:.4f}ç§’")
            
            # å¦‚æœä½¿ç”¨ GPUï¼Œæ¸¬è©¦ GPU æ€§èƒ½
            if self.device.startswith('cuda'):
                logger.info("ğŸ® é€²è¡Œ GPU æ€§èƒ½æ¸¬è©¦...")
                test_texts = ["æ¸¬è©¦æ–‡æœ¬"] * 100
                start_time = time.time()
                test_embeddings = self.model.encode(test_texts, batch_size=32)
                batch_time = time.time() - start_time
                avg_time = batch_time / 100
                logger.info(f"æ‰¹æ¬¡è™•ç† 100 å€‹æ–‡æœ¬è€—æ™‚: {batch_time:.4f}ç§’ (å¹³å‡æ¯å€‹: {avg_time:.6f}ç§’)")
                
                # é¡¯ç¤º GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
                if torch.cuda.is_available():
                    memory_allocated = torch.cuda.memory_allocated() / 1024**2
                    memory_cached = torch.cuda.memory_reserved() / 1024**2
                    logger.info(f"GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_allocated:.1f} MB (å·²åˆ†é…) / {memory_cached:.1f} MB (å·²å¿«å–)")
            
            # æ¸¬è©¦æª”æ¡ˆç³»çµ±
            logger.info(f"Embeddings ç›®éŒ„: {self.embeddings_dir}")
            logger.info(f"å·²è™•ç†çš„è¨˜éŒ„æ•¸: {len(self.pos_tid_to_index)}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¸¬è©¦å¤±æ•—: {str(e)}")
            return False
            
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_processed': len(self.pos_tid_to_index),
            'embedding_dimension': self.embedding_dim,
            'embeddings_file_size': os.path.getsize(self.embeddings_file) if os.path.exists(self.embeddings_file) else 0,
            'index_file_size': os.path.getsize(self.index_file) if os.path.exists(self.index_file) else 0
        }
        
        if stats['embeddings_file_size'] > 0:
            stats['embeddings_file_size_mb'] = stats['embeddings_file_size'] / 1024 / 1024
            
        return stats

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    try:
        # å‰µå»º embedding ç”Ÿæˆå™¨
        generator = EmbeddingGeneratorMemmap(
            batch_size=65536,  # èª¿æ•´åˆ° 65536 ä»¥æå‡è™•ç†æ•ˆç‡
            embeddings_dir="embeddings_data",
            source_table="posts_deduplicated",  # æŒ‡å®šä¾†æºè¡¨
            device="auto"  # ä½¿ç”¨è‡ªå‹•åµæ¸¬è£ç½®
        )
        
        # æ¸¬è©¦é€£æ¥
        if not generator.test_connection():
            logger.error("é€£æ¥æ¸¬è©¦å¤±æ•—ï¼Œç¨‹å¼çµæŸ")
            return
            
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = generator.get_statistics()
        logger.info(f"ç•¶å‰çµ±è¨ˆ: {stats}")
        
        # é–‹å§‹è™•ç†
        logger.info("é–‹å§‹ç”Ÿæˆå’Œå„²å­˜ embeddings...")
        generator.process_all_posts()
        
        # æœ€çµ‚çµ±è¨ˆ
        final_stats = generator.get_statistics()
        logger.info(f"è™•ç†å®Œæˆçµ±è¨ˆ: {final_stats}")
        
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == "__main__":
    main() 