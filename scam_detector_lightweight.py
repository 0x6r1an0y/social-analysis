#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¼•é‡ç´šè©é¨™æª¢æ¸¬å™¨ - è§£æ±ºè¨˜æ†¶é«”ä¸è¶³å•é¡Œ
"""

from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
import logging
import os
import json
import argparse
from typing import List, Dict, Optional
import psutil
import gc
import random

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,
        'vms_mb': memory_info.vms / 1024 / 1024,
        'percent': process.memory_percent()
    }

class ScamDetectorLightweight:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 20,  # éå¸¸å°çš„æ‰¹æ¬¡å¤§å°
                 embeddings_dir: str = "embeddings_data"):
        """
        è¼•é‡ç´šè©é¨™æª¢æ¸¬å™¨
        
        Args:
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²
            model_name: ä½¿ç”¨çš„ sentence-transformers æ¨¡å‹
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°ï¼ˆå»ºè­° 10-50ï¼‰
            embeddings_dir: embeddings å­˜å„²ç›®éŒ„
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.embeddings_dir = embeddings_dir
        self.engine = None
        self.model = None
        
        # é è¨­è©é¨™æç¤ºè©
        self.default_scam_phrases = [
            "åŠ å…¥LINE", "åŠ å…¥Telegram", "å¿«é€Ÿè³ºéŒ¢", "è¢«å‹•æ”¶å…¥", 
            "æŠ•è³‡åŒ…ä½ è³º", "ç§è¨Šæˆ‘", "è€å¸«å¸¶å–®", "ç©©è³ºä¸è³ ",
            "è¼•é¬†è³ºéŒ¢", "ä¸€å¤©è³ºè¬å…ƒ", "ä¿è­‰ç²åˆ©", "é«˜å ±é…¬ä½é¢¨éšª",
            "åŠ ç¾¤çµ„", "è·Ÿå–®", "æ“ç›¤æ‰‹", "è²¡å¯Œè‡ªç”±",
            "æœˆæ”¶å…¥", "å…¼è·è³ºéŒ¢", "åœ¨å®¶è³ºéŒ¢", "ç¶²è·¯è³ºéŒ¢",
            "æŠ•è³‡ç†è²¡", "è™›æ“¬è²¨å¹£", "æ¯”ç‰¹å¹£", "æŒ–ç¤¦",
            "å€Ÿè²¸", "å°é¡è²¸æ¬¾", "æ€¥ç”¨éŒ¢", "å…æŠµæŠ¼",
            "ä»£è¾¦ä¿¡è²¸", "ä¿¡ç”¨å¡ä»£å„Ÿ", "å‚µå‹™æ•´åˆ"
        ]
        
        # æª”æ¡ˆè·¯å¾‘
        self.embeddings_file = os.path.join(embeddings_dir, "embeddings.dat")
        self.index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        self.metadata_file = os.path.join(embeddings_dir, "metadata.json")
        
        # åˆå§‹åŒ–
        self._init_db_connection()
        self._load_model(model_name)
        self._load_embeddings_metadata()
        
    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(self.db_url)
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _load_model(self, model_name: str):
        """è¼‰å…¥æ¨¡å‹"""
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def _load_embeddings_metadata(self):
        """è¼‰å…¥ embeddings metadata"""
        try:
            # è¼‰å…¥ç´¢å¼•
            if not os.path.exists(self.index_file):
                raise FileNotFoundError(f"ç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨: {self.index_file}")
                
            with open(self.index_file, 'r', encoding='utf-8') as f:
                self.pos_tid_to_index = json.load(f)
                
            # è¼‰å…¥ metadata
            if not os.path.exists(self.metadata_file):
                raise FileNotFoundError(f"Metadata æª”æ¡ˆä¸å­˜åœ¨: {self.metadata_file}")
                
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
                
            self.embedding_dim = self.metadata['embedding_dim']
            self.total_embeddings = self.metadata['total_embeddings']
            
            logger.info(f"è¼‰å…¥ embeddings metadataï¼š")
            logger.info(f"  - ç¸½è¨˜éŒ„æ•¸: {self.total_embeddings}")
            logger.info(f"  - Embedding ç¶­åº¦: {self.embedding_dim}")
            logger.info(f"  - æœ€å¾Œæ›´æ–°: {self.metadata.get('last_updated', 'Unknown')}")
            
            # æª¢æŸ¥ embeddings æª”æ¡ˆ
            if not os.path.exists(self.embeddings_file):
                raise FileNotFoundError(f"Embeddings æª”æ¡ˆä¸å­˜åœ¨: {self.embeddings_file}")
                
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings metadata å¤±æ•—: {str(e)}")
            raise
            
    def _get_single_embedding(self, pos_tid: str) -> Optional[np.ndarray]:
        """ç²å–å–®ä¸€è²¼æ–‡çš„ embedding"""
        try:
            if pos_tid not in self.pos_tid_to_index:
                return None
                
            index = self.pos_tid_to_index[pos_tid]
            
            # ä½¿ç”¨ memmap åªè®€å–å–®ä¸€ embedding
            embeddings_array = np.memmap(
                self.embeddings_file,
                dtype=np.float32,
                mode='r',
                shape=(self.total_embeddings, self.embedding_dim)
            )
            
            embedding = embeddings_array[index].copy()
            del embeddings_array  # ç«‹å³é‡‹æ”¾è¨˜æ†¶é«”
            return embedding
            
        except Exception as e:
            logger.error(f"ç²å– embedding å¤±æ•—: {str(e)}")
            return None
            
    def _get_posts_sample(self, limit: int = 100) -> pd.DataFrame:
        """ç²å–è²¼æ–‡æ¨£æœ¬ï¼ˆé¿å…è¼‰å…¥å…¨éƒ¨è³‡æ–™ï¼‰"""
        try:
            # å¾æœ‰ embeddings çš„è²¼æ–‡ä¸­éš¨æ©Ÿé¸å–æ¨£æœ¬
            valid_pos_tids = list(self.pos_tid_to_index.keys())
            
            if not valid_pos_tids:
                logger.warning("æ²’æœ‰æ‰¾åˆ°æœ‰ embeddings çš„è²¼æ–‡")
                return pd.DataFrame()
            
            # ç‚ºäº†ç¢ºä¿èƒ½ç²å–åˆ°è¶³å¤ çš„è²¼æ–‡ï¼Œæˆ‘å€‘é¸å–æ›´å¤šçš„ pos_tid
            # è€ƒæ…®åˆ°æœ‰äº›è²¼æ–‡å¯èƒ½åœ¨è³‡æ–™åº«ä¸­ä¸å­˜åœ¨æˆ–å…§å®¹ç‚ºç©º
            target_sample_size = int(limit * 1.2)  # å¢åŠ  20% çš„æ¨£æœ¬
            selected_pos_tids = random.sample(valid_pos_tids, min(target_sample_size, len(valid_pos_tids)))
            
            logger.info(f"å¾ {len(valid_pos_tids):,} ç­†æœ‰ embeddings çš„è²¼æ–‡ä¸­éš¨æ©Ÿé¸å– {len(selected_pos_tids)} ç­†")
            
            # æ§‹å»º SQL æŸ¥è©¢ - æ”¹ç‚ºå¾ posts_deduplicated è¡¨æŸ¥è©¢
            placeholders = ','.join([f"'{pid}'" for pid in selected_pos_tids])
            sql = f"""
                SELECT pos_tid, content, page_name, created_time
                FROM posts_deduplicated 
                WHERE pos_tid IN ({placeholders})
                AND content IS NOT NULL 
                AND content != ''
                ORDER BY pos_tid
            """
            
            df = pd.read_sql_query(text(sql), self.engine)
            logger.info(f"æˆåŠŸç²å– {len(df)} ç­†è²¼æ–‡è³‡æ–™")
            
            # å¦‚æœç²å–åˆ°çš„è²¼æ–‡æ•¸é‡ä¸è¶³ï¼Œè¨˜éŒ„è­¦å‘Š
            if len(df) < limit:
                logger.warning(f"åªç²å–åˆ° {len(df)} ç­†è²¼æ–‡ï¼Œå°‘æ–¼è¦æ±‚çš„ {limit} ç­†")
                logger.warning("é€™å¯èƒ½æ˜¯å› ç‚ºéƒ¨åˆ†è²¼æ–‡åœ¨ posts_deduplicated è¡¨ä¸­ä¸å­˜åœ¨æˆ–å…§å®¹ç‚ºç©º")
            elif len(df) > limit:
                # å¦‚æœç²å–åˆ°å¤ªå¤šï¼Œéš¨æ©Ÿé¸å–æŒ‡å®šæ•¸é‡
                df = df.sample(n=limit, random_state=42).reset_index(drop=True)
                logger.info(f"éš¨æ©Ÿé¸å– {limit} ç­†è²¼æ–‡")
            
            return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡æ¨£æœ¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def calculate_scam_score_single(self, content: str, scam_phrases: Optional[List[str]] = None) -> Dict:
        """è¨ˆç®—å–®ä¸€è²¼æ–‡çš„è©é¨™åˆ†æ•¸"""
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆå…§å®¹ embedding
            content_embedding = self.model.encode(content, convert_to_tensor=True)
            
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarities = util.cos_sim(content_embedding, phrase_embeddings).squeeze().cpu().numpy()
            scam_score = float(np.max(similarities))
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª
            top_matches = []
            for i, phrase in enumerate(scam_phrases):
                top_matches.append({
                    'phrase': phrase,
                    'similarity': float(similarities[i])
                })
                
            top_matches.sort(key=lambda x: x['similarity'], reverse=True)
            
            return {
                'scam_score': scam_score,
                'top_matches': top_matches[:5],
                'is_potential_scam': scam_score >= 0.6
            }
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è©é¨™åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_scam_sample(self, 
                          sample_size: int = 100,
                          scam_phrases: Optional[List[str]] = None,
                          threshold: float = 0.6,
                          output_file: Optional[str] = None) -> pd.DataFrame:
        """
        æª¢æ¸¬è²¼æ–‡æ¨£æœ¬
        
        Args:
            sample_size: æ¨£æœ¬å¤§å°
            scam_phrases: è©é¨™æç¤ºè©
            threshold: è©é¨™é¢¨éšªé–¾å€¼
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸çš„ DataFrame
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        logger.info(f"é–‹å§‹æª¢æ¸¬ {sample_size} ç­†è²¼æ–‡æ¨£æœ¬")
        logger.info(f"è©é¨™é¢¨éšªé–¾å€¼: {threshold}")
        
        try:
            # é¡¯ç¤ºåˆå§‹è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
            initial_memory = get_memory_usage()
            logger.info(f"åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨: {initial_memory['rss_mb']:.1f} MB ({initial_memory['percent']:.1f}%)")
            
            # ç²å–è²¼æ–‡æ¨£æœ¬
            df = self._get_posts_sample(sample_size)
            
            if df.empty:
                logger.warning("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è²¼æ–‡")
                return pd.DataFrame()
                
            logger.info(f"ç²å–åˆ° {len(df)} ç­†è²¼æ–‡æ¨£æœ¬")
            
            results = []
            
            for idx, row in df.iterrows():
                try:
                    # è¨ˆç®—è©é¨™åˆ†æ•¸
                    scam_result = self.calculate_scam_score_single(row['content'], scam_phrases)
                    
                    # åªä¿ç•™é«˜é¢¨éšªè²¼æ–‡
                    if scam_result['scam_score'] >= threshold:
                        result_row = row.copy()
                        result_row['scam_score'] = scam_result['scam_score']
                        result_row['is_potential_scam'] = scam_result['is_potential_scam']
                        result_row['top_matches'] = scam_result['top_matches']
                        results.append(result_row)
                        
                    # æ¯è™•ç† 10 ç­†æª¢æŸ¥ä¸€æ¬¡è¨˜æ†¶é«”
                    if (idx + 1) % 10 == 0:
                        current_memory = get_memory_usage()
                        logger.info(f"å·²è™•ç† {idx + 1}/{len(df)} ç­†ï¼Œè¨˜æ†¶é«”ä½¿ç”¨: {current_memory['rss_mb']:.1f} MB")
                        
                        # å¼·åˆ¶åƒåœ¾å›æ”¶
                        gc.collect()
                        
                except Exception as e:
                    logger.error(f"è™•ç†è²¼æ–‡ {row['pos_tid']} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    continue
                    
            if results:
                final_results = pd.DataFrame(results)
                final_results = final_results.sort_values('scam_score', ascending=False)
                
                logger.info(f"ğŸ¯ æª¢æ¸¬å®Œæˆï¼åœ¨ {len(df)} ç­†æ¨£æœ¬ä¸­ç™¼ç¾ {len(final_results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡")
                
                # è¼¸å‡ºçµæœ
                if output_file:
                    # è™•ç† top_matches æ¬„ä½
                    output_df = final_results.copy()
                    output_df['top_matches'] = output_df['top_matches'].apply(
                        lambda x: '; '.join([f"{item['phrase']}({item['similarity']:.3f})" for item in x])
                    )
                    output_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"çµæœå·²å„²å­˜åˆ°: {output_file}")
                    
                return final_results
            else:
                logger.info("æ²’æœ‰ç™¼ç¾å¯ç–‘çš„è©é¨™è²¼æ–‡")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"æ¨£æœ¬æª¢æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_embeddings': len(self.pos_tid_to_index),
            'embedding_dimension': self.embedding_dim,
            'embeddings_file_size_mb': os.path.getsize(self.embeddings_file) / 1024 / 1024 if os.path.exists(self.embeddings_file) else 0,
            'model_name': self.metadata.get('model_name', 'Unknown'),
            'last_updated': self.metadata.get('last_updated', 'Unknown')
        }
        return stats

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description='è¼•é‡ç´šè©é¨™è²¼æ–‡æª¢æ¸¬å·¥å…·')
    parser.add_argument('--sample-size', type=int, default=100, help='æ¨£æœ¬å¤§å°')
    parser.add_argument('--threshold', type=float, default=0.6, help='è©é¨™é¢¨éšªé–¾å€¼')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--scam-phrases', nargs='+', help='è‡ªå®šç¾©è©é¨™æç¤ºè©')
    parser.add_argument('--embeddings-dir', type=str, default='embeddings_data', help='Embeddings å­˜å„²ç›®éŒ„')
    
    args = parser.parse_args()
    
    try:
        # å‰µå»ºæª¢æ¸¬å™¨
        detector = ScamDetectorLightweight(embeddings_dir=args.embeddings_dir)
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = detector.get_statistics()
        logger.info(f"ç³»çµ±çµ±è¨ˆ: {stats}")
        
        # æ¨£æœ¬æª¢æ¸¬
        results = detector.detect_scam_sample(
            sample_size=args.sample_size,
            scam_phrases=args.scam_phrases,
            threshold=args.threshold,
            output_file=args.output
        )
        
        if not results.empty:
            print(f"\nğŸ¯ ç™¼ç¾ {len(results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡:")
            print("\nå‰ 10 ç­†çµæœ:")
            for idx, row in results.head(10).iterrows():
                print(f"\n--- è²¼æ–‡ {idx + 1} ---")
                print(f"ID: {row['pos_tid']}")
                print(f"é é¢: {row['page_name']}")
                print(f"è©é¨™åˆ†æ•¸: {row['scam_score']:.3f}")
                print(f"å…§å®¹: {row['content'][:100]}...")
                
                print("æœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª:")
                for i, match in enumerate(row['top_matches'][:3], 1):
                    print(f"  {i}. {match['phrase']}: {match['similarity']:.3f}")
        else:
            print("æ²’æœ‰ç™¼ç¾å¯ç–‘çš„è©é¨™è²¼æ–‡")
            
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == "__main__":
    main() 