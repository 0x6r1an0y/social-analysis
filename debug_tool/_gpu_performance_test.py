import time
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
import logging

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_device_performance():
    """æ¸¬è©¦ä¸åŒè£ç½®çš„ embedding ç”Ÿæˆæ€§èƒ½"""
    
    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "é€™æ˜¯ä¸€å€‹æ¸¬è©¦æ–‡æœ¬ï¼Œç”¨æ–¼è©•ä¼°æ¨¡å‹æ€§èƒ½ã€‚",
        "Machine learning is transforming the way we analyze data.",
        "äººå·¥æ™ºæ…§åœ¨å„å€‹é ˜åŸŸéƒ½æœ‰å»£æ³›çš„æ‡‰ç”¨ã€‚",
        "The quick brown fox jumps over the lazy dog.",
        "æ·±åº¦å­¸ç¿’æ¨¡å‹éœ€è¦å¤§é‡çš„è¨ˆç®—è³‡æºã€‚"
    ] * 100  # 500 å€‹æ–‡æœ¬
    
    model_name = "paraphrase-multilingual-MiniLM-L12-v2"
    
    # æ¸¬è©¦çµæœ
    results = {}
    
    # æ¸¬è©¦ CPU
    logger.info("ğŸ–¥ï¸ æ¸¬è©¦ CPU æ€§èƒ½...")
    try:
        cpu_model = SentenceTransformer(model_name, device='cpu')
        
        start_time = time.time()
        cpu_embeddings = cpu_model.encode(test_texts, batch_size=16, show_progress_bar=False)
        cpu_time = time.time() - start_time
        
        results['cpu'] = {
            'time': cpu_time,
            'texts_per_second': len(test_texts) / cpu_time,
            'embedding_shape': cpu_embeddings.shape
        }
        
        logger.info(f"âœ… CPU æ¸¬è©¦å®Œæˆ: {cpu_time:.2f}ç§’, {results['cpu']['texts_per_second']:.2f} texts/ç§’")
        
    except Exception as e:
        logger.error(f"CPU æ¸¬è©¦å¤±æ•—: {str(e)}")
        results['cpu'] = {'error': str(e)}
    
    # æ¸¬è©¦ GPU (å¦‚æœå¯ç”¨)
    if torch.cuda.is_available():
        logger.info("ğŸ® æ¸¬è©¦ GPU æ€§èƒ½...")
        try:
            gpu_model = SentenceTransformer(model_name, device='cuda')
            
            # é ç†± GPU
            logger.info("é ç†± GPU...")
            gpu_model.encode(test_texts[:10])
            torch.cuda.empty_cache()
            
            start_time = time.time()
            gpu_embeddings = gpu_model.encode(test_texts, batch_size=32, show_progress_bar=False)
            gpu_time = time.time() - start_time
            
            results['gpu'] = {
                'time': gpu_time,
                'texts_per_second': len(test_texts) / gpu_time,
                'embedding_shape': gpu_embeddings.shape,
                'gpu_name': torch.cuda.get_device_name(),
                'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
            
            logger.info(f"âœ… GPU æ¸¬è©¦å®Œæˆ: {gpu_time:.2f}ç§’, {results['gpu']['texts_per_second']:.2f} texts/ç§’")
            
            # è¨ˆç®—åŠ é€Ÿæ¯”
            if 'cpu' in results and 'time' in results['cpu']:
                speedup = results['cpu']['time'] / gpu_time
                results['speedup'] = speedup
                logger.info(f"ğŸš€ GPU åŠ é€Ÿæ¯”: {speedup:.2f}x")
                
            # GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
            memory_allocated = torch.cuda.memory_allocated() / 1024**2
            memory_cached = torch.cuda.memory_reserved() / 1024**2
            logger.info(f"GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_allocated:.1f} MB (å·²åˆ†é…) / {memory_cached:.1f} MB (å·²å¿«å–)")
            
        except Exception as e:
            logger.error(f"GPU æ¸¬è©¦å¤±æ•—: {str(e)}")
            results['gpu'] = {'error': str(e)}
    else:
        logger.warning("âš ï¸ ç³»çµ±ä¸æ”¯æ´ CUDAï¼Œè·³é GPU æ¸¬è©¦")
        results['gpu'] = {'error': 'CUDA not available'}
    
    return results

def benchmark_batch_sizes():
    """æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½å½±éŸ¿ (åƒ… GPU)"""
    if not torch.cuda.is_available():
        logger.warning("GPU ä¸å¯ç”¨ï¼Œè·³éæ‰¹æ¬¡å¤§å°æ¸¬è©¦")
        return {}
        
    logger.info("ğŸ”§ æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ€§èƒ½...")
    
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2", device='cuda')
    test_texts = ["æ¸¬è©¦æ–‡æœ¬ " + str(i) for i in range(200)]
    
    batch_sizes = [8, 16, 32, 64, 128]
    results = {}
    
    for batch_size in batch_sizes:
        try:
            torch.cuda.empty_cache()
            
            start_time = time.time()
            embeddings = model.encode(test_texts, batch_size=batch_size, show_progress_bar=False)
            elapsed_time = time.time() - start_time
            
            results[batch_size] = {
                'time': elapsed_time,
                'texts_per_second': len(test_texts) / elapsed_time
            }
            
            logger.info(f"æ‰¹æ¬¡å¤§å° {batch_size}: {elapsed_time:.2f}ç§’, {results[batch_size]['texts_per_second']:.2f} texts/ç§’")
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡å¤§å° {batch_size} æ¸¬è©¦å¤±æ•—: {str(e)}")
            results[batch_size] = {'error': str(e)}
    
    return results

def main():
    """ä¸»è¦æ¸¬è©¦å‡½æ•¸"""
    logger.info("ğŸ§ª é–‹å§‹ GPU æ€§èƒ½æ¸¬è©¦...")
    
    # ç³»çµ±è³‡è¨Š
    logger.info("ğŸ“Š ç³»çµ±è³‡è¨Š:")
    logger.info(f"   - PyTorch ç‰ˆæœ¬: {torch.__version__}")
    logger.info(f"   - CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"   - CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"   - GPU æ•¸é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            logger.info(f"   - GPU {i}: {torch.cuda.get_device_name(i)}")
    
    print("\n" + "="*50)
    
    # æ€§èƒ½æ¸¬è©¦
    performance_results = test_device_performance()
    
    print("\n" + "="*50)
    print("ğŸ“Š æ€§èƒ½æ¸¬è©¦çµæœæ‘˜è¦:")
    print("="*50)
    
    for device, result in performance_results.items():
        if device == 'speedup':
            continue
            
        print(f"\n{device.upper()}:")
        if 'error' in result:
            print(f"  âŒ éŒ¯èª¤: {result['error']}")
        else:
            print(f"  â±ï¸ æ™‚é–“: {result['time']:.2f} ç§’")
            print(f"  ğŸš€ é€Ÿåº¦: {result['texts_per_second']:.2f} texts/ç§’")
            if 'gpu_name' in result:
                print(f"  ğŸ® GPU: {result['gpu_name']}")
                print(f"  ğŸ’¾ è¨˜æ†¶é«”: {result['gpu_memory_gb']:.1f} GB")
    
    if 'speedup' in performance_results:
        print(f"\nğŸš€ GPU åŠ é€Ÿæ¯”: {performance_results['speedup']:.2f}x å€å¿«")
    
    # æ‰¹æ¬¡å¤§å°æ¸¬è©¦
    if torch.cuda.is_available():
        print("\n" + "="*50)
        batch_results = benchmark_batch_sizes()
        
        if batch_results:
            print("ğŸ”§ æœ€ä½³æ‰¹æ¬¡å¤§å°å»ºè­°:")
            best_batch = max(batch_results.items(), 
                           key=lambda x: x[1].get('texts_per_second', 0) if 'error' not in x[1] else 0)
            print(f"   æ¨è–¦æ‰¹æ¬¡å¤§å°: {best_batch[0]} (é€Ÿåº¦: {best_batch[1]['texts_per_second']:.2f} texts/ç§’)")
    
    print("\n" + "="*50)
    print("ğŸ’¡ ä½¿ç”¨å»ºè­°:")
    if torch.cuda.is_available():
        print("  âœ… æ‚¨çš„ç³»çµ±æ”¯æ´ GPU åŠ é€Ÿï¼")
        print("  ğŸ“ ä¿®æ”¹ç¨‹å¼ç¢¼æ™‚ä½¿ç”¨ device='cuda' æˆ– device='auto'")
        print("  ğŸ”§ å»ºè­°ä½¿ç”¨è¼ƒå¤§çš„æ‰¹æ¬¡å¤§å° (32-64)")
        print("  âš¡ é æœŸå¯ç²å¾— 2-10x çš„æ€§èƒ½æå‡")
    else:
        print("  âš ï¸ æ‚¨çš„ç³»çµ±ä¸æ”¯æ´ GPU åŠ é€Ÿ")
        print("  ğŸ“ éœ€è¦å®‰è£ CUDA å’Œ GPU ç‰ˆæœ¬çš„ PyTorch")
        print("  ğŸ’» æˆ–ç¹¼çºŒä½¿ç”¨ CPU (device='cpu')")

if __name__ == "__main__":
    main() 