import torch
from sentence_transformers import SentenceTransformer
import time

def test_large_batch_sizes():
    print('ğŸ§ª æ¸¬è©¦å¤§æ‰¹æ¬¡å¤§å°æ€§èƒ½...')
    print(f'GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
    
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device='cuda')
    test_texts = ['æ¸¬è©¦æ–‡æœ¬ ' + str(i) for i in range(2048)]  # æº–å‚™æ›´å¤šæ¸¬è©¦æ–‡æœ¬
    
    batch_sizes = [128, 256, 512, 1024, 2048]
    results = {}
    
    for batch_size in batch_sizes:
        try:
            torch.cuda.empty_cache()
            print(f'\næ¸¬è©¦æ‰¹æ¬¡å¤§å°: {batch_size}')
            
            # åªä½¿ç”¨ç›¸æ‡‰æ•¸é‡çš„æ–‡æœ¬ä¾†æ¸¬è©¦
            current_texts = test_texts[:batch_size]
            
            start_time = time.time()
            embeddings = model.encode(current_texts, batch_size=batch_size, show_progress_bar=False)
            elapsed_time = time.time() - start_time
            
            texts_per_second = len(current_texts) / elapsed_time
            memory_used = torch.cuda.memory_allocated() / 1024**2
            memory_cached = torch.cuda.memory_reserved() / 1024**2
            
            print(f'  â±ï¸ æ™‚é–“: {elapsed_time:.3f}ç§’')
            print(f'  ğŸš€ é€Ÿåº¦: {texts_per_second:.1f} texts/ç§’')
            print(f'  ğŸ’¾ è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.1f} MB (å·²åˆ†é…) / {memory_cached:.1f} MB (å·²å¿«å–)')
            
            results[batch_size] = texts_per_second
            
        except Exception as e:
            print(f'  âŒ éŒ¯èª¤: {str(e)}')
            results[batch_size] = 0
    
    print(f'\nğŸ† æœ€ä½³æ‰¹æ¬¡å¤§å°: {max(results, key=results.get)} ({max(results.values()):.1f} texts/ç§’)')
    
    # åˆ†æçµæœ
    print('\nğŸ“Š è©³ç´°åˆ†æ:')
    for batch_size, speed in sorted(results.items()):
        if speed > 0:
            efficiency = speed / batch_size  # æ¯å€‹æ–‡æœ¬çš„è™•ç†æ•ˆç‡
            print(f'  æ‰¹æ¬¡ {batch_size:4d}: {speed:8.1f} texts/ç§’ (æ•ˆç‡: {efficiency:.3f})')
    
    return results

def test_embedding_generator_batch_size():
    """æ¸¬è©¦åœ¨å¯¦éš› embedding generator ä¸­ä½¿ç”¨å¤§æ‰¹æ¬¡å¤§å°"""
    try:
        from embedding_generator_memmap import EmbeddingGeneratorMemmap
        
        print('\nğŸ”§ æ¸¬è©¦ EmbeddingGeneratorMemmap çš„å¤§æ‰¹æ¬¡è™•ç†...')
        
        # æ¸¬è©¦ä¸åŒ batch_size è¨­å®š
        for batch_size in [512, 1024, 2048]:
            try:
                print(f'\næ¸¬è©¦ batch_size={batch_size}:')
                
                generator = EmbeddingGeneratorMemmap(
                    batch_size=batch_size,
                    device="cuda"
                )
                
                print(f'  âœ… æˆåŠŸå‰µå»º generator (batch_size={batch_size})')
                print(f'  ğŸ“± ä½¿ç”¨è£ç½®: {generator.device}')
                
                # æ¸¬è©¦é€£æ¥
                if generator.test_connection():
                    print(f'  âœ… é€£æ¥æ¸¬è©¦é€šé')
                else:
                    print(f'  âŒ é€£æ¥æ¸¬è©¦å¤±æ•—')
                    
            except Exception as e:
                print(f'  âŒ batch_size={batch_size} å¤±æ•—: {str(e)}')
                
    except ImportError:
        print('âš ï¸ ç„¡æ³•å°å…¥ EmbeddingGeneratorMemmapï¼Œè·³éå¯¦éš›æ¸¬è©¦')

if __name__ == "__main__":
    if torch.cuda.is_available():
        test_large_batch_sizes()
        test_embedding_generator_batch_size()
    else:
        print('âŒ GPU ä¸å¯ç”¨ï¼Œç„¡æ³•æ¸¬è©¦å¤§æ‰¹æ¬¡å¤§å°') 