#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è©é¨™è²¼æ–‡æª¢æ¸¬å·¥å…· (Memmap ç‰ˆæœ¬)

åƒæ•¸èª¿æ•´æŒ‡å—ï¼š
1. batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
   - 100-200: è¨˜æ†¶é«”æœ‰é™ç’°å¢ƒ
   - 500-1000: ä¸€èˆ¬ä½¿ç”¨ï¼ˆæ¨è–¦ï¼‰
   - 2000-5000: è¨˜æ†¶é«”å……è¶³ç’°å¢ƒ
   - 10000+: é«˜æ€§èƒ½ç’°å¢ƒ

2. èª¿æ•´æ–¹å¼ï¼š
   - å‘½ä»¤åˆ—: --batch-size 500
   - ç¨‹å¼ç¢¼: batch_size=500
   - é è¨­å€¼: 1024

3. è¨˜æ†¶é«”ç›£æ§ï¼š
   - ç¨‹å¼æœƒè‡ªå‹•ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨
   - è¶…é 85% æ™‚æœƒå¼·åˆ¶åƒåœ¾å›æ”¶
   - å»ºè­°å…ˆæ¸¬è©¦è¼ƒå°æ‰¹æ¬¡å¤§å°
"""
from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
import logging
import os
import json
import argparse
from typing import List, Dict, Optional
import time
import psutil  # æ·»åŠ è¨˜æ†¶é«”ç›£æ§
import gc  # æ·»åŠ åƒåœ¾å›æ”¶

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,  # RSS in MB
        'vms_mb': memory_info.vms / 1024 / 1024,  # VMS in MB
        'percent': process.memory_percent()
    }

class ScamDetectorMemmap:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 1024,  # ä¿®æ”¹ç‚ºåˆç†çš„é è¨­å€¼
                 embeddings_dir: str = "embeddings_data",
                 memory_optimized: bool = True):  # æ–°å¢è¨˜æ†¶é«”å„ªåŒ–é¸é …
        """
        åˆå§‹åŒ–è©é¨™æª¢æ¸¬å™¨ (ä½¿ç”¨ memmap å­˜å„²)
        
        Args:
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²
            model_name: ä½¿ç”¨çš„ sentence-transformers æ¨¡å‹
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°ï¼ˆå»ºè­° 200-1000ï¼‰
            embeddings_dir: embeddings å­˜å„²ç›®éŒ„
            memory_optimized: æ˜¯å¦å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.embeddings_dir = embeddings_dir
        self.memory_optimized = memory_optimized
        self.engine = None
        self.model = None
        self.embeddings_array = None  # å…¨å±€ memmap ç‰©ä»¶
        
        # é è¨­è©é¨™æç¤ºè©
        self.default_scam_phrases = [
            "åŠ å…¥LINE", "åŠ å…¥Telegram", "å¿«é€Ÿè³ºéŒ¢", "è¢«å‹•æ”¶å…¥", 
            "æŠ•è³‡åŒ…ä½ è³º", "ç§è¨Šæˆ‘", "è€å¸«å¸¶å–®", "ç©©è³ºä¸è³ ",
            "è¼•é¬†è³ºéŒ¢", "ä¸€å¤©è³ºè¬å…ƒ", "ä¿è­‰ç²åˆ©", "é«˜å ±é…¬ä½é¢¨éšª",
            "åŠ ç¾¤çµ„", "è·Ÿå–®", "æ“ç›¤æ‰‹", "è²¡å¯Œè‡ªç”±",
            "æœˆæ”¶å…¥", "å…¼è·è³ºéŒ¢", "åœ¨å®¶è³ºéŒ¢", "ç¶²è·¯è³ºéŒ¢",
            "æŠ•è³‡ç†è²¡", "è™›æ“¬è²¨å¹£", "æ¯”ç‰¹å¹£", "æŒ–ç¤¦",
            "å€Ÿè²¸", "å°é¡è²¸æ¬¾", "æ€¥ç”¨éŒ¢", "å…æŠµæŠ¼",
            "ä»£è¾¦ä¿¡è²¸", "ä¿¡ç”¨å¡ä»£å„Ÿ", "å‚µå‹™æ•´åˆ"
        ]
        
        # æª”æ¡ˆè·¯å¾‘
        self.embeddings_file = os.path.join(embeddings_dir, "embeddings.dat")
        self.index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        self.metadata_file = os.path.join(embeddings_dir, "metadata.json")
        
        # åˆå§‹åŒ–
        self._init_db_connection()
        self._load_model(model_name)
        self._load_embeddings_metadata()
        self._init_embeddings_memmap()  # æ·»åŠ å…¨å±€ memmap åˆå§‹åŒ–
        
    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(self.db_url)
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _load_model(self, model_name: str):
        """è¼‰å…¥æ¨¡å‹"""
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def _load_embeddings_metadata(self):
        """è¼‰å…¥ embeddings metadata"""
        try:
            # è¼‰å…¥ç´¢å¼•
            if not os.path.exists(self.index_file):
                raise FileNotFoundError(f"ç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨: {self.index_file}")
                
            with open(self.index_file, 'r', encoding='utf-8') as f:
                self.pos_tid_to_index = json.load(f)
                
            # è¼‰å…¥ metadata
            if not os.path.exists(self.metadata_file):
                raise FileNotFoundError(f"Metadata æª”æ¡ˆä¸å­˜åœ¨: {self.metadata_file}")
                
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
                
            self.embedding_dim = self.metadata['embedding_dim']
            self.total_embeddings = self.metadata['total_embeddings']
            
            logger.info(f"è¼‰å…¥ embeddings metadataï¼š")
            logger.info(f"  - ç¸½è¨˜éŒ„æ•¸: {self.total_embeddings}")
            logger.info(f"  - Embedding ç¶­åº¦: {self.embedding_dim}")
            logger.info(f"  - æœ€å¾Œæ›´æ–°: {self.metadata.get('last_updated', 'Unknown')}")
            
            # æª¢æŸ¥ embeddings æª”æ¡ˆ
            if not os.path.exists(self.embeddings_file):
                raise FileNotFoundError(f"Embeddings æª”æ¡ˆä¸å­˜åœ¨: {self.embeddings_file}")
                
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings metadata å¤±æ•—: {str(e)}")
            raise
            
    def _init_embeddings_memmap(self):
        """åˆå§‹åŒ– embeddings memmap"""
        try:
            # ç›´æ¥ä½¿ç”¨ metadata ä¸­çš„ç¸½è¨˜éŒ„æ•¸ï¼Œé¿å…é‡è¤‡æŸ¥è©¢è³‡æ–™åº«
            total_records = self.total_embeddings
            
            # ä½¿ç”¨ memmap è¼‰å…¥ embeddings æª”æ¡ˆ
            self.embeddings_array = np.memmap(
                self.embeddings_file,
                dtype=np.float32,
                mode='r',
                shape=(total_records, self.embedding_dim)
            )
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– embeddings memmap å¤±æ•—: {str(e)}")
            raise
            
    def _get_embeddings_array(self) -> np.ndarray:
        """ç²å– embeddings memmap é™£åˆ—"""
        try:
            # ç›´æ¥ä½¿ç”¨ metadata ä¸­çš„ç¸½è¨˜éŒ„æ•¸ï¼Œé¿å…é‡è¤‡æŸ¥è©¢è³‡æ–™åº«
            total_records = self.total_embeddings
            
            # ä½¿ç”¨ memmap è¼‰å…¥ embeddings æª”æ¡ˆ
            embeddings_array = np.memmap(
                self.embeddings_file,
                dtype=np.float32,
                mode='r',
                shape=(total_records, self.embedding_dim)
            )
            
            return embeddings_array
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings é™£åˆ—å¤±æ•—: {str(e)}")
            raise
            
    def _get_posts_batch(self, offset: int = 0, limit: Optional[int] = None) -> pd.DataFrame:
        """ç²å–æ‰¹æ¬¡è²¼æ–‡è³‡æ–™ï¼ˆåŒ…å«æœ‰ embeddings çš„è²¼æ–‡ï¼‰"""
        try:
            if limit is None:
                limit = self.batch_size  # ä½¿ç”¨å¯¦éš›çš„ batch_sizeï¼Œç§»é™¤ 50 ç­†é™åˆ¶
                
            # åªç²å–æœ‰ embeddings çš„è²¼æ–‡
            valid_pos_tids = list(self.pos_tid_to_index.keys())
            
            if not valid_pos_tids:
                return pd.DataFrame()
                
            # åˆ†æ‰¹ç²å–
            start_idx = offset
            end_idx = min(offset + limit, len(valid_pos_tids))
            batch_pos_tids = valid_pos_tids[start_idx:end_idx]
            
            if not batch_pos_tids:
                return pd.DataFrame()
                
            # æ§‹å»º SQL æŸ¥è©¢ - æ”¹ç‚ºå¾ posts_deduplicated è¡¨æŸ¥è©¢
            placeholders = ','.join([f"'{pid}'" for pid in batch_pos_tids])
            sql = f"""
                SELECT pos_tid, content, page_name, created_time
                FROM posts_deduplicated 
                WHERE pos_tid IN ({placeholders})
                ORDER BY pos_tid
            """
            
            df = pd.read_sql_query(text(sql), self.engine)
            return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_embeddings_for_pos_tids_optimized(self, pos_tids: List[str], batch_size: int = 100) -> Dict[str, np.ndarray]:
        """å„ªåŒ–ç‰ˆæœ¬ï¼šåˆ†æ‰¹ç²å–æŒ‡å®š pos_tids çš„ embeddingsï¼ˆä½¿ç”¨å…¨å±€ memmapï¼‰"""
        try:
            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
            current_memory = get_memory_usage()
            if current_memory['percent'] > 85:
                logger.warning(f"è¨˜æ†¶é«”ä½¿ç”¨éé«˜: {current_memory['percent']:.1f}%ï¼Œå¼·åˆ¶åƒåœ¾å›æ”¶")
                gc.collect()
            
            result = {}
            
            # ä½¿ç”¨å…¨å±€ memmap ç‰©ä»¶
            for pos_tid in pos_tids:
                if pos_tid in self.pos_tid_to_index:
                    index = self.pos_tid_to_index[pos_tid]
                    result[pos_tid] = self.embeddings_array[index].copy()
                    
            return result
            
        except Exception as e:
            logger.error(f"ç²å– embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def calculate_scam_scores_with_top_k(self, 
                                        pos_tids: List[str],
                                        content_embeddings: Dict[str, np.ndarray],
                                        scam_phrases: Optional[List[str]] = None,
                                        top_k: int = 5) -> Dict[str, Dict]:
        """
        è¨ˆç®—è©é¨™åˆ†æ•¸ä¸¦è¿”å› top_k ç›¸ä¼¼åº¦
        
        Args:
            pos_tids: è²¼æ–‡ ID åˆ—è¡¨
            content_embeddings: è²¼æ–‡ embeddings å­—å…¸
            scam_phrases: è©é¨™æç¤ºè©åˆ—è¡¨
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çŸ­èª
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸å’Œ top_k ç›¸ä¼¼åº¦çš„å­—å…¸
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            # ç²å–æ¨¡å‹æ‰€åœ¨çš„ device
            device = phrase_embeddings.device
            
            results = {}
            
            for pos_tid in pos_tids:
                if pos_tid not in content_embeddings:
                    results[pos_tid] = {
                        'scam_score': 0.0,
                        'top_k_similarities': []
                    }
                    continue
                    
                content_emb = content_embeddings[pos_tid]
                
                # ç›´æ¥ä½¿ç”¨é è¨ˆç®—çš„ embedding ä¾†è¨ˆç®—ç›¸ä¼¼åº¦ï¼ŒæŒ‡å®šæ­£ç¢ºçš„ device
                from torch import tensor
                content_tensor = tensor(content_emb, device=device).unsqueeze(0)
                similarities = util.cos_sim(content_tensor, phrase_embeddings).squeeze().cpu().numpy()
                
                # ç²å– top_k ç›¸ä¼¼åº¦
                top_indices = np.argsort(similarities)[::-1][:top_k]
                top_similarities = []
                
                for idx in top_indices:
                    top_similarities.append({
                        'phrase': scam_phrases[idx],
                        'similarity': float(similarities[idx])
                    })
                
                # å–æœ€å¤§ç›¸ä¼¼åº¦ä½œç‚ºè©é¨™åˆ†æ•¸
                score = float(np.max(similarities))
                
                results[pos_tid] = {
                    'scam_score': score,
                    'top_k_similarities': top_similarities
                }
                
            return results
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è©é¨™åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise

    def calculate_scam_scores(self, 
                            pos_tids: List[str],
                            content_embeddings: Dict[str, np.ndarray],
                            scam_phrases: Optional[List[str]] = None) -> Dict[str, float]:
        """
        è¨ˆç®—è©é¨™åˆ†æ•¸
        
        Args:
            pos_tids: è²¼æ–‡ ID åˆ—è¡¨
            content_embeddings: è²¼æ–‡ embeddings å­—å…¸
            scam_phrases: è©é¨™æç¤ºè©åˆ—è¡¨
            
        Returns:
            è©é¨™é¢¨éšªåˆ†æ•¸å­—å…¸
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            # ç²å–æ¨¡å‹æ‰€åœ¨çš„ device
            device = phrase_embeddings.device
            
            scam_scores = {}
            
            for pos_tid in pos_tids:
                if pos_tid not in content_embeddings:
                    scam_scores[pos_tid] = 0.0
                    continue
                    
                content_emb = content_embeddings[pos_tid]
                
                # ç›´æ¥ä½¿ç”¨é è¨ˆç®—çš„ embedding ä¾†è¨ˆç®—ç›¸ä¼¼åº¦ï¼ŒæŒ‡å®šæ­£ç¢ºçš„ device
                from torch import tensor
                content_tensor = tensor(content_emb, device=device).unsqueeze(0)
                similarities = util.cos_sim(content_tensor, phrase_embeddings).squeeze().cpu().numpy()
                
                # å–æœ€å¤§ç›¸ä¼¼åº¦ä½œç‚ºè©é¨™åˆ†æ•¸
                score = float(np.max(similarities))
                scam_scores[pos_tid] = score
                
            return scam_scores
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è©é¨™åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_scam_in_batch(self, 
                           scam_phrases: Optional[List[str]] = None,
                           threshold: float = 0.6,
                           output_file: Optional[str] = None,
                           max_results: Optional[int] = None,
                           top_k: int = 5,
                           return_top_k: bool = False) -> pd.DataFrame:
        """
        æ‰¹æ¬¡æª¢æ¸¬è©é¨™è²¼æ–‡
        
        Args:
            scam_phrases: è‡ªå®šç¾©è©é¨™æç¤ºè©
            threshold: è©é¨™é¢¨éšªé–¾å€¼
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            max_results: æœ€å¤§çµæœæ•¸é‡
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çŸ­èª
            return_top_k: æ˜¯å¦è¿”å› top_k ç›¸ä¼¼åº¦
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸çš„ DataFrame
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        logger.info(f"ä½¿ç”¨çš„è©é¨™æç¤ºè©: {scam_phrases}")
        logger.info(f"è©é¨™é¢¨éšªé–¾å€¼: {threshold}")
        if return_top_k:
            logger.info(f"å°‡è¿”å›å‰ {top_k} å€‹æœ€ç›¸ä¼¼çš„çŸ­èª")
        
        try:
            total_posts = len(self.pos_tid_to_index)
            logger.info(f"å¾…æª¢æ¸¬çš„è²¼æ–‡ç¸½æ•¸: {total_posts}")
            
            # é¡¯ç¤ºåˆå§‹è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
            initial_memory = get_memory_usage()
            logger.info(f"åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨: {initial_memory['rss_mb']:.1f} MB ({initial_memory['percent']:.1f}%)")
            
            all_results = []
            processed = 0
            offset = 0
            
            while processed < total_posts:
                start_time = time.time()
                
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_batch(offset)
                
                if df.empty:
                    break
                    
                logger.info(f"æ­£åœ¨è™•ç†ç¬¬ {processed + 1} åˆ° {processed + len(df)} ç­†è³‡æ–™...")
                
                # ç²å–é€™æ‰¹æ¬¡çš„ embeddings
                pos_tids = df['pos_tid'].tolist()
                embeddings_dict = self._get_embeddings_for_pos_tids_optimized(pos_tids)
                
                # è¨ˆç®—è©é¨™åˆ†æ•¸
                if return_top_k:
                    scam_scores_dict = self.calculate_scam_scores_with_top_k(
                        pos_tids,
                        embeddings_dict,
                        scam_phrases,
                        top_k
                    )
                    
                    # æ·»åŠ åˆ†æ•¸å’Œ top_k ç›¸ä¼¼åº¦åˆ° DataFrame
                    df['scam_score'] = df['pos_tid'].map(lambda x: scam_scores_dict.get(x, {}).get('scam_score', 0.0))
                    df['top_k_similarities'] = df['pos_tid'].map(lambda x: scam_scores_dict.get(x, {}).get('top_k_similarities', []))
                else:
                    scam_scores_dict = self.calculate_scam_scores(
                        pos_tids,
                        embeddings_dict,
                        scam_phrases
                    )
                    df['scam_score'] = df['pos_tid'].map(scam_scores_dict).fillna(0.0)
                
                df['is_potential_scam'] = df['scam_score'] >= threshold
                
                # åªä¿ç•™å¯èƒ½çš„è©é¨™è²¼æ–‡
                high_risk_posts = df[df['is_potential_scam']].copy()
                
                if not high_risk_posts.empty:
                    all_results.append(high_risk_posts)
                    
                processed += len(df)
                offset += len(df)
                
                # å¦‚æœé”åˆ°æœ€å¤§çµæœæ•¸é‡ï¼Œæå‰çµæŸ
                if max_results and sum(len(r) for r in all_results) >= max_results:
                    break
                    
                elapsed_time = time.time() - start_time
                logger.info(f"å·²è™•ç† {processed}/{total_posts} ç­†ï¼Œç™¼ç¾ {len(high_risk_posts)} ç­†å¯ç–‘è²¼æ–‡ - è€—æ™‚: {elapsed_time:.2f}ç§’")
                
                # æ¯è™•ç† 10 å€‹æ‰¹æ¬¡æª¢æŸ¥ä¸€æ¬¡è¨˜æ†¶é«”
                if processed % (self.batch_size * 10) == 0:
                    current_memory = get_memory_usage()
                    logger.info(f"å·²è™•ç† {processed}/{total_posts} ç­†ï¼Œè¨˜æ†¶é«”ä½¿ç”¨: {current_memory['rss_mb']:.1f} MB")
                    
                    # å¼·åˆ¶åƒåœ¾å›æ”¶
                    gc.collect()
                
            # åˆä½µæ‰€æœ‰çµæœ
            if all_results:
                final_results = pd.concat(all_results, ignore_index=True)
                final_results = final_results.sort_values('scam_score', ascending=False)
                
                if max_results:
                    final_results = final_results.head(max_results)
                    
                logger.info(f"ğŸ¯ æª¢æ¸¬å®Œæˆï¼å…±ç™¼ç¾ {len(final_results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡")
                
                # è¼¸å‡ºçµæœ
                if output_file:
                    # å¦‚æœåŒ…å« top_k ç›¸ä¼¼åº¦ï¼Œéœ€è¦ç‰¹æ®Šè™•ç†è¼¸å‡º
                    if return_top_k and 'top_k_similarities' in final_results.columns:
                        # å‰µå»ºä¸€å€‹æ–°çš„ DataFrame ä¾†è™•ç† top_k ç›¸ä¼¼åº¦
                        output_df = final_results.copy()
                        # å°‡ top_k_similarities è½‰æ›ç‚ºå¯è®€çš„æ ¼å¼
                        output_df['top_k_similarities'] = output_df['top_k_similarities'].apply(
                            lambda x: '; '.join([f"{item['phrase']}({item['similarity']:.3f})" for item in x])
                        )
                        output_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    else:
                        final_results.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"çµæœå·²å„²å­˜åˆ°: {output_file}")
                    
                return final_results
            else:
                logger.info("æ²’æœ‰ç™¼ç¾å¯ç–‘çš„è©é¨™è²¼æ–‡")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡æª¢æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_single_post(self, content: str, scam_phrases: Optional[List[str]] = None, top_k: int = 5) -> Dict:
        """
        æª¢æ¸¬å–®ä¸€è²¼æ–‡
        
        Args:
            content: è²¼æ–‡å…§å®¹
            scam_phrases: è©é¨™æç¤ºè©
            top_k: è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çŸ­èª
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸å’Œè©³ç´°è³‡è¨Šçš„å­—å…¸
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆå…§å®¹ embedding
            content_embedding = self.model.encode(content, convert_to_tensor=True)
            
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarities = util.cos_sim(content_embedding, phrase_embeddings).squeeze().cpu().numpy()
            scam_score = float(np.max(similarities))
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª
            top_matches = []
            for i, phrase in enumerate(scam_phrases):
                top_matches.append({
                    'phrase': phrase,
                    'similarity': float(similarities[i])
                })
                
            top_matches.sort(key=lambda x: x['similarity'], reverse=True)
            
            result = {
                'content': content,
                'scam_score': scam_score,
                'is_potential_scam': scam_score >= 0.6,
                'top_matching_phrases': top_matches[:top_k],
                'risk_level': self._get_risk_level(scam_score)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"æª¢æ¸¬å–®ä¸€è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_risk_level(self, score: float) -> str:
        """æ ¹æ“šåˆ†æ•¸ç²å–é¢¨éšªç­‰ç´š"""
        if score >= 0.8:
            return "æ¥µé«˜é¢¨éšª"
        elif score >= 0.6:
            return "é«˜é¢¨éšª"
        elif score >= 0.4:
            return "ä¸­ç­‰é¢¨éšª"
        elif score >= 0.2:
            return "ä½é¢¨éšª"
        else:
            return "ç„¡é¢¨éšª"
            
    def search_by_keywords(self, 
                          keywords: List[str], 
                          limit: int = 100,
                          threshold: float = 0.5) -> pd.DataFrame:
        """
        ä½¿ç”¨é—œéµå­—é€²è¡Œèªæ„æœå°‹
        
        Args:
            keywords: æœå°‹é—œéµå­—
            limit: è¿”å›çµæœæ•¸é‡
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            æœå°‹çµæœ DataFrame
        """
        try:
            # ç”Ÿæˆé—œéµå­— embeddings
            keyword_embeddings = self.model.encode(keywords, convert_to_tensor=True)
            
            # ç²å–æ¨¡å‹æ‰€åœ¨çš„ device
            device = keyword_embeddings.device
            
            results = []
            processed = 0
            offset = 0
            
            while len(results) < limit and processed < len(self.pos_tid_to_index):
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_batch(offset, self.batch_size)
                
                if df.empty:
                    break
                    
                # ç²å– embeddings
                pos_tids = df['pos_tid'].tolist()
                embeddings_dict = self._get_embeddings_for_pos_tids_optimized(pos_tids)
                
                for _, row in df.iterrows():
                    pos_tid = row['pos_tid']
                    if pos_tid not in embeddings_dict:
                        continue
                        
                    # è¨ˆç®—èˆ‡é—œéµå­—çš„ç›¸ä¼¼åº¦ï¼ŒæŒ‡å®šæ­£ç¢ºçš„ device
                    content_emb = embeddings_dict[pos_tid]
                    from torch import tensor
                    content_tensor = tensor(content_emb, device=device).unsqueeze(0)
                    similarities = util.cos_sim(content_tensor, keyword_embeddings).squeeze().cpu().numpy()
                    max_similarity = float(np.max(similarities))
                    
                    if max_similarity >= threshold:
                        result_row = row.copy()
                        result_row['similarity_score'] = max_similarity
                        result_row['matching_keyword'] = keywords[np.argmax(similarities)]
                        results.append(result_row)
                        
                        if len(results) >= limit:
                            break
                            
                processed += len(df)
                offset += len(df)
                
                logger.info(f"å·²è™•ç† {processed} ç­†ï¼Œæ‰¾åˆ° {len(results)} ç­†ç¬¦åˆçš„çµæœ")
                
            if results:
                results_df = pd.DataFrame(results)
                results_df = results_df.sort_values('similarity_score', ascending=False)
                return results_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"é—œéµå­—æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_embeddings': len(self.pos_tid_to_index),
            'embedding_dimension': self.embedding_dim,
            'embeddings_file_size_mb': os.path.getsize(self.embeddings_file) / 1024 / 1024 if os.path.exists(self.embeddings_file) else 0,
            'model_name': self.metadata.get('model_name', 'Unknown'),
            'last_updated': self.metadata.get('last_updated', 'Unknown'),
            'batch_size': self.batch_size
        }
        return stats
        
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        if hasattr(self, 'embeddings_array') and self.embeddings_array is not None:
            del self.embeddings_array
            logger.info("å·²æ¸…ç† memmap è³‡æº")

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description='è©é¨™è²¼æ–‡æª¢æ¸¬å·¥å…· (Memmap ç‰ˆæœ¬)')
    parser.add_argument('--mode', choices=['batch', 'single', 'search'], default='batch',
                       help='åŸ·è¡Œæ¨¡å¼: batch=æ‰¹æ¬¡æª¢æ¸¬, single=å–®ä¸€è²¼æ–‡æª¢æ¸¬, search=é—œéµå­—æœå°‹')
    parser.add_argument('--content', type=str, help='å–®ä¸€è²¼æ–‡å…§å®¹ (single æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--keywords', nargs='+', help='æœå°‹é—œéµå­— (search æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--threshold', type=float, default=0.6, help='è©é¨™é¢¨éšªé–¾å€¼')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--limit', type=int, default=500, help='æœ€å¤§çµæœæ•¸é‡')
    parser.add_argument('--scam-phrases', nargs='+', help='è‡ªå®šç¾©è©é¨™æç¤ºè©')
    parser.add_argument('--embeddings-dir', type=str, default='embeddings_data', help='Embeddings å­˜å„²ç›®éŒ„')
    parser.add_argument('--batch-size', type=int, default=131072, help='æ‰¹æ¬¡å¤§å°ï¼ˆå»ºè­° 200-1000ï¼‰')
    parser.add_argument('--top-k', type=int, default=5, help='è¿”å›å‰ k å€‹æœ€ç›¸ä¼¼çš„çŸ­èª')
    parser.add_argument('--return-top-k', action='store_true', help='æ˜¯å¦è¿”å› top_k ç›¸ä¼¼åº¦ (batch æ¨¡å¼)')
    
    args = parser.parse_args()
    
    try:
        # å‰µå»ºæª¢æ¸¬å™¨
        detector = ScamDetectorMemmap(
            embeddings_dir=args.embeddings_dir,
            batch_size=args.batch_size
        )
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = detector.get_statistics()
        logger.info(f"ç³»çµ±çµ±è¨ˆ: {stats}")
        
        if args.mode == 'batch':
            # æ‰¹æ¬¡æª¢æ¸¬
            results = detector.detect_scam_in_batch(
                scam_phrases=args.scam_phrases,
                threshold=args.threshold,
                output_file=args.output,
                max_results=args.limit,
                top_k=args.top_k,
                return_top_k=args.return_top_k
            )
            
            if not results.empty:
                print(f"\nğŸ¯ ç™¼ç¾ {len(results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡:")
                if args.return_top_k and 'top_k_similarities' in results.columns:
                    # é¡¯ç¤ºåŒ…å« top_k ç›¸ä¼¼åº¦çš„çµæœ
                    display_columns = ['pos_tid', 'page_name', 'scam_score', 'top_k_similarities', 'content']
                    print(results[display_columns].head(10).to_string())
                else:
                    print(results[['pos_tid', 'page_name', 'scam_score', 'content']].head(10).to_string())
                
        elif args.mode == 'single':
            if not args.content:
                print("âŒ è«‹æä¾›è²¼æ–‡å…§å®¹ (--content)")
                return
                
            # å–®ä¸€è²¼æ–‡æª¢æ¸¬
            result = detector.detect_single_post(args.content, args.scam_phrases, args.top_k)
            
            print(f"\nğŸ“ è²¼æ–‡å…§å®¹: {result['content']}")
            print(f"ğŸ¯ è©é¨™åˆ†æ•¸: {result['scam_score']:.3f}")
            print(f"âš ï¸  é¢¨éšªç­‰ç´š: {result['risk_level']}")
            print(f"ğŸš¨ æ˜¯å¦å¯ç–‘: {'æ˜¯' if result['is_potential_scam'] else 'å¦'}")
            print(f"\nğŸ” å‰ {args.top_k} å€‹æœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª:")
            for match in result['top_matching_phrases']:
                print(f"  - {match['phrase']}: {match['similarity']:.3f}")
                
        elif args.mode == 'search':
            if not args.keywords:
                print("âŒ è«‹æä¾›æœå°‹é—œéµå­— (--keywords)")
                return
                
            # é—œéµå­—æœå°‹
            results = detector.search_by_keywords(
                keywords=args.keywords,
                limit=args.limit,
                threshold=args.threshold
            )
            
            if not results.empty:
                print(f"\nğŸ” æ‰¾åˆ° {len(results)} ç­†ç›¸é—œè²¼æ–‡:")
                print(results[['pos_tid', 'page_name', 'similarity_score', 'matching_keyword', 'content']].head(10).to_string())
                
                if args.output:
                    results.to_csv(args.output, index=False, encoding='utf-8-sig')
                    print(f"çµæœå·²å„²å­˜åˆ°: {args.output}")
            else:
                print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œè²¼æ–‡")
                
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == "__main__":
    main() 