from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
import pickle
import logging
from typing import List, Dict, Optional
import argparse

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ScamDetector:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 1000):
        """
        åˆå§‹åŒ–è©é¨™æª¢æ¸¬å™¨
        
        Args:
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²
            model_name: ä½¿ç”¨çš„ sentence-transformers æ¨¡å‹
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.engine = None
        self.model = None
        
        # é è¨­è©é¨™æç¤ºè©
        self.default_scam_phrases = [
            "åŠ å…¥LINE", "åŠ å…¥Telegram", "å¿«é€Ÿè³ºéŒ¢", "è¢«å‹•æ”¶å…¥", 
            "æŠ•è³‡åŒ…ä½ è³º", "ç§è¨Šæˆ‘", "è€å¸«å¸¶å–®", "ç©©è³ºä¸è³ ",
            "è¼•é¬†è³ºéŒ¢", "ä¸€å¤©è³ºè¬å…ƒ", "ä¿è­‰ç²åˆ©", "é«˜å ±é…¬ä½é¢¨éšª",
            "åŠ ç¾¤çµ„", "è·Ÿå–®", "æ“ç›¤æ‰‹", "è²¡å¯Œè‡ªç”±",
            "æœˆæ”¶å…¥", "å…¼è·è³ºéŒ¢", "åœ¨å®¶è³ºéŒ¢", "ç¶²è·¯è³ºéŒ¢",
            "æŠ•è³‡ç†è²¡", "è™›æ“¬è²¨å¹£", "æ¯”ç‰¹å¹£", "æŒ–ç¤¦",
            "å€Ÿè²¸", "å°é¡è²¸æ¬¾", "æ€¥ç”¨éŒ¢", "å…æŠµæŠ¼",
            "ä»£è¾¦ä¿¡è²¸", "ä¿¡ç”¨å¡ä»£å„Ÿ", "å‚µå‹™æ•´åˆ"
        ]
        
        # åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥
        self._init_db_connection()
        
        # è¼‰å…¥æ¨¡å‹
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(self.db_url)
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _load_embeddings_from_db(self, pos_tids: List[str]) -> Dict[str, np.ndarray]:
        """å¾è³‡æ–™åº«è¼‰å…¥é è¨ˆç®—çš„ embeddings"""
        try:
            embeddings_dict = {}
            
            with self.engine.connect() as conn:
                for pos_tid in pos_tids:
                    result = conn.execute(text("""
                        SELECT content_emb 
                        FROM posts 
                        WHERE pos_tid = :pos_tid AND content_emb IS NOT NULL
                    """), {'pos_tid': pos_tid})
                    
                    row = result.fetchone()
                    if row and row[0]:
                        # ååºåˆ—åŒ– embedding
                        embedding = pickle.loads(row[0])
                        embeddings_dict[pos_tid] = embedding
                        
            return embeddings_dict
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_posts_batch(self, offset: int = 0, limit: Optional[int] = None) -> pd.DataFrame:
        """ç²å–æ‰¹æ¬¡è²¼æ–‡è³‡æ–™"""
        try:
            if limit is None:
                limit = self.batch_size
                
            sql = """
                SELECT pos_tid, content, page_name, created_time, content_emb
                FROM posts 
                WHERE content IS NOT NULL 
                AND content != ''
                AND content_emb IS NOT NULL
                ORDER BY pos_tid
                LIMIT %s OFFSET %s
            """
            
            df = pd.read_sql_query(text(sql), self.engine, params=(limit, offset))
            return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def calculate_scam_scores(self, 
                            contents: List[str], 
                            content_embeddings: List[np.ndarray],
                            scam_phrases: Optional[List[str]] = None,
                            aggregation_method: str = 'max') -> List[float]:
        """
        è¨ˆç®—è©é¨™åˆ†æ•¸
        
        Args:
            contents: è²¼æ–‡å…§å®¹åˆ—è¡¨
            content_embeddings: è²¼æ–‡çš„ embeddings
            scam_phrases: è©é¨™æç¤ºè©åˆ—è¡¨
            aggregation_method: èšåˆæ–¹æ³• ('max', 'mean', 'weighted_mean')
            
        Returns:
            è©é¨™é¢¨éšªåˆ†æ•¸åˆ—è¡¨
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            scam_scores = []
            
            for content, content_emb in zip(contents, content_embeddings):
                if content_emb is None:
                    scam_scores.append(0.0)
                    continue
                    
                # è½‰æ›ç‚º tensor
                content_tensor = util.pytorch_cos_sim(
                    self.model.encode(content, convert_to_tensor=True).unsqueeze(0),
                    phrase_embeddings
                )
                
                # è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
                similarities = content_tensor.squeeze().cpu().numpy()
                
                # æ ¹æ“šèšåˆæ–¹æ³•è¨ˆç®—æœ€çµ‚åˆ†æ•¸
                if aggregation_method == 'max':
                    score = float(np.max(similarities))
                elif aggregation_method == 'mean':
                    score = float(np.mean(similarities))
                elif aggregation_method == 'weighted_mean':
                    # çµ¦é«˜ç›¸ä¼¼åº¦æ›´é«˜çš„æ¬Šé‡
                    weights = np.exp(similarities * 2)  # æŒ‡æ•¸æ¬Šé‡
                    score = float(np.average(similarities, weights=weights))
                else:
                    score = float(np.max(similarities))
                    
                scam_scores.append(score)
                
            return scam_scores
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è©é¨™åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_scam_in_batch(self, 
                           scam_phrases: Optional[List[str]] = None,
                           threshold: float = 0.6,
                           output_file: Optional[str] = None,
                           max_results: Optional[int] = None) -> pd.DataFrame:
        """
        æ‰¹æ¬¡æª¢æ¸¬è©é¨™è²¼æ–‡
        
        Args:
            scam_phrases: è‡ªå®šç¾©è©é¨™æç¤ºè©
            threshold: è©é¨™é¢¨éšªé–¾å€¼
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            max_results: æœ€å¤§çµæœæ•¸é‡
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸çš„ DataFrame
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        logger.info(f"ä½¿ç”¨çš„è©é¨™æç¤ºè©: {scam_phrases}")
        logger.info(f"è©é¨™é¢¨éšªé–¾å€¼: {threshold}")
        
        try:
            # ç²å–ç¸½æ•¸
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT COUNT(*) 
                    FROM posts 
                    WHERE content IS NOT NULL 
                    AND content != ''
                    AND content_emb IS NOT NULL
                """))
                total_posts = result.scalar()
                
            logger.info(f"å¾…æª¢æ¸¬çš„è²¼æ–‡ç¸½æ•¸: {total_posts}")
            
            all_results = []
            processed = 0
            offset = 0
            
            while processed < total_posts:
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_batch(offset)
                
                if df.empty:
                    break
                    
                logger.info(f"æ­£åœ¨è™•ç†ç¬¬ {processed + 1} åˆ° {processed + len(df)} ç­†è³‡æ–™...")
                
                # è¼‰å…¥ embeddings
                embeddings = []
                valid_indices = []
                
                for idx, row in df.iterrows():
                    if row['content_emb'] is not None:
                        embedding = pickle.loads(row['content_emb'])
                        embeddings.append(embedding)
                        valid_indices.append(idx)
                    else:
                        embeddings.append(None)
                        
                # è¨ˆç®—è©é¨™åˆ†æ•¸
                scam_scores = self.calculate_scam_scores(
                    df['content'].tolist(),
                    embeddings,
                    scam_phrases
                )
                
                # æ·»åŠ åˆ†æ•¸åˆ° DataFrame
                df['scam_score'] = scam_scores
                df['is_potential_scam'] = df['scam_score'] >= threshold
                
                # åªä¿ç•™å¯èƒ½çš„è©é¨™è²¼æ–‡
                high_risk_posts = df[df['is_potential_scam']].copy()
                
                if not high_risk_posts.empty:
                    all_results.append(high_risk_posts)
                    
                processed += len(df)
                offset += self.batch_size
                
                # å¦‚æœé”åˆ°æœ€å¤§çµæœæ•¸é‡ï¼Œæå‰çµæŸ
                if max_results and sum(len(r) for r in all_results) >= max_results:
                    break
                    
                logger.info(f"å·²è™•ç† {processed}/{total_posts} ç­†ï¼Œç™¼ç¾ {len(high_risk_posts)} ç­†å¯ç–‘è²¼æ–‡")
                
            # åˆä½µæ‰€æœ‰çµæœ
            if all_results:
                final_results = pd.concat(all_results, ignore_index=True)
                final_results = final_results.sort_values('scam_score', ascending=False)
                
                if max_results:
                    final_results = final_results.head(max_results)
                    
                logger.info(f"ğŸ¯ æª¢æ¸¬å®Œæˆï¼å…±ç™¼ç¾ {len(final_results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡")
                
                # è¼¸å‡ºçµæœ
                if output_file:
                    final_results.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"çµæœå·²å„²å­˜åˆ°: {output_file}")
                    
                return final_results
            else:
                logger.info("æ²’æœ‰ç™¼ç¾å¯ç–‘çš„è©é¨™è²¼æ–‡")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡æª¢æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_single_post(self, content: str, scam_phrases: Optional[List[str]] = None) -> Dict:
        """
        æª¢æ¸¬å–®ä¸€è²¼æ–‡
        
        Args:
            content: è²¼æ–‡å…§å®¹
            scam_phrases: è©é¨™æç¤ºè©
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸å’Œè©³ç´°è³‡è¨Šçš„å­—å…¸
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆå…§å®¹ embedding
            content_embedding = self.model.encode(content, convert_to_tensor=False)
            
            # è¨ˆç®—è©é¨™åˆ†æ•¸
            scam_scores = self.calculate_scam_scores(
                [content], 
                [content_embedding], 
                scam_phrases
            )
            
            scam_score = scam_scores[0]
            
            # è¨ˆç®—èˆ‡æ¯å€‹è©é¨™çŸ­èªçš„ç›¸ä¼¼åº¦
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            content_tensor = self.model.encode(content, convert_to_tensor=True)
            
            similarities = util.cos_sim(content_tensor, phrase_embeddings).squeeze().cpu().numpy()
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª
            top_matches = []
            for i, phrase in enumerate(scam_phrases):
                top_matches.append({
                    'phrase': phrase,
                    'similarity': float(similarities[i])
                })
                
            top_matches.sort(key=lambda x: x['similarity'], reverse=True)
            
            result = {
                'content': content,
                'scam_score': scam_score,
                'is_potential_scam': scam_score >= 0.6,
                'top_matching_phrases': top_matches[:5],
                'risk_level': self._get_risk_level(scam_score)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"æª¢æ¸¬å–®ä¸€è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_risk_level(self, score: float) -> str:
        """æ ¹æ“šåˆ†æ•¸ç²å–é¢¨éšªç­‰ç´š"""
        if score >= 0.8:
            return "æ¥µé«˜é¢¨éšª"
        elif score >= 0.6:
            return "é«˜é¢¨éšª"
        elif score >= 0.4:
            return "ä¸­ç­‰é¢¨éšª"
        elif score >= 0.2:
            return "ä½é¢¨éšª"
        else:
            return "ç„¡é¢¨éšª"
            
    def search_by_keywords(self, 
                          keywords: List[str], 
                          limit: int = 100,
                          threshold: float = 0.5) -> pd.DataFrame:
        """
        ä½¿ç”¨é—œéµå­—é€²è¡Œèªæ„æœå°‹
        
        Args:
            keywords: æœå°‹é—œéµå­—
            limit: è¿”å›çµæœæ•¸é‡
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            æœå°‹çµæœ DataFrame
        """
        try:
            # ç”Ÿæˆé—œéµå­— embeddings
            keyword_embeddings = self.model.encode(keywords, convert_to_tensor=True)
            
            results = []
            processed = 0
            offset = 0
            
            while len(results) < limit:
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_batch(offset, self.batch_size)
                
                if df.empty:
                    break
                    
                for _, row in df.iterrows():
                    if row['content_emb'] is None:
                        continue
                        
                    # è¼‰å…¥ embedding
                    content_embedding = pickle.loads(row['content_emb'])
                    content_tensor = self.model.encode(row['content'], convert_to_tensor=True)
                    
                    # è¨ˆç®—èˆ‡é—œéµå­—çš„ç›¸ä¼¼åº¦
                    similarities = util.cos_sim(content_tensor, keyword_embeddings).squeeze().cpu().numpy()
                    max_similarity = float(np.max(similarities))
                    
                    if max_similarity >= threshold:
                        result_row = row.copy()
                        result_row['similarity_score'] = max_similarity
                        result_row['matching_keyword'] = keywords[np.argmax(similarities)]
                        results.append(result_row)
                        
                        if len(results) >= limit:
                            break
                            
                processed += len(df)
                offset += self.batch_size
                
                logger.info(f"å·²è™•ç† {processed} ç­†ï¼Œæ‰¾åˆ° {len(results)} ç­†ç¬¦åˆçš„çµæœ")
                
            if results:
                results_df = pd.DataFrame(results)
                results_df = results_df.sort_values('similarity_score', ascending=False)
                return results_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"é—œéµå­—æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description='è©é¨™è²¼æ–‡æª¢æ¸¬å·¥å…·')
    parser.add_argument('--mode', choices=['batch', 'single', 'search'], default='batch',
                       help='åŸ·è¡Œæ¨¡å¼: batch=æ‰¹æ¬¡æª¢æ¸¬, single=å–®ä¸€è²¼æ–‡æª¢æ¸¬, search=é—œéµå­—æœå°‹')
    parser.add_argument('--content', type=str, help='å–®ä¸€è²¼æ–‡å…§å®¹ (single æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--keywords', nargs='+', help='æœå°‹é—œéµå­— (search æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--threshold', type=float, default=0.6, help='è©é¨™é¢¨éšªé–¾å€¼')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--limit', type=int, default=1000, help='æœ€å¤§çµæœæ•¸é‡')
    parser.add_argument('--scam-phrases', nargs='+', help='è‡ªå®šç¾©è©é¨™æç¤ºè©')
    
    args = parser.parse_args()
    
    try:
        # å‰µå»ºæª¢æ¸¬å™¨
        detector = ScamDetector()
        
        if args.mode == 'batch':
            # æ‰¹æ¬¡æª¢æ¸¬
            results = detector.detect_scam_in_batch(
                scam_phrases=args.scam_phrases,
                threshold=args.threshold,
                output_file=args.output,
                max_results=args.limit
            )
            
            if not results.empty:
                print(f"\nğŸ¯ ç™¼ç¾ {len(results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡:")
                print(results[['pos_tid', 'page_name', 'scam_score', 'content']].head(10).to_string())
                
        elif args.mode == 'single':
            if not args.content:
                print("âŒ è«‹æä¾›è²¼æ–‡å…§å®¹ (--content)")
                return
                
            # å–®ä¸€è²¼æ–‡æª¢æ¸¬
            result = detector.detect_single_post(args.content, args.scam_phrases)
            
            print(f"\nğŸ“ è²¼æ–‡å…§å®¹: {result['content']}")
            print(f"ğŸ¯ è©é¨™åˆ†æ•¸: {result['scam_score']:.3f}")
            print(f"âš ï¸  é¢¨éšªç­‰ç´š: {result['risk_level']}")
            print(f"ğŸš¨ æ˜¯å¦å¯ç–‘: {'æ˜¯' if result['is_potential_scam'] else 'å¦'}")
            print("\nğŸ” æœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª:")
            for match in result['top_matching_phrases'][:3]:
                print(f"  - {match['phrase']}: {match['similarity']:.3f}")
                
        elif args.mode == 'search':
            if not args.keywords:
                print("âŒ è«‹æä¾›æœå°‹é—œéµå­— (--keywords)")
                return
                
            # é—œéµå­—æœå°‹
            results = detector.search_by_keywords(
                keywords=args.keywords,
                limit=args.limit,
                threshold=args.threshold
            )
            
            if not results.empty:
                print(f"\nğŸ” æ‰¾åˆ° {len(results)} ç­†ç›¸é—œè²¼æ–‡:")
                print(results[['pos_tid', 'page_name', 'similarity_score', 'matching_keyword', 'content']].head(10).to_string())
                
                if args.output:
                    results.to_csv(args.output, index=False, encoding='utf-8-sig')
                    print(f"çµæœå·²å„²å­˜åˆ°: {args.output}")
            else:
                print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œè²¼æ–‡")
                
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == "__main__":
    main() 