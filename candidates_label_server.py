import streamlit as st
import pandas as pd
from sqlalchemy import create_engine, text
from candidates_dataloader_to_sql import fetch_candidate_posts
import logging
import datetime
import subprocess
import os
from logging import Filter, Formatter
import socket
from sentence_transformers import SentenceTransformer, util
import numpy as np
import json
import time
import psutil
import gc
from typing import List, Dict, Optional
import random
import atexit
import multiprocessing as mp
from multiprocessing import Process, Queue, Manager

# è‡ªå®šç¾©çš„ IP éæ¿¾å™¨
class IPFilter(Filter):
    def filter(self, record):
        try:
            # ä½¿ç”¨ st.context.headers ç²å–è«‹æ±‚æ¨™é ­
            headers = st.context.headers if hasattr(st, 'context') else None
            if headers:
                # å˜—è©¦å¾ X-Forwarded-For ç²å–çœŸå¯¦ IPï¼ˆé©ç”¨æ–¼ ngrokï¼‰
                ip = headers.get('X-Forwarded-For', '').split(',')[0].strip()
                if not ip:
                    # å¦‚æœæ²’æœ‰ X-Forwarded-Forï¼Œå‰‡ä½¿ç”¨ X-Real-IP
                    ip = headers.get('X-Real-IP', '')
                if not ip:
                    # å¦‚æœéƒ½æ²’æœ‰ï¼Œå‰‡ä½¿ç”¨ Remote-Addr
                    ip = headers.get('Remote-Addr', '')
                record.ip = ip if ip else 'unknown'
            else:
                record.ip = 'unknown'
        except Exception:
            record.ip = 'unknown'
        return True

# å»ºç«‹ logs ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
if not os.path.exists('logs'):
    os.makedirs('logs')

# è¨­å®šæ—¥èªŒæª”æ¡ˆåç¨±ï¼ˆä½¿ç”¨ç•¶å‰æ—¥æœŸï¼‰
current_date = datetime.datetime.now().strftime('%Y-%m-%d')
log_filename = f'logs/candidates_label_{current_date}.log'

# å»ºç«‹è‡ªå®šç¾©çš„æ ¼å¼åŒ–å™¨
class SafeFormatter(Formatter):
    def format(self, record):
        # ç¢ºä¿ record æœ‰ ip å±¬æ€§
        if not hasattr(record, 'ip'):
            record.ip = 'unknown'
        return super().format(record)

formatter = SafeFormatter(
    fmt='%(asctime)s [%(ip)s] - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# è¨­å®šæª”æ¡ˆè™•ç†å™¨
file_handler = logging.FileHandler(log_filename, encoding='utf-8', mode='a')
file_handler.setFormatter(formatter)

# è¨­å®šæ§åˆ¶å°è™•ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)

# è¨­å®šæ ¹ logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ç§»é™¤æ‰€æœ‰ç¾æœ‰çš„è™•ç†å™¨
for handler in logger.handlers[:]:
    logger.removeHandler(handler)

# æ·»åŠ è‡ªå®šç¾©çš„è™•ç†å™¨
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# æ·»åŠ  IP éæ¿¾å™¨
ip_filter = IPFilter()
logger.addFilter(ip_filter)

# è¨­å®šè³‡æ–™åº«é€£ç·šï¼ˆæ¨™è¨˜è³‡æ–™ï¼‰
LABELING_DB_URL = "postgresql+psycopg2://postgres:00000000@localhost:5432/labeling_db"
SOURCE_DB_URL = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash"

def init_database_engines():
    """åˆå§‹åŒ–è³‡æ–™åº«å¼•æ“"""
    try:
        # å»ºç«‹å…©å€‹è³‡æ–™åº«çš„é€£ç·šå¼•æ“ï¼Œæ·»åŠ é€£æ¥æ± è¨­å®š
        labeling_engine = create_engine(
            LABELING_DB_URL,
            pool_size=5,  # é€£æ¥æ± å¤§å°
            max_overflow=10,  # æœ€å¤§æº¢å‡ºé€£æ¥æ•¸
            pool_pre_ping=True,  # é€£æ¥å‰æª¢æŸ¥
            pool_recycle=3600,  # é€£æ¥å›æ”¶æ™‚é–“ï¼ˆç§’ï¼‰
            pool_timeout=30  # é€£æ¥è¶…æ™‚æ™‚é–“
        )
        source_engine = create_engine(
            SOURCE_DB_URL,
            pool_size=5,  # é€£æ¥æ± å¤§å°
            max_overflow=10,  # æœ€å¤§æº¢å‡ºé€£æ¥æ•¸
            pool_pre_ping=True,  # é€£æ¥å‰æª¢æŸ¥
            pool_recycle=3600,  # é€£æ¥å›æ”¶æ™‚é–“ï¼ˆç§’ï¼‰
            pool_timeout=30  # é€£æ¥è¶…æ™‚æ™‚é–“
        )
        
        # æ¸¬è©¦é€£æ¥
        with labeling_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        with source_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
            
        logger.info("è³‡æ–™åº«é€£æ¥åˆå§‹åŒ–æˆåŠŸ")
        return labeling_engine, source_engine
        
    except Exception as e:
        logger.error(f"è³‡æ–™åº«é€£æ¥åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        raise

def initialize_app():
    """åˆå§‹åŒ–æ‡‰ç”¨ç¨‹å¼ï¼Œåªåœ¨ç¬¬ä¸€æ¬¡è¼‰å…¥æ™‚åŸ·è¡Œ"""
    if 'app_initialized' not in st.session_state:
        # è¨˜éŒ„ç¨‹å¼å•Ÿå‹•
        logger.info("ç¨‹å¼å•Ÿå‹•")
        
        # åˆå§‹åŒ–è³‡æ–™åº«å¼•æ“
        try:
            labeling_engine, source_engine = init_database_engines()
            
            # ç¢ºä¿æœ‰ system_settings è³‡æ–™è¡¨
            try:
                with labeling_engine.begin() as conn:
                    conn.execute(text("""
                        CREATE TABLE IF NOT EXISTS system_settings (
                            key VARCHAR(50) PRIMARY KEY,
                            value TEXT,
                            updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                        )
                    """))
            except Exception as e:
                logger.error(f"å»ºç«‹ system_settings è³‡æ–™è¡¨å¤±æ•—: {str(e)}")
                st.error(f"è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {str(e)}")
                st.stop()
            
            # å°‡å¼•æ“å­˜å„²åˆ° session state
            st.session_state.labeling_engine = labeling_engine
            st.session_state.source_engine = source_engine
            st.session_state.app_initialized = True
            
        except Exception as e:
            st.error(f"ç„¡æ³•é€£æ¥åˆ°è³‡æ–™åº«ï¼š{str(e)}")
            st.error("è«‹æª¢æŸ¥ PostgreSQL æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œï¼Œä»¥åŠé€£æ¥è¨­å®šæ˜¯å¦æ­£ç¢º")
            st.stop()

# åˆå§‹åŒ–æ‡‰ç”¨ç¨‹å¼
initialize_app()

# å¾ session state ç²å–å¼•æ“
labeling_engine = st.session_state.labeling_engine
source_engine = st.session_state.source_engine

# æ–°å¢ ScamDetectorMemmap é¡åˆ¥
class ScamDetectorMemmap:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 8192,
                 embeddings_dir: str = "embeddings_data",
                 memory_optimized: bool = True):
        """
        åˆå§‹åŒ–è©é¨™æª¢æ¸¬å™¨ (ä½¿ç”¨ memmap å­˜å„²)
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.embeddings_dir = embeddings_dir
        self.memory_optimized = memory_optimized
        self.engine = None
        self.model = None
        self.embeddings_array = None
        
        # é è¨­è©é¨™æç¤ºè©
        self.default_scam_phrases = [
            "åŠ å…¥LINE", "åŠ å…¥Telegram", "å¿«é€Ÿè³ºéŒ¢", "è¢«å‹•æ”¶å…¥", 
            "æŠ•è³‡åŒ…ä½ è³º", "ç§è¨Šæˆ‘", "è€å¸«å¸¶å–®", "ç©©è³ºä¸è³ ",
            "è¼•é¬†è³ºéŒ¢", "ä¸€å¤©è³ºè¬å…ƒ", "ä¿è­‰ç²åˆ©", "é«˜å ±é…¬ä½é¢¨éšª",
            "åŠ ç¾¤çµ„", "è·Ÿå–®", "æ“ç›¤æ‰‹", "è²¡å¯Œè‡ªç”±",
            "æœˆæ”¶å…¥", "å…¼è·è³ºéŒ¢", "åœ¨å®¶è³ºéŒ¢", "ç¶²è·¯è³ºéŒ¢",
            "æŠ•è³‡ç†è²¡", "è™›æ“¬è²¨å¹£", "æ¯”ç‰¹å¹£", "æŒ–ç¤¦",
            "å€Ÿè²¸", "å°é¡è²¸æ¬¾", "æ€¥ç”¨éŒ¢", "å…æŠµæŠ¼",
            "ä»£è¾¦ä¿¡è²¸", "ä¿¡ç”¨å¡ä»£å„Ÿ", "å‚µå‹™æ•´åˆ"
        ]
        
        # æª”æ¡ˆè·¯å¾‘
        self.embeddings_file = os.path.join(embeddings_dir, "embeddings.dat")
        self.index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        self.metadata_file = os.path.join(embeddings_dir, "metadata.json")
        
        # åˆå§‹åŒ–
        self._init_db_connection()
        self._load_model(model_name)
        self._load_embeddings_metadata()
        self._init_embeddings_memmap()
        
    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(
                self.db_url,
                pool_size=3,  # é€£æ¥æ± å¤§å°
                max_overflow=5,  # æœ€å¤§æº¢å‡ºé€£æ¥æ•¸
                pool_pre_ping=True,  # é€£æ¥å‰æª¢æŸ¥
                pool_recycle=3600,  # é€£æ¥å›æ”¶æ™‚é–“ï¼ˆç§’ï¼‰
                pool_timeout=30  # é€£æ¥è¶…æ™‚æ™‚é–“
            )
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _load_model(self, model_name: str):
        """è¼‰å…¥æ¨¡å‹"""
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def _load_embeddings_metadata(self):
        """è¼‰å…¥ embeddings metadata"""
        try:
            if not os.path.exists(self.index_file):
                raise FileNotFoundError(f"ç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨: {self.index_file}")
                
            with open(self.index_file, 'r', encoding='utf-8') as f:
                self.pos_tid_to_index = json.load(f)
                
            if not os.path.exists(self.metadata_file):
                raise FileNotFoundError(f"Metadata æª”æ¡ˆä¸å­˜åœ¨: {self.metadata_file}")
                
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
                
            self.embedding_dim = self.metadata['embedding_dim']
            self.total_embeddings = self.metadata['total_embeddings']
            
            logger.info(f"è¼‰å…¥ embeddings metadataï¼š")
            logger.info(f"  - ç¸½è¨˜éŒ„æ•¸: {self.total_embeddings}")
            logger.info(f"  - Embedding ç¶­åº¦: {self.embedding_dim}")
            
            if not os.path.exists(self.embeddings_file):
                raise FileNotFoundError(f"Embeddings æª”æ¡ˆä¸å­˜åœ¨: {self.embeddings_file}")
                
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings metadata å¤±æ•—: {str(e)}")
            raise
            
    def _init_embeddings_memmap(self):
        """åˆå§‹åŒ– embeddings memmap"""
        try:
            total_records = self.total_embeddings
            
            self.embeddings_array = np.memmap(
                self.embeddings_file,
                dtype=np.float32,
                mode='r',
                shape=(total_records, self.embedding_dim)
            )
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– embeddings memmap å¤±æ•—: {str(e)}")
            raise
            
    def _get_posts_batch(self, offset: int = 0, limit: Optional[int] = None) -> pd.DataFrame:
        """ç²å–æ‰¹æ¬¡è²¼æ–‡è³‡æ–™"""
        try:
            if limit is None:
                limit = self.batch_size
                
            valid_pos_tids = list(self.pos_tid_to_index.keys())
            
            if not valid_pos_tids:
                return pd.DataFrame()
                
            start_idx = offset
            end_idx = min(offset + limit, len(valid_pos_tids))
            batch_pos_tids = valid_pos_tids[start_idx:end_idx]
            
            if not batch_pos_tids:
                return pd.DataFrame()
                
            placeholders = ','.join([f"'{pid}'" for pid in batch_pos_tids])
            sql = f"""
                SELECT pos_tid, content, page_name, created_time
                FROM posts_deduplicated 
                WHERE pos_tid IN ({placeholders})
                ORDER BY pos_tid
            """
            
            with self.engine.connect() as conn:
                df = pd.read_sql_query(text(sql), conn)
            return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_embeddings_for_pos_tids_optimized(self, pos_tids: List[str], batch_size: int = 100) -> Dict[str, np.ndarray]:
        """å„ªåŒ–ç‰ˆæœ¬ï¼šåˆ†æ‰¹ç²å–æŒ‡å®š pos_tids çš„ embeddings"""
        try:
            current_memory = get_memory_usage()
            if current_memory['percent'] > 85:
                logger.warning(f"è¨˜æ†¶é«”ä½¿ç”¨éé«˜: {current_memory['percent']:.1f}%ï¼Œå¼·åˆ¶åƒåœ¾å›æ”¶")
                gc.collect()
            
            result = {}
            
            for pos_tid in pos_tids:
                if pos_tid in self.pos_tid_to_index:
                    index = self.pos_tid_to_index[pos_tid]
                    result[pos_tid] = self.embeddings_array[index].copy()
                    
            return result
            
        except Exception as e:
            logger.error(f"ç²å– embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def search_similar_posts(self, 
                           query_text: str, 
                           limit: int = 20,
                           threshold: float = 0.3,
                           random_search: bool = False,
                           progress_callback=None) -> pd.DataFrame:
        """
        æœå°‹ç›¸ä¼¼è²¼æ–‡
        
        Args:
            query_text: æŸ¥è©¢æ–‡å­—
            limit: è¿”å›çµæœæ•¸é‡
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            random_search: æ˜¯å¦éš¨æ©Ÿæœå°‹
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸ï¼Œç”¨æ–¼å³æ™‚æ›´æ–°é€²åº¦
        Returns:
            æœå°‹çµæœ DataFrame
        """
        try:
            # ç”ŸæˆæŸ¥è©¢æ–‡å­—çš„ embedding
            query_embedding = self.model.encode(query_text, convert_to_tensor=True)
            device = query_embedding.device
            
            results = []
            processed = 0
            offset = 0
            
            # å–å¾—æ‰€æœ‰ pos_tid ä¸¦æ ¹æ“š random_search æ±ºå®šæ˜¯å¦æ‰“äº‚
            valid_pos_tids = list(self.pos_tid_to_index.keys())
            total_pos_tids = len(valid_pos_tids)
            if random_search:
                random.shuffle(valid_pos_tids)
            
            while len(results) < limit and processed < total_pos_tids:
                # å–å‡ºé€™ä¸€æ‰¹çš„ pos_tid
                batch_pos_tids = valid_pos_tids[offset:offset + self.batch_size]
                if not batch_pos_tids:
                    break
                    
                # æŸ¥è©¢é€™ä¸€æ‰¹è²¼æ–‡
                placeholders = ','.join([f"'{pid}'" for pid in batch_pos_tids])
                sql = f"""
                    SELECT pos_tid, content, page_name, created_time
                    FROM posts_deduplicated 
                    WHERE pos_tid IN ({placeholders})
                    ORDER BY pos_tid
                """
                with self.engine.connect() as conn:
                    df = pd.read_sql_query(text(sql), conn)
                if df.empty:
                    break
                    
                # å–å¾—é€™æ‰¹çš„ embeddings
                embeddings_dict = self._get_embeddings_for_pos_tids_optimized(batch_pos_tids)
                for _, row in df.iterrows():
                    pos_tid = row['pos_tid']
                    if pos_tid not in embeddings_dict:
                        continue
                    # è¨ˆç®—ç›¸ä¼¼åº¦
                    content_emb = embeddings_dict[pos_tid]
                    from torch import tensor
                    content_tensor = tensor(content_emb, device=device).unsqueeze(0)
                    similarity = util.cos_sim(content_tensor, query_embedding).squeeze().cpu().numpy()
                    similarity_score = float(similarity)
                    if similarity_score >= threshold:
                        result_row = row.copy()
                        result_row['similarity_score'] = similarity_score
                        results.append(result_row)
                        if len(results) >= limit:
                            break
                            
                processed += len(batch_pos_tids)
                offset += len(batch_pos_tids)
                
                # è¨˜éŒ„é€²åº¦
                progress_msg = f"å·²è™•ç† {processed} ç­†ï¼Œæ‰¾åˆ° {len(results)} ç­†ç¬¦åˆçš„çµæœ"
                logger.info(progress_msg)
                
                # å¦‚æœæä¾›äº†é€²åº¦å›èª¿å‡½æ•¸ï¼Œå‰‡èª¿ç”¨å®ƒ
                if progress_callback:
                    progress_callback({
                        'processed': processed,
                        'total': total_pos_tids,
                        'found': len(results),
                        'message': progress_msg
                    })
            
            # æœå°‹å®Œæˆï¼Œå›å‚³çµæœ
            if results:
                results_df = pd.DataFrame(results)
                results_df = results_df.sort_values('similarity_score', ascending=False)
                return results_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"ç›¸ä¼¼è²¼æ–‡æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_embeddings': len(self.pos_tid_to_index),
            'embedding_dimension': self.embedding_dim,
            'embeddings_file_size_mb': os.path.getsize(self.embeddings_file) / 1024 / 1024 if os.path.exists(self.embeddings_file) else 0,
            'model_name': self.metadata.get('model_name', 'Unknown'),
            'last_updated': self.metadata.get('last_updated', 'Unknown'),
            'batch_size': self.batch_size
        }
        return stats
        
    def __del__(self):
        """æ¸…ç†è³‡æº"""
        try:
            if hasattr(self, 'embeddings_array') and self.embeddings_array is not None:
                del self.embeddings_array
                self.embeddings_array = None
                logger.info("å·²æ¸…ç† memmap è³‡æº")
            
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None
                logger.info("å·²æ¸…ç†æ¨¡å‹è³‡æº")
                
            if hasattr(self, 'engine') and self.engine is not None:
                self.engine.dispose()
                self.engine = None
                logger.info("å·²æ¸…ç†è³‡æ–™åº«é€£æ¥")
                
        except Exception as e:
            logger.warning(f"æ¸…ç†è³‡æºæ™‚ç™¼ç”Ÿè­¦å‘Š: {str(e)}")
            
    def cleanup(self):
        """æ‰‹å‹•æ¸…ç†è³‡æº"""
        self.__del__()

def get_memory_usage():
    """ç²å–ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    return {
        'rss_mb': memory_info.rss / 1024 / 1024,
        'vms_mb': memory_info.vms / 1024 / 1024,
        'percent': process.memory_percent()
    }

# --- ç²å–æ‰€æœ‰ç¾¤çµ„ç·¨è™Ÿ ---
@st.cache_data
def get_all_group_ids() -> list:
    """å¾è³‡æ–™åº«ç²å–æ‰€æœ‰ä¸é‡è¤‡çš„ç¾¤çµ„ç·¨è™Ÿ"""
    query = "SELECT DISTINCT group_id FROM candidates ORDER BY group_id"
    result = pd.read_sql(query, labeling_engine)
    return result['group_id'].tolist()

# --- è¼‰å…¥è³‡æ–™çš„å‡½æ•¸ï¼ˆå¸¶å¿«å–ï¼‰ ---
@st.cache_data
def load_data_from_db(group_id: int) -> pd.DataFrame:
    """å¾sqlè¼‰å…¥è©²groupçš„data"""
    query = f"SELECT * FROM candidates WHERE group_id = {group_id}"
    logger.info(f"ğŸ”„ è¼‰å…¥ç¾¤çµ„ {group_id} çš„sqlè³‡æ–™")
    return pd.read_sql(query, labeling_engine)

def get_current_data(group_id: int) -> pd.DataFrame:
    """æ™ºæ…§å–å¾—ç•¶å‰è³‡æ–™"""
    
    # å¦‚æœç¾¤çµ„æ”¹è®Šï¼Œå¼·åˆ¶é‡æ–°è¼‰å…¥ä¸¦é‡ç½®é¡Œè™Ÿ
    if st.session_state.current_group != group_id:
        st.session_state.current_group = group_id
        st.session_state.need_update = False
        st.session_state.label_index = 0  # é‡ç½®é¡Œè™Ÿç‚º0 (ç¬¬1é¡Œ)
        load_data_from_db.clear()  # æ¸…é™¤èˆŠç¾¤çµ„çš„å¿«å–
        # è¼‰å…¥æ–°ç¾¤çµ„è³‡æ–™
        db = load_data_from_db(group_id)
        # è¨ˆç®—ä¸¦è¨­ç½®åˆ°æœ€æ–°é€²åº¦
        latest_index = get_latest_progress(db)
        st.session_state.label_index = latest_index
        logger.info(f"ğŸ”„ åˆ‡æ›åˆ°ç¾¤çµ„ {group_id}ï¼Œé¡Œè™Ÿå°å‘åˆ°ç¬¬{latest_index+1}é¡Œ")
        st.success(f"å·²æ¢å¾©é€²åº¦åˆ°ç¬¬{latest_index+1}é¡Œ")
        return db
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå°èˆªå‹•ä½œä¸”éœ€è¦æ›´æ–°
    is_navigation = st.session_state.get('just_navigated', False)
    if is_navigation and st.session_state.need_update:
        logger.info("ğŸ“¥ å°èˆªæ™‚æª¢æ¸¬åˆ°è³‡æ–™éœ€è¦æ›´æ–°ï¼Œé‡æ–°è¼‰å…¥...")
        load_data_from_db.clear()  # æ¸…é™¤å¿«å–
        st.session_state.need_update = False
        st.session_state.just_navigated = False
        db = load_data_from_db(group_id)
        return db
    
    # é‡ç½®å°èˆªæ¨™è¨˜
    if st.session_state.get('just_navigated', False):
        st.session_state.just_navigated = False
    
    # å…¶ä»–æƒ…æ³ä½¿ç”¨å¿«å–
    return load_data_from_db(group_id)

# --- å„²å­˜æ¨™è¨˜çµæœï¼ˆåªæ›´æ–°è³‡æ–™åº«ï¼‰ ---
def save_label_only(pos_tid: str, label: str, note: str, group_id: int) -> None:
    """å„²å­˜åˆ°è³‡æ–™åº«ï¼Œå¦‚æœæ˜¯é—œéµå­—æœå°‹çš„çµæœ(group_id=999)ä¸”ä¸å­˜åœ¨å‰‡æ–°å¢è¨˜éŒ„"""
    # å…ˆæª¢æŸ¥è²¼æ–‡æ˜¯å¦å­˜åœ¨
    check_sql = "SELECT COUNT(*) FROM candidates WHERE pos_tid = :pos_tid"
    
    with labeling_engine.begin() as conn:
        result = conn.execute(text(check_sql), {"pos_tid": pos_tid})
        exists = result.scalar() > 0
        
        if group_id == 999 and not exists:
            # å¦‚æœæ˜¯é—œéµå­—æœå°‹ä¸”è²¼æ–‡ä¸å­˜åœ¨ï¼Œå‰‡å¾åŸå§‹è³‡æ–™åº«ç²å–å…§å®¹ä¸¦æ–°å¢è¨˜éŒ„
            try:
                # å…ˆå¾åŸå§‹è³‡æ–™åº«ç²å–è²¼æ–‡å…§å®¹
                source_query = "SELECT pos_tid, content FROM posts_deduplicated WHERE pos_tid = :pos_tid"
                with source_engine.connect() as source_conn:
                    source_result = source_conn.execute(text(source_query), {"pos_tid": pos_tid})
                    post_data = source_result.fetchone()
                    
                    if post_data is None:
                        st.error(f"åœ¨åŸå§‹è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°è²¼æ–‡ï¼š{pos_tid}")
                        return
                    
                    # æ’å…¥åˆ°æ¨™è¨˜è³‡æ–™åº«
                    insert_sql = """
                        INSERT INTO candidates (pos_tid, content, group_id, label, note)
                        VALUES (:pos_tid, :content, :group_id, :label, :note)
                    """
                    conn.execute(text(insert_sql), {
                        "pos_tid": pos_tid,
                        "content": post_data.content,
                        "group_id": group_id,
                        "label": label,
                        "note": note
                    })
                    logger.info(f"ğŸ“ æ–°å¢é—œéµå­—æœå°‹çµæœåˆ°è³‡æ–™åº«ï¼š{pos_tid}")
            except Exception as e:
                logger.error(f"âŒ æ–°å¢è¨˜éŒ„å¤±æ•—ï¼š{str(e)}")
                st.error(f"ç„¡æ³•æ–°å¢è¨˜éŒ„ï¼š{str(e)}")
                return
        else:
            # æ›´æ–°ç¾æœ‰è¨˜éŒ„
            update_sql = """
                UPDATE candidates
                SET label = :label, note = :note
                WHERE pos_tid = :pos_tid
            """
            result = conn.execute(text(update_sql), {
                "label": label,
                "note": note,
                "pos_tid": pos_tid
            })
            if result.rowcount == 0 and group_id != 999:
                st.warning(f"è­¦å‘Šï¼šæ²’æœ‰æ‰¾åˆ° pos_tid = {pos_tid} çš„è¨˜éŒ„")
    
    if group_id != 999:
        logger.info(f"ğŸ’¾ å„²å­˜æ¨™è¨˜ï¼š{pos_tid} -> {label} from group {group_id} ç¬¬{st.session_state.label_index+1}é¡Œ")
    else:
        logger.info(f"ğŸ”‘ å¾é—œéµå­—æœå°‹å„²å­˜æ¨™è¨˜ï¼š{pos_tid} -> {label} from group {group_id}")
    
    # æ¨™è¨˜éœ€è¦æ›´æ–°ï¼Œä½†ä¸ç«‹å³è¼‰å…¥
    st.session_state.need_update = True

# --- é¡¯ç¤ºä¸€ç­†è²¼æ–‡é€²è¡Œæ¨™è¨˜ ---
def show_labeling_ui(group_id: int) -> None:
    index = st.session_state.label_index
    row = df.iloc[index]
    st.markdown(f"### ç›®å‰ç¬¬ {index + 1} / {len(df)} ç­†")
    st.markdown(f"**pos_tidï¼š** `{row['pos_tid']}`")
    st.text_area("è²¼æ–‡å…§å®¹", row["content"], height=400, disabled=False)

    # é¡¯ç¤ºç•¶å‰æ¨™è¨˜ç‹€æ…‹
    current_label = row.get('label')
    if pd.isna(current_label) or current_label is None:
        current_label = 'å°šæœªåˆ¤æ–·'
    if current_label != 'å°šæœªåˆ¤æ–·':
        st.info(f"ç•¶å‰æ¨™è¨˜ï¼š{current_label}")

    # é¡¯ç¤ºæ›´æ–°ç‹€æ…‹ï¼ˆé™¤éŒ¯ç”¨ï¼‰
    #if st.session_state.need_update:
    #    st.warning("è³‡æ–™æŒ‰ä¸Šä¸‹é¡Œæœƒè‡ªå‹•æ›´æ–°")

    # å‚™è¨»æ¬„ä½
    note = st.text_input("å‚™è¨»ï¼ˆå¯é¸ï¼‰", value=row.get('note', ''))

    # æ‰‹å‹•è·³è½‰é¡Œè™Ÿ
    st.markdown("---")
    st.markdown("**è·³è½‰åˆ°ç¬¬å¹¾é¡Œ**")
    col_nav1, col_nav2, col_nav3 = st.columns([2, 1, 2])
    with col_nav1:
        target_question = st.number_input(
            "ç·¨è™Ÿ", 
            min_value=1, 
            max_value=len(df), 
            value=index + 1,
            key="jump_to_question",
            label_visibility="collapsed"
        )
    with col_nav2:
        if st.button("ğŸ¯ è·³è½‰", type="secondary"):
            st.session_state.label_index = target_question - 1
            st.rerun()
    with col_nav3:
        if st.button("ğŸ“ ç§»å‹•åˆ°æœªå®Œæˆçš„é¡Œç›®", type="secondary"):
            unlabeled_mask = df['label'].isna() | (df['label'] == 'å°šæœªåˆ¤æ–·') | (df['label'] == '') | df['label'].isnull()
            unlabeled_indices = df[unlabeled_mask].index.tolist()
            if unlabeled_indices:
                st.session_state.label_index = min(unlabeled_indices)
                st.rerun()
            else:
                st.info("æ‰€æœ‰é¡Œç›®éƒ½å·²å®Œæˆï¼")

    # æŒ‰éˆ•å€åŸŸ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("â¬…ï¸ ä¸Šä¸€é¡Œ", disabled = index<=0, key="labeling_prev"):
            st.session_state.just_navigated = True
            st.session_state.label_index -= 1
            st.rerun()
    
    with col2:
        if st.button("âœ… æ˜¯", type="secondary", disabled=(index == len(df)), key="labeling_yes"):
            save_label_only(row["pos_tid"], "æ˜¯", note, group_id)
            # é˜²æ­¢è¶…å‡ºç¯„åœ
            if not index >= (len(df) - 1): # if index < 799:
                st.session_state.label_index += 1
            st.rerun()
    
    with col3:
        if st.button("âŒ å¦", type="secondary", disabled=(index == len(df)), key="labeling_no"):
            save_label_only(row["pos_tid"], "å¦", note, group_id)
            # é˜²æ­¢è¶…å‡ºç¯„åœ
            if not index >= (len(df) - 1):
                st.session_state.label_index += 1
            st.rerun()
    
    with col4:
        if st.button("ä¸‹ä¸€é¡Œ â¡ï¸", disabled = index >= (len(df) - 1), key="labeling_next"):
            st.session_state.just_navigated = True
            st.session_state.label_index += 1
            st.rerun()

    # é¡¯ç¤ºé€²åº¦
    total = len(df)
    labeled = len(df[df['label'].isin(['æ˜¯', 'å¦'])])
    if labeled == total:
        st.success("ğŸ‰ æœ¬çµ„è²¼æ–‡å·²å…¨éƒ¨æ¨™è¨˜å®Œç•¢ï¼")
    st.progress(labeled / total)
    st.caption(f"å·²å®Œæˆï¼š{labeled}/{total} é¡Œ")

def get_latest_progress(df: pd.DataFrame) -> int:
    """è¨ˆç®—ç•¶å‰æœ€æ–°é€²åº¦ï¼ˆä¸‹ä¸€å€‹æœªæ¨™è¨˜çš„é¡Œç›®ç´¢å¼•ï¼‰"""
    # æ‰¾å‡ºæ‰€æœ‰æœªæ¨™è¨˜çš„é¡Œç›®
    unlabeled_mask = df['label'].isna() | (df['label'] == 'å°šæœªåˆ¤æ–·') | (df['label'] == '') | df['label'].isnull()
    unlabeled_indices = df[unlabeled_mask].index.tolist()
    
    if unlabeled_indices:
        # å›å‚³ç¬¬ä¸€å€‹æœªæ¨™è¨˜é¡Œç›®çš„ç´¢å¼•
        return unlabeled_indices[0]
    else:
        # å…¨éƒ¨æ¨™è¨˜å®Œç•¢ï¼Œå›å‚³æœ€å¾Œä¸€é¡Œ
        return len(df) - 1

def show_scam_posts_view() -> None:
    """é¡¯ç¤ºæ‰€æœ‰è¢«æ¨™è¨˜ç‚ºè©é¨™çš„è²¼æ–‡"""
    st.markdown("### ğŸ“± è©é¨™è²¼æ–‡ç€è¦½")
    
    # åˆå§‹åŒ– session state ç”¨æ–¼è·³è½‰åˆ°ç›¸ä¼¼æœå°‹
    if 'jump_to_similar_search' not in st.session_state:
        st.session_state.jump_to_similar_search = False
    if 'similar_search_content' not in st.session_state:
        st.session_state.similar_search_content = ""
    if 'auto_switch_to_similar' not in st.session_state:
        st.session_state.auto_switch_to_similar = False
    if 'current_tab' not in st.session_state:
        st.session_state.current_tab = "tab1"
    
    # å–å¾—æ‰€æœ‰è¢«æ¨™è¨˜ç‚ºè©é¨™çš„è²¼æ–‡
    query = """
        SELECT pos_tid, content, label, note, group_id
        FROM candidates 
        WHERE label = 'æ˜¯'
        ORDER BY pos_tid DESC
    """
    scam_posts = pd.read_sql(query, labeling_engine)
    
    if len(scam_posts) == 0:
        st.info("ç›®å‰é‚„æ²’æœ‰è¢«æ¨™è¨˜ç‚ºè©é¨™çš„è²¼æ–‡")
        return
    
    # é¡¯ç¤ºè²¼æ–‡æ•¸é‡
    st.caption(f"å…±æ‰¾åˆ° {len(scam_posts)} å‰‡è©é¨™è²¼æ–‡")
    
    # é¡¯ç¤ºæ¯å‰‡è²¼æ–‡
    for idx, post in scam_posts.iterrows():
        with st.container():
            st.markdown("---")
            
            # è²¼æ–‡æ¨™é¡Œå’ŒæŒ‰éˆ•å€åŸŸ
            col_title, col_button = st.columns([3, 1])
            with col_title:
                st.markdown(f"**è²¼æ–‡ IDï¼š** `{post['pos_tid']}`")
            with col_button:
                if st.button("ğŸ” å°‹æ‰¾é¡ä¼¼", key=f"find_similar_{post['pos_tid']}", help="é»æ“Šå°‹æ‰¾èˆ‡æ­¤è²¼æ–‡ç›¸ä¼¼çš„è²¼æ–‡"):
                    st.session_state.jump_to_similar_search = True
                    st.session_state.similar_search_content = post['content']
                    st.session_state.auto_switch_to_similar = True
                    st.session_state.current_tab = "ğŸ” ç›¸ä¼¼è²¼æ–‡æœå°‹"  # ç›´æ¥è¨­ç½®è¦è·³è½‰çš„åˆ†é 
                    st.rerun()
            
            # è²¼æ–‡å…§å®¹
            st.text_area("è²¼æ–‡å…§å®¹", post['content'], height=200, disabled=True, 
                        label_visibility="collapsed", 
                        key=f"scam_posts_{idx}_{post['pos_tid']}")
            
            # è²¼æ–‡è³‡è¨Š
            col1, col2 = st.columns(2)
            with col1:
                st.caption(f"ç¾¤çµ„ï¼š{post['group_id']}")
            with col2:
                if pd.notna(post['note']) and post['note']:
                    st.caption(f"å‚™è¨»ï¼š{post['note']}")
    
    # å¦‚æœé»æ“Šäº†å°‹æ‰¾é¡ä¼¼æŒ‰éˆ•ï¼Œé¡¯ç¤ºè·³è½‰æç¤º
    if st.session_state.jump_to_similar_search:
        st.success("âœ… å·²æº–å‚™è·³è½‰åˆ°ç›¸ä¼¼è²¼æ–‡æœå°‹é é¢")
        st.info("ğŸ’¡ è«‹åˆ‡æ›åˆ°ã€ŒğŸ” ç›¸ä¼¼è²¼æ–‡æœå°‹ã€åˆ†é æŸ¥çœ‹çµæœ")
        
        # é‡ç½®è·³è½‰ç‹€æ…‹
        st.session_state.jump_to_similar_search = False

def show_word_analysis() -> None:
    """é¡¯ç¤ºè©å½™åˆ†æçµæœ"""
    st.markdown("### ğŸ“Š è©å½™åˆ†æ")
    
    # å¾è³‡æ–™åº«è®€å–ä¸Šæ¬¡ç”Ÿæˆæ™‚é–“
    with labeling_engine.connect() as conn:
        result = conn.execute(text("SELECT value, updated_at FROM system_settings WHERE key = 'last_word_analysis_time'"))
        row = result.fetchone()
        last_generation_time = row[0] if row else None
    
    # é¡¯ç¤ºä¸Šæ¬¡ç”Ÿæˆæ™‚é–“
    if last_generation_time:
        st.info(f"ä¸Šæ¬¡ç”Ÿæˆæ™‚é–“ï¼š{last_generation_time}")
    
    # æ‰‹å‹•ç”ŸæˆæŒ‰éˆ•
    if st.button("ğŸ”„ ç”Ÿæˆè©å½™åˆ†æåœ–è¡¨", type="primary", key="generate_word_analysis"):
        try:
            
            # åŸ·è¡Œåˆ†æç¨‹å¼
            subprocess.run(['python', 'analyze_scam_posts.py'], check=True)
            
            # æ›´æ–°è³‡æ–™åº«ä¸­çš„ç”Ÿæˆæ™‚é–“
            current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with labeling_engine.begin() as conn:
                conn.execute(
                    text("""
                        INSERT INTO system_settings (key, value, updated_at)
                        VALUES ('last_word_analysis_time', :value, CURRENT_TIMESTAMP)
                        ON CONFLICT (key) DO UPDATE
                        SET value = :value, updated_at = CURRENT_TIMESTAMP
                    """),
                    {"value": current_time}
                )
            
            st.success("âœ… è©å½™åˆ†æåœ–è¡¨ç”ŸæˆæˆåŠŸï¼")
            st.rerun()
            
        except Exception as e:
            st.error(f"ç”Ÿæˆåœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            return
    
    # é¡¯ç¤ºåœ–è¡¨
    try:
        # æª¢æŸ¥åœ–è¡¨æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if os.path.exists('word_frequency.png') and os.path.exists('wordcloud.png'):
            # é¡¯ç¤ºè©é »åˆ†æåœ–
            st.markdown("#### ğŸ“ˆ è©é »åˆ†æåœ–")
            st.image('word_frequency.png', use_container_width=True)
            
            # é¡¯ç¤ºæ–‡å­—é›²åœ–
            st.markdown("#### â˜ï¸ æ–‡å­—é›²")
            st.image('wordcloud.png', use_container_width=True)
        else:
            st.info("è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•ç”Ÿæˆè©å½™åˆ†æåœ–è¡¨")
    except Exception as e:
        st.error(f"è®€å–åœ–è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

def show_post_search() -> None:
    """æ ¹æ“š pos_tid æŸ¥è©¢ç‰¹å®šè²¼æ–‡"""
    
    # å»ºç«‹åˆ†é 
    search_tab1, search_tab2 = st.tabs(["ğŸ“ æ¨™è¨˜è³‡æ–™åº«æŸ¥è©¢", "ğŸ” åŸå§‹è³‡æ–™åº«æŸ¥è©¢"])
    
    # åˆå§‹åŒ–å…±äº«çš„æœå°‹ ID
    if 'shared_search_id' not in st.session_state:
        st.session_state.shared_search_id = ""
    
    with search_tab1:
        st.text("éå…¨è²¼æ–‡æŸ¥è©¢ï¼Œéœ€è¦æœ‰æ¨™è¨˜éæ˜¯æˆ–å¦çš„è³‡æ–™æ‰å¯ä»¥æŸ¥è©¢")
        
        # åˆå§‹åŒ–ç·¨è¼¯ç‹€æ…‹
        if 'has_unsaved_changes' not in st.session_state:
            st.session_state.has_unsaved_changes = False
        if 'edited_label' not in st.session_state:
            st.session_state.edited_label = None
        if 'edited_note' not in st.session_state:
            st.session_state.edited_note = None
        if 'current_post_id' not in st.session_state:
            st.session_state.current_post_id = None
        
        # æœå°‹è¼¸å…¥æ¡†
        pos_tid = st.text_input("è«‹è¼¸å…¥è²¼æ–‡ ID (pos_tid)", 
                               value=st.session_state.shared_search_id,
                               key="labeling_search")
        
        # æ›´æ–°å…±äº«çš„æœå°‹ ID
        if pos_tid != st.session_state.shared_search_id:
            st.session_state.shared_search_id = pos_tid
        
        if pos_tid:
            # æŸ¥è©¢è²¼æ–‡
            query = """
                SELECT pos_tid, content, label, note, group_id
                FROM candidates 
                WHERE pos_tid = :pos_tid
            """
            result = pd.read_sql(text(query), labeling_engine, params={"pos_tid": pos_tid})
            
            if len(result) == 0:
                st.warning(f"æ‰¾ä¸åˆ° ID ç‚º {pos_tid} çš„è²¼æ–‡")
                st.info("ğŸ’¡ æ‚¨å¯ä»¥åˆ‡æ›åˆ°ã€ŒåŸå§‹è³‡æ–™åº«æŸ¥è©¢ã€åˆ†é æŸ¥çœ‹æ­¤è²¼æ–‡æ˜¯å¦åœ¨åŸå§‹è³‡æ–™åº«ä¸­")
            else:
                post = result.iloc[0]
                
                # å¦‚æœæ˜¯æ–°è²¼æ–‡ï¼Œé‡ç½®ç·¨è¼¯ç‹€æ…‹
                if st.session_state.current_post_id != pos_tid:
                    st.session_state.current_post_id = pos_tid
                    st.session_state.has_unsaved_changes = False
                    st.session_state.edited_label = post['label']
                    st.session_state.edited_note = post['note']
                
                # é¡¯ç¤ºè²¼æ–‡å…§å®¹
                st.markdown("---")
                st.markdown(f"**è²¼æ–‡ IDï¼š** `{post['pos_tid']}`")
                # è²¼æ–‡å…§å®¹ï¼ˆæ”¹ç‚ºç´”æ–‡å­—é¡¯ç¤ºï¼‰
                st.text_area("è²¼æ–‡å…§å®¹", post['content'], height=200, disabled=True, label_visibility="collapsed", key=f"scam_posts_search_{post['pos_tid']}_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}")
                
                # ç·¨è¼¯å€åŸŸ
                st.markdown("### ç·¨è¼¯æ¨™è¨˜")
                col1, col2 = st.columns(2)
                
                with col1:
                    st.caption(f"ç¾¤çµ„ï¼š{post['group_id']}")
                    # æ¨™è¨˜é¸æ“‡
                    new_label = st.radio(
                        "æ¨™è¨˜",
                        options=["æ˜¯", "å¦", "å°šæœªåˆ¤æ–·"],
                        index=["æ˜¯", "å¦", "å°šæœªåˆ¤æ–·"].index(st.session_state.edited_label if st.session_state.edited_label else "å°šæœªåˆ¤æ–·"),
                        key=f"label_edit_{post['pos_tid']}"
                    )
                    # åªæœ‰ç•¶å¯¦éš›å€¼æ”¹è®Šæ™‚æ‰æ¨™è¨˜ç‚ºæœªå­˜æª”
                    if new_label != post['label']:
                        st.session_state.edited_label = new_label
                        st.session_state.has_unsaved_changes = True
                    elif new_label == post['label'] and st.session_state.edited_label != post['label']:
                        st.session_state.edited_label = new_label
                        st.session_state.has_unsaved_changes = False
                
                with col2:
                    # å‚™è¨»ç·¨è¼¯
                    new_note = st.text_area(
                        "å‚™è¨»",
                        value=st.session_state.edited_note if pd.notna(st.session_state.edited_note) else "",
                        key=f"note_edit_{post['pos_tid']}"
                    )
                    # åªæœ‰ç•¶å¯¦éš›å€¼æ”¹è®Šæ™‚æ‰æ¨™è¨˜ç‚ºæœªå­˜æª”
                    if new_note != (post['note'] if pd.notna(post['note']) else ""):
                        st.session_state.edited_note = new_note
                        st.session_state.has_unsaved_changes = True
                    elif new_note == (post['note'] if pd.notna(post['note']) else "") and st.session_state.edited_note != post['note']:
                        st.session_state.edited_note = new_note
                        st.session_state.has_unsaved_changes = False
                
                # å­˜æª”æŒ‰éˆ•
                col_save1, col_save2 = st.columns([1, 3])
                with col_save1:
                    if st.button("ğŸ’¾ å„²å­˜æ›´æ”¹", type="primary", disabled=not st.session_state.has_unsaved_changes):
                        save_label_only(post['pos_tid'], st.session_state.edited_label, st.session_state.edited_note, post['group_id'])
                        st.session_state.has_unsaved_changes = False
                        st.success("âœ… å·²å„²å­˜æ›´æ”¹")
                        st.rerun()
                
                # é¡¯ç¤ºæœªå­˜æª”æé†’
                if st.session_state.has_unsaved_changes:
                    st.warning("âš ï¸ æ‚¨æœ‰æœªå­˜æª”çš„æ›´æ”¹ï¼")
    
    with search_tab2:
        st.text("æŸ¥è©¢åŸå§‹è³‡æ–™åº«ä¸­çš„æ‰€æœ‰è²¼æ–‡")
        
        # æœå°‹è¼¸å…¥æ¡†ï¼ˆä½¿ç”¨å…±äº«çš„æœå°‹ IDï¼‰
        source_pos_tid = st.text_input("è«‹è¼¸å…¥è²¼æ–‡ ID (pos_tid)", 
                                      value=st.session_state.shared_search_id,
                                      key="source_search")
        
        # æ›´æ–°å…±äº«çš„æœå°‹ ID
        if source_pos_tid != st.session_state.shared_search_id:
            st.session_state.shared_search_id = source_pos_tid
        
        if source_pos_tid:
            # æŸ¥è©¢åŸå§‹è³‡æ–™åº«
            query = """
                SELECT pos_tid, content, created_time, date, post_type, page_name, 
                       reaction_all, comment_count, share_count
                FROM posts_deduplicated 
                WHERE pos_tid = :pos_tid
            """
            try:
                result = pd.read_sql(text(query), source_engine, params={"pos_tid": source_pos_tid})
                
                if len(result) == 0:
                    st.warning(f"åœ¨åŸå§‹è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ° ID ç‚º {source_pos_tid} çš„è²¼æ–‡")
                else:
                    post = result.iloc[0]
                    
                    # é¡¯ç¤ºè²¼æ–‡å…§å®¹
                    st.markdown("---")
                    st.markdown(f"**è²¼æ–‡ IDï¼š** `{post['pos_tid']}`")
                    st.text_area("è²¼æ–‡å…§å®¹", post['content'], height=200, disabled=True, label_visibility="collapsed", key=f"source_posts_search_{post['pos_tid']}_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}")
                    
                    # é¡¯ç¤ºè²¼æ–‡è³‡è¨Š
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.caption(f"å»ºç«‹æ™‚é–“ï¼š{post['created_time']}")
                        st.caption(f"æ—¥æœŸï¼š{post['date']}")
                    with col2:
                        st.caption(f"è²¼æ–‡é¡å‹ï¼š{post['post_type']}")
                        st.caption(f"é é¢åç¨±ï¼š{post['page_name']}")
                    with col3:
                        st.caption(f"äº’å‹•æ•¸ï¼š{post['reaction_all']}")
                        st.caption(f"ç•™è¨€æ•¸ï¼š{post['comment_count']}")
                        st.caption(f"åˆ†äº«æ•¸ï¼š{post['share_count']}")
                    
                    # æª¢æŸ¥æ˜¯å¦å·²åœ¨æ¨™è¨˜è³‡æ–™åº«ä¸­
                    check_query = "SELECT label FROM candidates WHERE pos_tid = :pos_tid"
                    check_result = pd.read_sql(text(check_query), labeling_engine, params={"pos_tid": source_pos_tid})
                    
                    if len(check_result) > 0:
                        st.info(f"æ­¤è²¼æ–‡å·²åœ¨æ¨™è¨˜è³‡æ–™åº«ä¸­ï¼Œç•¶å‰æ¨™è¨˜ï¼š{check_result.iloc[0]['label']}")
                    else:
                        st.info("æ­¤è²¼æ–‡å°šæœªåŠ å…¥æ¨™è¨˜è³‡æ–™åº«")
                        
                        # æä¾›å¿«é€ŸåŠ å…¥æ¨™è¨˜è³‡æ–™åº«çš„æŒ‰éˆ•
                        if st.button("ğŸ“ åŠ å…¥æ¨™è¨˜è³‡æ–™åº«", type="primary"):
                            try:
                                # æ’å…¥åˆ°æ¨™è¨˜è³‡æ–™åº«
                                insert_sql = """
                                    INSERT INTO candidates (pos_tid, content, group_id, label, note)
                                    VALUES (:pos_tid, :content, 999, 'å°šæœªåˆ¤æ–·', '')
                                """
                                with labeling_engine.begin() as conn:
                                    conn.execute(text(insert_sql), {
                                        "pos_tid": post['pos_tid'],
                                        "content": post['content']
                                    })
                                st.success("âœ… å·²æˆåŠŸåŠ å…¥æ¨™è¨˜è³‡æ–™åº«ï¼")
                                st.rerun()
                            except Exception as e:
                                st.error(f"åŠ å…¥æ¨™è¨˜è³‡æ–™åº«å¤±æ•—ï¼š{str(e)}")
                
            except Exception as e:
                st.error(f"æŸ¥è©¢åŸå§‹è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

def show_keyword_search() -> None:
    """é¡¯ç¤ºé—œéµå­—æœå°‹æ¨¡å¼çš„ä»‹é¢"""
    
    # åˆå§‹åŒ–åˆ†é ç›¸é—œçš„ session state
    if 'search_page' not in st.session_state:
        st.session_state.search_page = 0
    if 'search_results' not in st.session_state:
        st.session_state.search_results = None
    if 'search_keywords' not in st.session_state:
        st.session_state.search_keywords = None
    if 'exclude_keywords' not in st.session_state:
        st.session_state.exclude_keywords = None
    if 'search_logic' not in st.session_state:
        st.session_state.search_logic = None
    if 'search_table' not in st.session_state:
        st.session_state.search_table = 'posts_deduplicated'
    if 'label_message' not in st.session_state:
        st.session_state.label_message = None
    if 'label_message_pos_tid' not in st.session_state:
        st.session_state.label_message_pos_tid = None
    
    # è³‡æ–™è¡¨é¸æ“‡
    table_options = {
        'posts_deduplicated': 'å»é‡å¾Œè²¼æ–‡ (posts_deduplicated)',
        'posts': 'åŸå§‹è²¼æ–‡ (posts)'
    }
    
    selected_table = st.selectbox(
        "é¸æ“‡è¦æœå°‹çš„è³‡æ–™è¡¨",
        options=list(table_options.keys()),
        format_func=lambda x: table_options[x],
        index=list(table_options.keys()).index(st.session_state.search_table),
        help="é¸æ“‡è¦æœå°‹çš„è³‡æ–™è¡¨ã€‚posts_deduplicated æ˜¯å»é‡å¾Œçš„è³‡æ–™ï¼Œposts æ˜¯åŸå§‹è³‡æ–™"
    )
    
    # æ›´æ–° session state
    if selected_table != st.session_state.search_table:
        st.session_state.search_table = selected_table
        # æ¸…é™¤ä¹‹å‰çš„æœå°‹çµæœ
        st.session_state.search_results = None
        st.session_state.search_page = 0
    
    # é—œéµå­—è¼¸å…¥å€åŸŸ
    keywords_input = st.text_area(
        "è«‹è¼¸å…¥é—œéµå­—(æ¯è¡Œä¸€å€‹) ",
        value="\n".join(st.session_state.search_keywords) if st.session_state.search_keywords else "",
        help="æ¯è¡Œè¼¸å…¥ä¸€å€‹é—œéµå­—ï¼Œç³»çµ±æœƒæ ¹æ“šé¸æ“‡çš„é‚è¼¯é€²è¡Œæœå°‹"
    )
    
    exclude_keywords_input = st.text_area(
        "è«‹è¼¸å…¥è¦æ’é™¤çš„é—œéµå­—(æ¯è¡Œä¸€å€‹)",
        value="\n".join(st.session_state.exclude_keywords) if st.session_state.exclude_keywords else "",
        help="æ¯è¡Œè¼¸å…¥ä¸€å€‹è¦æ’é™¤çš„é—œéµå­—ï¼Œç¬¦åˆé€™äº›é—œéµå­—çš„è²¼æ–‡å°‡ä¸æœƒé¡¯ç¤º"
    )

    st.text("(æœ€å¤š500ç­†çµæœ) \n (æ™‚é–“æœ€é•·éœ€è¦30ç§’) \n (è²¼æ–‡å‡ºç¾çš„é †åºæ˜¯éš¨æ©Ÿçš„)")
    
    # å°‡è¼¸å…¥è½‰æ›ç‚ºé—œéµå­—åˆ—è¡¨
    keywords = [kw.strip() for kw in keywords_input.split('\n') if kw.strip()]
    exclude_keywords = [kw.strip() for kw in exclude_keywords_input.split('\n') if kw.strip()]
    
    # æœå°‹é‚è¼¯é¸æ“‡
    search_logic = st.radio(
        "æœå°‹é‚è¼¯",
        options=["OR", "AND"],
        index=0 if st.session_state.search_logic != "AND" else 1,
        help="ORï¼šç¬¦åˆä»»ä¸€é—œéµå­—å³é¡¯ç¤º\nANDï¼šå¿…é ˆç¬¦åˆæ‰€æœ‰é—œéµå­—æ‰é¡¯ç¤º"
    )
    
    # æœå°‹æŒ‰éˆ•
    if st.button("ğŸ” é–‹å§‹æœå°‹", type="primary", disabled=not keywords):
        try:
            # åŸ·è¡Œæœå°‹
            results_df = fetch_candidate_posts(
                source_engine=source_engine,
                keywords=keywords,
                exclude_keywords=exclude_keywords,  # æ–°å¢æ’é™¤é—œéµå­—åƒæ•¸
                limit=500,  # å…ˆå–å¾—è¼ƒå¤šçµæœï¼Œä½†åˆ†é é¡¯ç¤º
                group_count=1,  # æœå°‹æ¨¡å¼ä¸‹ä¸éœ€è¦åˆ†çµ„
                search_logic=search_logic,
                table_name=st.session_state.search_table  # ä½¿ç”¨é¸æ“‡çš„è³‡æ–™è¡¨
            )
            
            if len(results_df) == 0:
                st.warning("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è²¼æ–‡")
                st.session_state.search_results = None
                st.session_state.search_page = 0
                return
            
            # å„²å­˜æœå°‹çµæœå’Œåƒæ•¸åˆ° session state
            st.session_state.search_results = results_df
            st.session_state.search_keywords = keywords
            st.session_state.exclude_keywords = exclude_keywords
            st.session_state.search_logic = search_logic
            st.session_state.search_page = 0  # é‡ç½®é ç¢¼
            
            st.success(f"æ‰¾åˆ° {len(results_df)} å‰‡ç¬¦åˆæ¢ä»¶çš„è²¼æ–‡")
            st.rerun()
            
        except Exception as e:
            st.error(f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
    
    # å¦‚æœæœ‰æœå°‹çµæœï¼Œé¡¯ç¤ºåˆ†é å…§å®¹
    if st.session_state.search_results is not None:
        num_per_page = 20
        df = st.session_state.search_results
        total_pages = (len(df) + (num_per_page - 1)) // num_per_page  # å‘ä¸Šå–æ•´ï¼Œè¨ˆç®—ç¸½é æ•¸
        
        # é¡¯ç¤ºåˆ†é è³‡è¨Š
        st.markdown(f"---\n#### æœå°‹çµæœï¼ˆç¬¬ {st.session_state.search_page + 1} é ï¼Œå…± {total_pages} é ï¼‰")
        st.caption(f"æœå°‹è³‡æ–™è¡¨ï¼š{table_options[st.session_state.search_table]}")
        
        # è¨ˆç®—ç•¶å‰é çš„è³‡æ–™ç¯„åœ
        start_idx = st.session_state.search_page * num_per_page
        end_idx = min(start_idx + num_per_page, len(df))
        
        # é¡¯ç¤ºç•¶å‰é çš„è³‡æ–™
        for idx in range(start_idx, end_idx):
            row = df.iloc[idx]
            with st.container():
                st.markdown("---")
                # è²¼æ–‡æ¨™é¡Œ
                st.markdown(f"**è²¼æ–‡ IDï¼š** `{row['pos_tid']}`")
                # è²¼æ–‡å…§å®¹
                st.text_area("è²¼æ–‡å…§å®¹", row['content'], height=200, disabled=True, label_visibility="collapsed", key=f"keyword_search_{st.session_state.search_page}_{idx}")
                
                # æ¨™è¨˜å€åŸŸ
                col1, col2, col3 = st.columns([1, 1, 2])
                with col1:
                    if st.button("âœ… æ˜¯", key=f"keyword_yes_{st.session_state.search_page}_{idx}_{row['pos_tid']}"):
                        try:
                            save_label_only(row['pos_tid'], "æ˜¯", "", 999)
                            st.session_state.label_message = "å·²æ¨™è¨˜ç‚ºã€Œæ˜¯ã€"
                            st.session_state.label_message_pos_tid = row['pos_tid']
                            st.rerun()
                        except Exception as e:
                            st.error(f"æ¨™è¨˜å¤±æ•—ï¼š{str(e)}")
                with col2:
                    if st.button("âŒ å¦", key=f"keyword_no_{st.session_state.search_page}_{idx}_{row['pos_tid']}"):
                        try:
                            save_label_only(row['pos_tid'], "å¦", "", 999)
                            st.session_state.label_message = "å·²æ¨™è¨˜ç‚ºã€Œå¦ã€"
                            st.session_state.label_message_pos_tid = row['pos_tid']
                            st.rerun()
                        except Exception as e:
                            st.error(f"æ¨™è¨˜å¤±æ•—ï¼š{str(e)}")
                with col3:
                    # é¡¯ç¤ºç•¶å‰æ¨™è¨˜ç‹€æ…‹
                    current_label = row.get('label')
                    if pd.notna(current_label) and current_label:
                        st.info(f"ç•¶å‰æ¨™è¨˜ï¼š{current_label}")
                
                # é¡¯ç¤ºæ¨™è¨˜è¨Šæ¯ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
                if st.session_state.label_message and st.session_state.label_message_pos_tid == row['pos_tid']:
                    st.success(st.session_state.label_message, icon="âœ…" if "æ˜¯" in st.session_state.label_message else "âŒ")
                    # æ¸…é™¤è¨Šæ¯ï¼Œé¿å…é‡è¤‡é¡¯ç¤º
                    st.session_state.label_message = None
                    st.session_state.label_message_pos_tid = None
        
        # åˆ†é å°èˆªæŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 2, 1])
        with col1:
            if st.button("â¬…ï¸ ä¸Šä¸€é ", disabled=st.session_state.search_page <= 0, key="keyword_prev_page"):
                st.session_state.search_page -= 1
                # ä½¿ç”¨ JavaScript è·³è½‰åˆ°é é¢é ‚éƒ¨
                st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
                st.rerun()
        with col2:
            # é ç¢¼è¼¸å…¥æ¡†
            target_page = st.number_input(
                "å‰å¾€é ç¢¼",
                min_value=1,
                max_value=total_pages,
                value=st.session_state.search_page + 1,
                label_visibility="collapsed",
                key="keyword_page_input"
            )
            # ç•¶é ç¢¼æ”¹è®Šæ™‚è·³è½‰
            if target_page != st.session_state.search_page + 1:
                st.session_state.search_page = target_page - 1
                # ä½¿ç”¨ JavaScript è·³è½‰åˆ°é é¢é ‚éƒ¨
                st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
                st.rerun()
        with col3:
            if st.button("ä¸‹ä¸€é  â¡ï¸", disabled=st.session_state.search_page >= total_pages - 1, key="keyword_next_page"):
                st.session_state.search_page += 1
                # ä½¿ç”¨ JavaScript è·³è½‰åˆ°é é¢é ‚éƒ¨
                st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
                st.rerun()

def show_similar_posts_search():
    """é¡¯ç¤ºç›¸ä¼¼è²¼æ–‡æœå°‹æ¨¡å¼çš„ä»‹é¢"""
    
    # åˆå§‹åŒ– session state
    if 'similar_search_results' not in st.session_state:
        st.session_state.similar_search_results = None
    if 'similar_search_query' not in st.session_state:
        st.session_state.similar_search_query = None
    if 'similar_search_page' not in st.session_state:
        st.session_state.similar_search_page = 0
    if 'similar_label_message' not in st.session_state:
        st.session_state.similar_label_message = None
    if 'similar_label_message_pos_tid' not in st.session_state:
        st.session_state.similar_label_message_pos_tid = None
    if 'similar_search_content' not in st.session_state:
        st.session_state.similar_search_content = ""
    # æ–°å¢é€²åº¦é¡¯ç¤ºç›¸é—œçš„ session state
    if 'search_progress' not in st.session_state:
        st.session_state.search_progress = None
    if 'search_progress_message' not in st.session_state:
        st.session_state.search_progress_message = ""
    # æ–°å¢æœå°‹é€²ç¨‹ç›¸é—œçš„ session state
    if 'search_process' not in st.session_state:
        st.session_state.search_process = None
    if 'search_in_progress' not in st.session_state:
        st.session_state.search_in_progress = False
    
    st.markdown("### ğŸ” ç›¸ä¼¼è²¼æ–‡æœå°‹")
    st.markdown("è¼¸å…¥ä¸€æ®µæ–‡å­—ï¼Œç³»çµ±æœƒæ‰¾åˆ°èªæ„ç›¸ä¼¼çš„è²¼æ–‡")
    
    # æœå°‹åƒæ•¸è¨­å®š
    col1, col2 = st.columns(2)
    with col1:
        limit = st.number_input("æœ€å¤§çµæœæ•¸é‡", min_value=5, max_value=100, value=20, step=5)
    with col2:
        threshold = st.slider("ç›¸ä¼¼åº¦é–¾å€¼", min_value=0.1, max_value=0.9, value=0.7, step=0.01, help="æ•¸å€¼è¶Šé«˜ï¼Œçµæœè¶Šç›¸ä¼¼")
    
    # æ–°å¢éš¨æ©Ÿæœå°‹é¸é …
    random_search = st.checkbox("éš¨æ©Ÿæœå°‹ (Random Search)", value=False, key="similar_random_search")
    
    # åœæ­¢æœå°‹æŒ‰éˆ•
    if st.session_state.search_in_progress:
        if st.button("â¹ï¸ åœæ­¢æœå°‹", type="secondary", key="stop_search_button"):
            if st.session_state.search_process:
                st.session_state.search_process.stop_search()
            st.session_state.search_in_progress = False
            st.success("å·²ç™¼é€åœæ­¢æœå°‹æŒ‡ä»¤")
            st.rerun()
    
    # æŸ¥è©¢æ–‡å­—è¼¸å…¥
    query_text = st.text_area(
        "è«‹è¼¸å…¥è¦æœå°‹çš„æ–‡å­—",
        value=st.session_state.similar_search_content if st.session_state.similar_search_content else 
              (st.session_state.similar_search_query if st.session_state.similar_search_query else ""),
        height=100,
        help="è¼¸å…¥ä»»ä½•æ–‡å­—ï¼Œç³»çµ±æœƒæ‰¾åˆ°èªæ„ç›¸ä¼¼çš„è²¼æ–‡"
    )
    
    # å¦‚æœå¾è©é¨™è²¼æ–‡ç€è¦½è·³è½‰éä¾†ï¼Œè‡ªå‹•åŸ·è¡Œæœå°‹
    if st.session_state.similar_search_content and not st.session_state.similar_search_query:
        st.session_state.similar_search_query = st.session_state.similar_search_content
        # æ¸…é™¤è·³è½‰å…§å®¹ï¼Œé¿å…é‡è¤‡åŸ·è¡Œ
        st.session_state.similar_search_content = ""
        
        # è¨­å®šæœå°‹ç‹€æ…‹
        st.session_state.search_in_progress = True
        st.session_state.search_progress = None
        st.session_state.search_progress_message = ""
        
        # åˆå§‹åŒ–æœå°‹é€²ç¨‹
        if st.session_state.search_process is None:
            st.session_state.search_process = SimilarSearchProcess()
        
        # å•Ÿå‹•æœå°‹
        st.session_state.search_process.start_search(
            query_text=st.session_state.similar_search_query,
            limit=limit,
            threshold=0.7,  # å›ºå®šä½¿ç”¨ 0.7 é–¾å€¼
            random_search=random_search
        )
        
        # é¡¯ç¤ºæœå°‹ç‹€æ…‹ä¸¦é‡æ–°è¼‰å…¥é é¢
        st.info("æ­£åœ¨æº–å‚™è‡ªå‹•æœå°‹...")
        st.rerun()
    
    # æ‰‹å‹•æœå°‹æŒ‰éˆ•
    if st.button("ğŸ” é–‹å§‹æœå°‹", type="primary", disabled=not query_text.strip(), key="similar_search_button"):
        # è¨­å®šæœå°‹ç‹€æ…‹
        st.session_state.search_in_progress = True
        st.session_state.similar_search_query = query_text
        st.session_state.similar_search_results = None  # æ¸…é™¤ä¹‹å‰çš„çµæœ
        st.session_state.search_progress = None
        st.session_state.search_progress_message = ""
        
        # åˆå§‹åŒ–æœå°‹é€²ç¨‹
        if st.session_state.search_process is None:
            st.session_state.search_process = SimilarSearchProcess()
        
        # å•Ÿå‹•æœå°‹
        st.session_state.search_process.start_search(
            query_text=query_text,
            limit=limit,
            threshold=threshold,
            random_search=random_search
        )
        
        # é¡¯ç¤ºæœå°‹ç‹€æ…‹ä¸¦é‡æ–°è¼‰å…¥é é¢
        st.info("æ­£åœ¨æº–å‚™æœå°‹...")
        st.rerun()
    
    # æª¢æŸ¥æœå°‹é€²åº¦
    if st.session_state.search_in_progress and st.session_state.search_process:
        # æª¢æŸ¥é€²åº¦
        progress = st.session_state.search_process.get_progress()
        if progress:
            st.session_state.search_progress = progress
            st.session_state.search_progress_message = progress['message']
        
        # æª¢æŸ¥çµæœ
        result = st.session_state.search_process.get_result()
        if result:
            st.session_state.search_in_progress = False
            
            if 'error' in result:
                st.error(f"æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{result['error']}")
                st.session_state.similar_search_results = None
                st.session_state.similar_search_page = 0
            else:
                # å°‡çµæœè½‰æ›å› DataFrame
                if result['data']:
                    results_df = pd.DataFrame(result['data'], columns=result['columns'])
                    st.session_state.similar_search_results = results_df
                    st.session_state.similar_search_page = 0
                    st.success(f"æ‰¾åˆ° {len(results_df)} å‰‡ç›¸ä¼¼è²¼æ–‡")
                else:
                    st.warning("æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„è²¼æ–‡ï¼Œè«‹å˜—è©¦é™ä½ç›¸ä¼¼åº¦é–¾å€¼æˆ–ä¿®æ”¹æœå°‹æ–‡å­—")
                    st.session_state.similar_search_results = None
                    st.session_state.similar_search_page = 0
            
            st.rerun()
        
        # é¡¯ç¤ºé€²åº¦
        if st.session_state.search_progress_message:
            st.info(st.session_state.search_progress_message)
        else:
            st.info("æ­£åœ¨é€²è¡Œæœå°‹...")
        
        # è‡ªå‹•é‡æ–°è¼‰å…¥ä»¥æ›´æ–°é€²åº¦
        time.sleep(0.5)
        st.rerun()
    
    # æ¸…ç†è³‡æºæŒ‰éˆ•ï¼ˆå¯é¸ï¼‰
    if st.button("ğŸ§¹ æ¸…ç†è¨˜æ†¶é«”", key="cleanup_memory", help="å¦‚æœé‡åˆ°è¨˜æ†¶é«”å•é¡Œï¼Œå¯ä»¥é»æ“Šæ­¤æŒ‰éˆ•æ¸…ç†è³‡æº"):
        if st.session_state.search_process:
            st.session_state.search_process.stop_search()
            st.session_state.search_process = None
            st.success("å·²æ¸…ç†è¨˜æ†¶é«”è³‡æº")
            st.rerun()
        else:
            st.info("æ²’æœ‰éœ€è¦æ¸…ç†çš„è³‡æº")
        
        # é‡ç½®æœå°‹ç‹€æ…‹
        st.session_state.search_in_progress = False
        st.session_state.search_progress = None
        st.session_state.search_progress_message = ""
    
    # å¦‚æœæœ‰æœå°‹çµæœï¼Œé¡¯ç¤ºåˆ†é å…§å®¹
    if st.session_state.similar_search_results is not None:
        num_per_page = 10
        df = st.session_state.similar_search_results
        total_pages = (len(df) + (num_per_page - 1)) // num_per_page
        
        # é¡¯ç¤ºæœå°‹è³‡è¨Š
        st.markdown(f"---\n#### æœå°‹çµæœï¼ˆç¬¬ {st.session_state.similar_search_page + 1} é ï¼Œå…± {total_pages} é ï¼‰")
        st.caption(f"æŸ¥è©¢æ–‡å­—ï¼š{st.session_state.similar_search_query}")
        st.caption(f"å…±æ‰¾åˆ° {len(df)} å‰‡ç›¸ä¼¼è²¼æ–‡")
        
        # è¨ˆç®—ç•¶å‰é çš„è³‡æ–™ç¯„åœ
        start_idx = st.session_state.similar_search_page * num_per_page
        end_idx = min(start_idx + num_per_page, len(df))
        
        # é¡¯ç¤ºç•¶å‰é çš„è³‡æ–™
        for idx in range(start_idx, end_idx):
            row = df.iloc[idx]
            with st.container():
                st.markdown("---")
                
                # ç›¸ä¼¼åº¦åˆ†æ•¸
                similarity_score = row['similarity_score']
                st.markdown(f"**ç›¸ä¼¼åº¦ï¼š** {similarity_score:.3f}")
                
                # è²¼æ–‡æ¨™é¡Œ
                st.markdown(f"**è²¼æ–‡ IDï¼š** `{row['pos_tid']}`")
                st.caption(f"é é¢ï¼š{row['page_name']} | å»ºç«‹æ™‚é–“ï¼š{row['created_time']}")
                
                # è²¼æ–‡å…§å®¹
                st.text_area("è²¼æ–‡å…§å®¹", row['content'], height=150, disabled=True, 
                           label_visibility="collapsed", 
                           key=f"similar_search_{st.session_state.similar_search_page}_{idx}")
                
                # æ¨™è¨˜å€åŸŸ
                col1, col2, col3 = st.columns([1, 1, 2])
                with col1:
                    if st.button("âœ… æ˜¯", key=f"similar_yes_{st.session_state.similar_search_page}_{idx}_{row['pos_tid']}"):
                        try:
                            save_label_only(row['pos_tid'], "æ˜¯", "", 999)
                            st.session_state.similar_label_message = "å·²æ¨™è¨˜ç‚ºã€Œæ˜¯ã€"
                            st.session_state.similar_label_message_pos_tid = row['pos_tid']
                            st.rerun()
                        except Exception as e:
                            st.error(f"æ¨™è¨˜å¤±æ•—ï¼š{str(e)}")
                with col2:
                    if st.button("âŒ å¦", key=f"similar_no_{st.session_state.similar_search_page}_{idx}_{row['pos_tid']}"):
                        try:
                            save_label_only(row['pos_tid'], "å¦", "", 999)
                            st.session_state.similar_label_message = "å·²æ¨™è¨˜ç‚ºã€Œå¦ã€"
                            st.session_state.similar_label_message_pos_tid = row['pos_tid']
                            st.rerun()
                        except Exception as e:
                            st.error(f"æ¨™è¨˜å¤±æ•—ï¼š{str(e)}")
                with col3:
                    # æª¢æŸ¥ç•¶å‰æ¨™è¨˜ç‹€æ…‹
                    check_query = "SELECT label FROM candidates WHERE pos_tid = :pos_tid"
                    check_result = pd.read_sql(text(check_query), labeling_engine, params={"pos_tid": row['pos_tid']})
                    
                    if len(check_result) > 0:
                        current_label = check_result.iloc[0]['label']
                        if pd.notna(current_label) and current_label:
                            st.info(f"ç•¶å‰æ¨™è¨˜ï¼š{current_label}")
                
                # é¡¯ç¤ºæ¨™è¨˜è¨Šæ¯
                if (st.session_state.similar_label_message and 
                    st.session_state.similar_label_message_pos_tid == row['pos_tid']):
                    st.success(st.session_state.similar_label_message, 
                             icon="âœ…" if "æ˜¯" in st.session_state.similar_label_message else "âŒ")
                    st.session_state.similar_label_message = None
                    st.session_state.similar_label_message_pos_tid = None
        
        # åˆ†é å°èˆªæŒ‰éˆ•
        col1, col2, col3 = st.columns([1, 2, 1])
        with col1:
            if st.button("â¬…ï¸ ä¸Šä¸€é ", disabled=st.session_state.similar_search_page <= 0, key="similar_prev_page"):
                st.session_state.similar_search_page -= 1
                st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
                st.rerun()
        with col2:
            target_page = st.number_input(
                "å‰å¾€é ç¢¼",
                min_value=1,
                max_value=total_pages,
                value=st.session_state.similar_search_page + 1,
                label_visibility="collapsed",
                key="similar_page_input"
            )
            if target_page != st.session_state.similar_search_page + 1:
                st.session_state.similar_search_page = target_page - 1
                st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
                st.rerun()
        with col3:
            if st.button("ä¸‹ä¸€é  â¡ï¸", disabled=st.session_state.similar_search_page >= total_pages - 1, key="similar_next_page"):
                st.session_state.similar_search_page += 1
                st.markdown("<script>window.scrollTo(0, 0);</script>", unsafe_allow_html=True)
                st.rerun()

#======================================================================================

def cleanup_database_connections():
    """æ¸…ç†æ‰€æœ‰è³‡æ–™åº«é€£æ¥"""
    try:
        # æ¸…ç† session state ä¸­çš„å¼•æ“
        if 'labeling_engine' in st.session_state:
            st.session_state.labeling_engine.dispose()
            logger.info("å·²æ¸…ç† labeling_engine é€£æ¥")
        if 'source_engine' in st.session_state:
            st.session_state.source_engine.dispose()
            logger.info("å·²æ¸…ç† source_engine é€£æ¥")
            
        # æ¸…ç†å…¨åŸŸè®Šæ•¸ä¸­çš„å¼•æ“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'labeling_engine' in globals():
            labeling_engine.dispose()
            logger.info("å·²æ¸…ç†å…¨åŸŸ labeling_engine é€£æ¥")
        if 'source_engine' in globals():
            source_engine.dispose()
            logger.info("å·²æ¸…ç†å…¨åŸŸ source_engine é€£æ¥")
    except Exception as e:
        logger.warning(f"æ¸…ç†è³‡æ–™åº«é€£æ¥æ™‚ç™¼ç”Ÿè­¦å‘Š: {str(e)}")

# è¨»å†Šç¨‹å¼çµæŸæ™‚çš„æ¸…ç†å‡½æ•¸
atexit.register(cleanup_database_connections)

# æ–°å¢ç›¸ä¼¼è²¼æ–‡æœå°‹é€²ç¨‹é¡åˆ¥
class SimilarSearchProcess:
    """ç¨ç«‹çš„ç›¸ä¼¼è²¼æ–‡æœå°‹é€²ç¨‹ï¼Œé¿å… PyTorch èˆ‡ Streamlit è¡çª"""
    
    def __init__(self, embeddings_dir="embeddings_data", batch_size=32768):
        self.embeddings_dir = embeddings_dir
        self.batch_size = batch_size
        self.process = None
        self.result_queue = Queue()
        self.progress_queue = Queue()
        self.stop_event = mp.Event()
        
    def start_search(self, query_text, limit=20, threshold=0.7, random_search=False):
        """å•Ÿå‹•æœå°‹é€²ç¨‹"""
        # åœæ­¢ä¹‹å‰çš„é€²ç¨‹ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        self.stop_search()
        
        # é‡ç½®åœæ­¢äº‹ä»¶
        self.stop_event.clear()
        
        # å•Ÿå‹•æ–°é€²ç¨‹
        self.process = Process(
            target=self._search_worker,
            args=(
                query_text, limit, threshold, random_search,
                self.embeddings_dir, self.batch_size,
                self.result_queue, self.progress_queue, self.stop_event
            )
        )
        self.process.start()
        
    def stop_search(self):
        """åœæ­¢æœå°‹é€²ç¨‹"""
        if self.process and self.process.is_alive():
            self.stop_event.set()
            self.process.terminate()
            self.process.join(timeout=5)
            if self.process.is_alive():
                self.process.kill()
            self.process = None
            # æ¸…ç©ºéšŠåˆ—
            try:
                while not self.result_queue.empty():
                    self.result_queue.get_nowait()
                while not self.progress_queue.empty():
                    self.progress_queue.get_nowait()
            except:
                pass
        
    def get_progress(self):
        """ç²å–é€²åº¦è³‡è¨Š"""
        try:
            while not self.progress_queue.empty():
                progress = self.progress_queue.get_nowait()
                return progress
        except:
            pass
        return None
        
    def get_result(self):
        """ç²å–æœå°‹çµæœ"""
        try:
            if self.process and not self.process.is_alive():
                # é€²ç¨‹å·²å®Œæˆï¼Œç²å–çµæœ
                result = self.result_queue.get(timeout=1)
                self.process = None
                return result
        except:
            pass
        return None
        
    def is_running(self):
        """æª¢æŸ¥é€²ç¨‹æ˜¯å¦æ­£åœ¨é‹è¡Œ"""
        return self.process is not None and self.process.is_alive()
        
    @staticmethod
    def _search_worker(query_text, limit, threshold, random_search, 
                      embeddings_dir, batch_size, result_queue, progress_queue, stop_event):
        """æœå°‹å·¥ä½œé€²ç¨‹"""
        try:
            # åœ¨å­é€²ç¨‹ä¸­å°å…¥ PyTorch ç›¸é—œæ¨¡çµ„
            import numpy as np
            import json
            import os
            from sentence_transformers import SentenceTransformer, util
            from sqlalchemy import create_engine, text
            import pandas as pd
            import random
            from torch import tensor
            
            # åˆå§‹åŒ– detector
            detector = ScamDetectorMemmap(
                embeddings_dir=embeddings_dir,
                batch_size=batch_size
            )
            
            # åŸ·è¡Œæœå°‹
            results_df = detector.search_similar_posts(
                query_text=query_text,
                limit=limit,
                threshold=threshold,
                random_search=random_search,
                progress_callback=lambda progress: progress_queue.put(progress) if not stop_event.is_set() else None
            )
            
            # æª¢æŸ¥æ˜¯å¦è¢«åœæ­¢
            if stop_event.is_set():
                result_queue.put({'data': [], 'columns': []})
                return
            
            # æ¸…ç†è³‡æº
            detector.cleanup()
            
            # å°‡çµæœåºåˆ—åŒ–ä¸¦æ”¾å…¥éšŠåˆ—
            if not results_df.empty:
                # å°‡ DataFrame è½‰æ›ç‚ºå­—å…¸æ ¼å¼ä»¥ä¾¿åºåˆ—åŒ–
                result_dict = {
                    'data': results_df.to_dict('records'),
                    'columns': results_df.columns.tolist()
                }
                result_queue.put(result_dict)
            else:
                result_queue.put({'data': [], 'columns': []})
                
        except Exception as e:
            result_queue.put({'error': str(e)})
        finally:
            # ç¢ºä¿é€²ç¨‹çµæŸæ™‚æ¸…ç†è³‡æº
            try:
                import gc
                gc.collect()
            except:
                pass

if __name__ == '__main__':
    st.title("è©é¨™è²¼æ–‡äººå·¥æ¨™è¨˜å·¥å…·")
    
    # åˆå§‹åŒ– session state
    if 'current_tab' not in st.session_state:
        st.session_state.current_tab = "ğŸ“ æ¨™è¨˜æ¨¡å¼"
    if 'auto_switch_to_similar' not in st.session_state:
        st.session_state.auto_switch_to_similar = False
    
    # ä½¿ç”¨ selectbox ä¾†å¯¦ç¾åˆ†é åˆ‡æ›
    tab_options = ["ğŸ“ æ¨™è¨˜æ¨¡å¼", "ğŸ‘€ ç€è¦½æ¨¡å¼", "ğŸ”‘ é—œéµå­—æœå°‹", "ğŸ” ç›¸ä¼¼è²¼æ–‡æœå°‹"]
    selected_tab = st.selectbox(
        "é¸æ“‡åŠŸèƒ½åˆ†é ",
        tab_options,
        index=tab_options.index(st.session_state.current_tab),
        label_visibility="collapsed"
    )
    
    # æ›´æ–°ç•¶å‰åˆ†é 
    if selected_tab != st.session_state.current_tab:
        st.session_state.current_tab = selected_tab
        st.rerun()
    
    # æ ¹æ“šé¸æ“‡çš„åˆ†é é¡¯ç¤ºå°æ‡‰å…§å®¹
    if selected_tab == "ğŸ“ æ¨™è¨˜æ¨¡å¼":
        # å‹•æ…‹ç²å–ç¾¤çµ„ç·¨è™Ÿ
        group_ids = get_all_group_ids()
        group_id = st.selectbox("è«‹é¸æ“‡ä½ çš„ç¾¤çµ„ç·¨è™Ÿ (999æ˜¯é—œéµå­—æœå°‹çš„æ¨™è¨˜)", group_ids)
        
        # --- åˆå§‹åŒ– session state ---
        if 'label_index' not in st.session_state:
            st.session_state.label_index = 0
        if 'need_update' not in st.session_state:
            st.session_state.need_update = False
        if 'current_group' not in st.session_state:
            st.session_state.current_group = None
        
        # --- å–å¾—è³‡æ–™ ---
        df = get_current_data(group_id)
        
        # --- ç¢ºä¿æœ‰ label/note æ¬„ä½ ---
        with labeling_engine.begin() as conn:
            conn.execute(text("ALTER TABLE candidates ADD COLUMN IF NOT EXISTS label TEXT"))
            conn.execute(text("ALTER TABLE candidates ADD COLUMN IF NOT EXISTS note TEXT"))
        
        # --- å•Ÿå‹•æ¨™è¨˜ UI ---
        show_labeling_ui(group_id)
        
    elif selected_tab == "ğŸ‘€ ç€è¦½æ¨¡å¼":
        # ç€è¦½æ¨¡å¼çš„å­é ç±¤
        subtab1, subtab2, subtab3 = st.tabs(["ğŸ“± è©é¨™è²¼æ–‡ç€è¦½", "ğŸ“– è²¼æ–‡æŸ¥è©¢", "ğŸ“Š è©å½™åˆ†æ"])
        
        with subtab1:
            show_scam_posts_view()
        
        with subtab2:
            show_post_search()
            
        with subtab3:
            show_word_analysis()
    
    elif selected_tab == "ğŸ”‘ é—œéµå­—æœå°‹":
        show_keyword_search()
        
    elif selected_tab == "ğŸ” ç›¸ä¼¼è²¼æ–‡æœå°‹":
        # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•è·³è½‰åˆ°ç›¸ä¼¼æœå°‹
        if st.session_state.get('auto_switch_to_similar', False):
            st.session_state.auto_switch_to_similar = False
            st.success("âœ… å·²è‡ªå‹•è·³è½‰åˆ°ç›¸ä¼¼è²¼æ–‡æœå°‹é é¢")
            st.info("ğŸ’¡ æœå°‹æ–‡å­—å·²è‡ªå‹•å¡«å…¥ï¼Œç³»çµ±å°‡è‡ªå‹•åŸ·è¡Œæœå°‹")
        
        show_similar_posts_search()