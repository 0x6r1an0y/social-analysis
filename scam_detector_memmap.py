from sqlalchemy import create_engine, text
from sentence_transformers import SentenceTransformer, util
import pandas as pd
import numpy as np
import logging
import os
import json
import argparse
from typing import List, Dict, Optional
import time

# è¨­å®š logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ScamDetectorMemmap:
    def __init__(self, 
                 db_url: str = "postgresql+psycopg2://postgres:00000000@localhost:5432/social_media_analysis_hash",
                 model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
                 batch_size: int = 1000,
                 embeddings_dir: str = "embeddings_data"):
        """
        åˆå§‹åŒ–è©é¨™æª¢æ¸¬å™¨ (ä½¿ç”¨ memmap å­˜å„²)
        
        Args:
            db_url: è³‡æ–™åº«é€£æ¥å­—ä¸²
            model_name: ä½¿ç”¨çš„ sentence-transformers æ¨¡å‹
            batch_size: æ‰¹æ¬¡è™•ç†å¤§å°
            embeddings_dir: embeddings å­˜å„²ç›®éŒ„
        """
        self.db_url = db_url
        self.batch_size = batch_size
        self.embeddings_dir = embeddings_dir
        self.engine = None
        self.model = None
        
        # é è¨­è©é¨™æç¤ºè©
        self.default_scam_phrases = [
            "åŠ å…¥LINE", "åŠ å…¥Telegram", "å¿«é€Ÿè³ºéŒ¢", "è¢«å‹•æ”¶å…¥", 
            "æŠ•è³‡åŒ…ä½ è³º", "ç§è¨Šæˆ‘", "è€å¸«å¸¶å–®", "ç©©è³ºä¸è³ ",
            "è¼•é¬†è³ºéŒ¢", "ä¸€å¤©è³ºè¬å…ƒ", "ä¿è­‰ç²åˆ©", "é«˜å ±é…¬ä½é¢¨éšª",
            "åŠ ç¾¤çµ„", "è·Ÿå–®", "æ“ç›¤æ‰‹", "è²¡å¯Œè‡ªç”±",
            "æœˆæ”¶å…¥", "å…¼è·è³ºéŒ¢", "åœ¨å®¶è³ºéŒ¢", "ç¶²è·¯è³ºéŒ¢",
            "æŠ•è³‡ç†è²¡", "è™›æ“¬è²¨å¹£", "æ¯”ç‰¹å¹£", "æŒ–ç¤¦",
            "å€Ÿè²¸", "å°é¡è²¸æ¬¾", "æ€¥ç”¨éŒ¢", "å…æŠµæŠ¼",
            "ä»£è¾¦ä¿¡è²¸", "ä¿¡ç”¨å¡ä»£å„Ÿ", "å‚µå‹™æ•´åˆ"
        ]
        
        # æª”æ¡ˆè·¯å¾‘
        self.embeddings_file = os.path.join(embeddings_dir, "embeddings.dat")
        self.index_file = os.path.join(embeddings_dir, "pos_tid_index.json")
        self.metadata_file = os.path.join(embeddings_dir, "metadata.json")
        
        # åˆå§‹åŒ–
        self._init_db_connection()
        self._load_model(model_name)
        self._load_embeddings_metadata()
        
    def _init_db_connection(self):
        """åˆå§‹åŒ–è³‡æ–™åº«é€£æ¥"""
        try:
            self.engine = create_engine(self.db_url)
            logger.info("è³‡æ–™åº«é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {str(e)}")
            raise
            
    def _load_model(self, model_name: str):
        """è¼‰å…¥æ¨¡å‹"""
        logger.info(f"æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        logger.info("æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
    def _load_embeddings_metadata(self):
        """è¼‰å…¥ embeddings metadata"""
        try:
            # è¼‰å…¥ç´¢å¼•
            if not os.path.exists(self.index_file):
                raise FileNotFoundError(f"ç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨: {self.index_file}")
                
            with open(self.index_file, 'r', encoding='utf-8') as f:
                self.pos_tid_to_index = json.load(f)
                
            # è¼‰å…¥ metadata
            if not os.path.exists(self.metadata_file):
                raise FileNotFoundError(f"Metadata æª”æ¡ˆä¸å­˜åœ¨: {self.metadata_file}")
                
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
                
            self.embedding_dim = self.metadata['embedding_dim']
            self.total_embeddings = self.metadata['total_embeddings']
            
            logger.info(f"è¼‰å…¥ embeddings metadataï¼š")
            logger.info(f"  - ç¸½è¨˜éŒ„æ•¸: {self.total_embeddings}")
            logger.info(f"  - Embedding ç¶­åº¦: {self.embedding_dim}")
            logger.info(f"  - æœ€å¾Œæ›´æ–°: {self.metadata.get('last_updated', 'Unknown')}")
            
            # æª¢æŸ¥ embeddings æª”æ¡ˆ
            if not os.path.exists(self.embeddings_file):
                raise FileNotFoundError(f"Embeddings æª”æ¡ˆä¸å­˜åœ¨: {self.embeddings_file}")
                
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings metadata å¤±æ•—: {str(e)}")
            raise
            
    def _get_embeddings_array(self) -> np.ndarray:
        """ç²å– embeddings memmap é™£åˆ—"""
        try:
            # ç²å–è³‡æ–™åº«ç¸½è¨˜éŒ„æ•¸ä¾†ç¢ºå®šæ­£ç¢ºçš„ shape
            with self.engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT COUNT(*) 
                    FROM posts 
                    WHERE content IS NOT NULL 
                    AND content != ''
                """))
                total_records = result.scalar()
                
            embeddings_array = np.memmap(
                self.embeddings_file,
                dtype=np.float32,
                mode='r',
                shape=(total_records, self.embedding_dim)
            )
            
            return embeddings_array
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ embeddings é™£åˆ—å¤±æ•—: {str(e)}")
            raise
            
    def _get_posts_batch(self, offset: int = 0, limit: Optional[int] = None) -> pd.DataFrame:
        """ç²å–æ‰¹æ¬¡è²¼æ–‡è³‡æ–™ï¼ˆåŒ…å«æœ‰ embeddings çš„è²¼æ–‡ï¼‰"""
        try:
            if limit is None:
                limit = self.batch_size
                
            # åªç²å–æœ‰ embeddings çš„è²¼æ–‡
            valid_pos_tids = list(self.pos_tid_to_index.keys())
            
            if not valid_pos_tids:
                return pd.DataFrame()
                
            # åˆ†æ‰¹ç²å–
            start_idx = offset
            end_idx = min(offset + limit, len(valid_pos_tids))
            batch_pos_tids = valid_pos_tids[start_idx:end_idx]
            
            if not batch_pos_tids:
                return pd.DataFrame()
                
            # æ§‹å»º SQL æŸ¥è©¢
            placeholders = ','.join([f"'{pid}'" for pid in batch_pos_tids])
            sql = f"""
                SELECT pos_tid, content, page_name, created_time
                FROM posts 
                WHERE pos_tid IN ({placeholders})
                ORDER BY pos_tid
            """
            
            df = pd.read_sql_query(text(sql), self.engine)
            return df
            
        except Exception as e:
            logger.error(f"ç²å–è²¼æ–‡è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_embeddings_for_pos_tids(self, pos_tids: List[str]) -> Dict[str, np.ndarray]:
        """ç²å–æŒ‡å®š pos_tids çš„ embeddings"""
        try:
            embeddings_array = self._get_embeddings_array()
            result = {}
            
            for pos_tid in pos_tids:
                if pos_tid in self.pos_tid_to_index:
                    index = self.pos_tid_to_index[pos_tid]
                    result[pos_tid] = embeddings_array[index].copy()
                    
            return result
            
        except Exception as e:
            logger.error(f"ç²å– embeddings æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def calculate_scam_scores(self, 
                            pos_tids: List[str],
                            content_embeddings: Dict[str, np.ndarray],
                            scam_phrases: Optional[List[str]] = None) -> Dict[str, float]:
        """
        è¨ˆç®—è©é¨™åˆ†æ•¸
        
        Args:
            pos_tids: è²¼æ–‡ ID åˆ—è¡¨
            content_embeddings: è²¼æ–‡ embeddings å­—å…¸
            scam_phrases: è©é¨™æç¤ºè©åˆ—è¡¨
            
        Returns:
            è©é¨™é¢¨éšªåˆ†æ•¸å­—å…¸
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            scam_scores = {}
            
            for pos_tid in pos_tids:
                if pos_tid not in content_embeddings:
                    scam_scores[pos_tid] = 0.0
                    continue
                    
                content_emb = content_embeddings[pos_tid]
                
                # è½‰æ›ç‚º tensor ä¸¦è¨ˆç®—ç›¸ä¼¼åº¦
                content_tensor = util.pytorch_cos_sim(
                    self.model.encode("dummy", convert_to_tensor=True).unsqueeze(0),  # ç²å–æ­£ç¢ºçš„ tensor æ ¼å¼
                    phrase_embeddings
                )[0:1, :]  # ä¿æŒç¶­åº¦
                
                # ç›´æ¥ä½¿ç”¨é è¨ˆç®—çš„ embedding ä¾†è¨ˆç®—ç›¸ä¼¼åº¦
                from torch import tensor
                content_tensor = tensor(content_emb).unsqueeze(0)
                similarities = util.cos_sim(content_tensor, phrase_embeddings).squeeze().cpu().numpy()
                
                # å–æœ€å¤§ç›¸ä¼¼åº¦ä½œç‚ºè©é¨™åˆ†æ•¸
                score = float(np.max(similarities))
                scam_scores[pos_tid] = score
                
            return scam_scores
            
        except Exception as e:
            logger.error(f"è¨ˆç®—è©é¨™åˆ†æ•¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_scam_in_batch(self, 
                           scam_phrases: Optional[List[str]] = None,
                           threshold: float = 0.6,
                           output_file: Optional[str] = None,
                           max_results: Optional[int] = None) -> pd.DataFrame:
        """
        æ‰¹æ¬¡æª¢æ¸¬è©é¨™è²¼æ–‡
        
        Args:
            scam_phrases: è‡ªå®šç¾©è©é¨™æç¤ºè©
            threshold: è©é¨™é¢¨éšªé–¾å€¼
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            max_results: æœ€å¤§çµæœæ•¸é‡
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸çš„ DataFrame
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        logger.info(f"ä½¿ç”¨çš„è©é¨™æç¤ºè©: {scam_phrases}")
        logger.info(f"è©é¨™é¢¨éšªé–¾å€¼: {threshold}")
        
        try:
            total_posts = len(self.pos_tid_to_index)
            logger.info(f"å¾…æª¢æ¸¬çš„è²¼æ–‡ç¸½æ•¸: {total_posts}")
            
            all_results = []
            processed = 0
            offset = 0
            
            while processed < total_posts:
                start_time = time.time()
                
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_batch(offset)
                
                if df.empty:
                    break
                    
                logger.info(f"æ­£åœ¨è™•ç†ç¬¬ {processed + 1} åˆ° {processed + len(df)} ç­†è³‡æ–™...")
                
                # ç²å–é€™æ‰¹æ¬¡çš„ embeddings
                pos_tids = df['pos_tid'].tolist()
                embeddings_dict = self._get_embeddings_for_pos_tids(pos_tids)
                
                # è¨ˆç®—è©é¨™åˆ†æ•¸
                scam_scores_dict = self.calculate_scam_scores(
                    pos_tids,
                    embeddings_dict,
                    scam_phrases
                )
                
                # æ·»åŠ åˆ†æ•¸åˆ° DataFrame
                df['scam_score'] = df['pos_tid'].map(scam_scores_dict).fillna(0.0)
                df['is_potential_scam'] = df['scam_score'] >= threshold
                
                # åªä¿ç•™å¯èƒ½çš„è©é¨™è²¼æ–‡
                high_risk_posts = df[df['is_potential_scam']].copy()
                
                if not high_risk_posts.empty:
                    all_results.append(high_risk_posts)
                    
                processed += len(df)
                offset += len(df)
                
                # å¦‚æœé”åˆ°æœ€å¤§çµæœæ•¸é‡ï¼Œæå‰çµæŸ
                if max_results and sum(len(r) for r in all_results) >= max_results:
                    break
                    
                elapsed_time = time.time() - start_time
                logger.info(f"å·²è™•ç† {processed}/{total_posts} ç­†ï¼Œç™¼ç¾ {len(high_risk_posts)} ç­†å¯ç–‘è²¼æ–‡ - è€—æ™‚: {elapsed_time:.2f}ç§’")
                
            # åˆä½µæ‰€æœ‰çµæœ
            if all_results:
                final_results = pd.concat(all_results, ignore_index=True)
                final_results = final_results.sort_values('scam_score', ascending=False)
                
                if max_results:
                    final_results = final_results.head(max_results)
                    
                logger.info(f"ğŸ¯ æª¢æ¸¬å®Œæˆï¼å…±ç™¼ç¾ {len(final_results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡")
                
                # è¼¸å‡ºçµæœ
                if output_file:
                    final_results.to_csv(output_file, index=False, encoding='utf-8-sig')
                    logger.info(f"çµæœå·²å„²å­˜åˆ°: {output_file}")
                    
                return final_results
            else:
                logger.info("æ²’æœ‰ç™¼ç¾å¯ç–‘çš„è©é¨™è²¼æ–‡")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡æª¢æ¸¬æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def detect_single_post(self, content: str, scam_phrases: Optional[List[str]] = None) -> Dict:
        """
        æª¢æ¸¬å–®ä¸€è²¼æ–‡
        
        Args:
            content: è²¼æ–‡å…§å®¹
            scam_phrases: è©é¨™æç¤ºè©
            
        Returns:
            åŒ…å«è©é¨™åˆ†æ•¸å’Œè©³ç´°è³‡è¨Šçš„å­—å…¸
        """
        if scam_phrases is None:
            scam_phrases = self.default_scam_phrases
            
        try:
            # ç”Ÿæˆå…§å®¹ embedding
            content_embedding = self.model.encode(content, convert_to_tensor=True)
            
            # ç”Ÿæˆè©é¨™æç¤ºè©çš„ embeddings
            phrase_embeddings = self.model.encode(scam_phrases, convert_to_tensor=True)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarities = util.cos_sim(content_embedding, phrase_embeddings).squeeze().cpu().numpy()
            scam_score = float(np.max(similarities))
            
            # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª
            top_matches = []
            for i, phrase in enumerate(scam_phrases):
                top_matches.append({
                    'phrase': phrase,
                    'similarity': float(similarities[i])
                })
                
            top_matches.sort(key=lambda x: x['similarity'], reverse=True)
            
            result = {
                'content': content,
                'scam_score': scam_score,
                'is_potential_scam': scam_score >= 0.6,
                'top_matching_phrases': top_matches[:5],
                'risk_level': self._get_risk_level(scam_score)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"æª¢æ¸¬å–®ä¸€è²¼æ–‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def _get_risk_level(self, score: float) -> str:
        """æ ¹æ“šåˆ†æ•¸ç²å–é¢¨éšªç­‰ç´š"""
        if score >= 0.8:
            return "æ¥µé«˜é¢¨éšª"
        elif score >= 0.6:
            return "é«˜é¢¨éšª"
        elif score >= 0.4:
            return "ä¸­ç­‰é¢¨éšª"
        elif score >= 0.2:
            return "ä½é¢¨éšª"
        else:
            return "ç„¡é¢¨éšª"
            
    def search_by_keywords(self, 
                          keywords: List[str], 
                          limit: int = 100,
                          threshold: float = 0.5) -> pd.DataFrame:
        """
        ä½¿ç”¨é—œéµå­—é€²è¡Œèªæ„æœå°‹
        
        Args:
            keywords: æœå°‹é—œéµå­—
            limit: è¿”å›çµæœæ•¸é‡
            threshold: ç›¸ä¼¼åº¦é–¾å€¼
            
        Returns:
            æœå°‹çµæœ DataFrame
        """
        try:
            # ç”Ÿæˆé—œéµå­— embeddings
            keyword_embeddings = self.model.encode(keywords, convert_to_tensor=True)
            
            results = []
            processed = 0
            offset = 0
            
            while len(results) < limit and processed < len(self.pos_tid_to_index):
                # ç²å–æ‰¹æ¬¡è³‡æ–™
                df = self._get_posts_batch(offset, self.batch_size)
                
                if df.empty:
                    break
                    
                # ç²å– embeddings
                pos_tids = df['pos_tid'].tolist()
                embeddings_dict = self._get_embeddings_for_pos_tids(pos_tids)
                
                for _, row in df.iterrows():
                    pos_tid = row['pos_tid']
                    if pos_tid not in embeddings_dict:
                        continue
                        
                    # è¨ˆç®—èˆ‡é—œéµå­—çš„ç›¸ä¼¼åº¦
                    content_emb = embeddings_dict[pos_tid]
                    from torch import tensor
                    content_tensor = tensor(content_emb).unsqueeze(0)
                    similarities = util.cos_sim(content_tensor, keyword_embeddings).squeeze().cpu().numpy()
                    max_similarity = float(np.max(similarities))
                    
                    if max_similarity >= threshold:
                        result_row = row.copy()
                        result_row['similarity_score'] = max_similarity
                        result_row['matching_keyword'] = keywords[np.argmax(similarities)]
                        results.append(result_row)
                        
                        if len(results) >= limit:
                            break
                            
                processed += len(df)
                offset += len(df)
                
                logger.info(f"å·²è™•ç† {processed} ç­†ï¼Œæ‰¾åˆ° {len(results)} ç­†ç¬¦åˆçš„çµæœ")
                
            if results:
                results_df = pd.DataFrame(results)
                results_df = results_df.sort_values('similarity_score', ascending=False)
                return results_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"é—œéµå­—æœå°‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise
            
    def get_statistics(self):
        """ç²å–çµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_embeddings': len(self.pos_tid_to_index),
            'embedding_dimension': self.embedding_dim,
            'embeddings_file_size_mb': os.path.getsize(self.embeddings_file) / 1024 / 1024 if os.path.exists(self.embeddings_file) else 0,
            'model_name': self.metadata.get('model_name', 'Unknown'),
            'last_updated': self.metadata.get('last_updated', 'Unknown')
        }
        return stats

def main():
    """ä¸»è¦åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description='è©é¨™è²¼æ–‡æª¢æ¸¬å·¥å…· (Memmap ç‰ˆæœ¬)')
    parser.add_argument('--mode', choices=['batch', 'single', 'search'], default='batch',
                       help='åŸ·è¡Œæ¨¡å¼: batch=æ‰¹æ¬¡æª¢æ¸¬, single=å–®ä¸€è²¼æ–‡æª¢æ¸¬, search=é—œéµå­—æœå°‹')
    parser.add_argument('--content', type=str, help='å–®ä¸€è²¼æ–‡å…§å®¹ (single æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--keywords', nargs='+', help='æœå°‹é—œéµå­— (search æ¨¡å¼ä½¿ç”¨)')
    parser.add_argument('--threshold', type=float, default=0.6, help='è©é¨™é¢¨éšªé–¾å€¼')
    parser.add_argument('--output', type=str, help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--limit', type=int, default=1000, help='æœ€å¤§çµæœæ•¸é‡')
    parser.add_argument('--scam-phrases', nargs='+', help='è‡ªå®šç¾©è©é¨™æç¤ºè©')
    parser.add_argument('--embeddings-dir', type=str, default='embeddings_data', help='Embeddings å­˜å„²ç›®éŒ„')
    
    args = parser.parse_args()
    
    try:
        # å‰µå»ºæª¢æ¸¬å™¨
        detector = ScamDetectorMemmap(embeddings_dir=args.embeddings_dir)
        
        # é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        stats = detector.get_statistics()
        logger.info(f"ç³»çµ±çµ±è¨ˆ: {stats}")
        
        if args.mode == 'batch':
            # æ‰¹æ¬¡æª¢æ¸¬
            results = detector.detect_scam_in_batch(
                scam_phrases=args.scam_phrases,
                threshold=args.threshold,
                output_file=args.output,
                max_results=args.limit
            )
            
            if not results.empty:
                print(f"\nğŸ¯ ç™¼ç¾ {len(results)} ç­†å¯ç–‘è©é¨™è²¼æ–‡:")
                print(results[['pos_tid', 'page_name', 'scam_score', 'content']].head(10).to_string())
                
        elif args.mode == 'single':
            if not args.content:
                print("âŒ è«‹æä¾›è²¼æ–‡å…§å®¹ (--content)")
                return
                
            # å–®ä¸€è²¼æ–‡æª¢æ¸¬
            result = detector.detect_single_post(args.content, args.scam_phrases)
            
            print(f"\nğŸ“ è²¼æ–‡å…§å®¹: {result['content']}")
            print(f"ğŸ¯ è©é¨™åˆ†æ•¸: {result['scam_score']:.3f}")
            print(f"âš ï¸  é¢¨éšªç­‰ç´š: {result['risk_level']}")
            print(f"ğŸš¨ æ˜¯å¦å¯ç–‘: {'æ˜¯' if result['is_potential_scam'] else 'å¦'}")
            print("\nğŸ” æœ€ç›¸ä¼¼çš„è©é¨™çŸ­èª:")
            for match in result['top_matching_phrases'][:3]:
                print(f"  - {match['phrase']}: {match['similarity']:.3f}")
                
        elif args.mode == 'search':
            if not args.keywords:
                print("âŒ è«‹æä¾›æœå°‹é—œéµå­— (--keywords)")
                return
                
            # é—œéµå­—æœå°‹
            results = detector.search_by_keywords(
                keywords=args.keywords,
                limit=args.limit,
                threshold=args.threshold
            )
            
            if not results.empty:
                print(f"\nğŸ” æ‰¾åˆ° {len(results)} ç­†ç›¸é—œè²¼æ–‡:")
                print(results[['pos_tid', 'page_name', 'similarity_score', 'matching_keyword', 'content']].head(10).to_string())
                
                if args.output:
                    results.to_csv(args.output, index=False, encoding='utf-8-sig')
                    print(f"çµæœå·²å„²å­˜åˆ°: {args.output}")
            else:
                print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œè²¼æ–‡")
                
    except Exception as e:
        logger.error(f"ç¨‹å¼åŸ·è¡Œå¤±æ•—: {str(e)}")
        raise

if __name__ == "__main__":
    main() 