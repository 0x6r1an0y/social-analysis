import pandas as pd
from sqlalchemy import create_engine, text
import time
import psycopg2
import os
import sys

DB_TYPE = "postgresql"
# --- 1. è¨­å®šè³‡æ–™åº«é€£æ¥åƒæ•¸ ---
# PostgreSQL ç¯„ä¾‹
DB_USER = "postgres"        # ä½ çš„è³‡æ–™åº«ä½¿ç”¨è€…åç¨±
DB_PASSWORD = "00000000"    # ä½ çš„è³‡æ–™åº«å¯†ç¢¼
DB_HOST = "localhost"            # è³‡æ–™åº«ä¸»æ©Ÿ (è‹¥æ˜¯æœ¬æ©Ÿé€šå¸¸æ˜¯ localhost)
DB_PORT = "5432"                 # è³‡æ–™åº«åŸ è™Ÿ (PostgreSQL é è¨­ 5432)
DB_NAME = "social_media_analysis" # ä½ å‰µå»ºçš„è³‡æ–™åº«åç¨±
TABLE_NAME = "posts"             # ä½ è¦å‰µå»ºçš„è³‡æ–™è¡¨åç¨±

# --- 2. è¨­å®š CSV æª”æ¡ˆè·¯å¾‘å’Œåˆ†å¡Šå¤§å° ---
CSV_FILE_PATH = "merged_output.csv"
CHUNK_SIZE = 200000  # æ¯æ¬¡è™•ç†çš„è¡Œæ•¸ï¼Œå¯æ ¹æ“šä½ çš„è¨˜æ†¶é«”å¤§å°èª¿æ•´ (ä¾‹å¦‚ 10,000 åˆ° 100,000)

# --- 3. æª¢æŸ¥ä¸¦å‰µå»ºè³‡æ–™åº« ---
try:
    # å…ˆé€£æ¥åˆ°é è¨­çš„ postgres è³‡æ–™åº«
    conn = psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database="postgres"  # é€£æ¥åˆ°é è¨­è³‡æ–™åº«
    )
    conn.autocommit = True  # è‡ªå‹•æäº¤ï¼Œé€™æ¨£å‰µå»ºè³‡æ–™åº«çš„æŒ‡ä»¤æ‰æœƒç”Ÿæ•ˆ
    cursor = conn.cursor()
    
    # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨
    cursor.execute(f"SELECT 1 FROM pg_database WHERE datname = '{DB_NAME}'")
    exists = cursor.fetchone()
    
    if not exists:
        print(f"è³‡æ–™åº« '{DB_NAME}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º...")
        cursor.execute(f"CREATE DATABASE {DB_NAME}")
        print(f"è³‡æ–™åº« '{DB_NAME}' å‰µå»ºæˆåŠŸï¼")
    else:
        print(f"è³‡æ–™åº« '{DB_NAME}' å·²å­˜åœ¨ã€‚")
    
    cursor.close()
    conn.close()
except Exception as e:
    print(f"æª¢æŸ¥/å‰µå»ºè³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    print("\nå¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
    print("1. ç¢ºèª PostgreSQL æœå‹™æ˜¯å¦å·²å®‰è£ä¸¦é‹è¡Œ")
    print("2. ç¢ºèªè³‡æ–™åº«ä½¿ç”¨è€…åç¨±å’Œå¯†ç¢¼æ˜¯å¦æ­£ç¢º")
    print("3. ç¢ºèª PostgreSQL æ˜¯å¦æ­£åœ¨ç›£è½ 5432 åŸ ")
    print("\nå¦‚éœ€å®‰è£ PostgreSQLï¼Œè«‹å‰å¾€: https://www.postgresql.org/download/windows/")
    sys.exit(1)

# --- 4. å‰µå»º SQLAlchemy å¼•æ“ ---
try:
    engine_url = f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    engine = create_engine(engine_url)
    
    # æ¸¬è©¦é€£æ¥
    with engine.connect() as connection:
        connection.execute(text("SELECT 1"))
    print("æˆåŠŸé€£æ¥åˆ° PostgreSQL è³‡æ–™åº«ï¼")
except Exception as e:
    print(f"é€£æ¥è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    sys.exit(1)

# --- 5. å‰µå»ºè³‡æ–™è¡¨çµæ§‹ ---
# å¦‚æœè³‡æ–™è¡¨å·²å­˜åœ¨ï¼Œé€™æ®µå¯ä»¥è·³éæˆ–ä¿®æ”¹
# æ³¨æ„ï¼šæ¬„ä½é¡å‹éœ€è¦æ ¹æ“šä½ çš„ CSV å…§å®¹ç²¾ç¢ºå®šç¾©
create_table_sql = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
    pos_tid VARCHAR(255) PRIMARY KEY,
    post_type VARCHAR(255),
    page_category TEXT,
    page_name TEXT,
    page_id VARCHAR(255),
    content TEXT,
    created_time BIGINT,
    reaction_all INTEGER,
    comment_count INTEGER,
    share_count INTEGER,
    date DATE
);
"""
# åŸ·è¡Œå‰µå»ºè³‡æ–™è¡¨çš„ SQL
try:
    with engine.connect() as connection:
        connection.execute(text(create_table_sql))
        connection.commit() # å° DDL æ“ä½œï¼ˆå¦‚ CREATE TABLEï¼‰é€²è¡Œ commit
    print(f"è³‡æ–™è¡¨ '{TABLE_NAME}' æª¢æŸ¥/å‰µå»ºæˆåŠŸã€‚")
except Exception as e:
    print(f"å‰µå»ºè³‡æ–™è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    print("è«‹æª¢æŸ¥è³‡æ–™åº«é€£æ¥è¨­å®šæˆ–è¯çµ¡ç®¡ç†å“¡ã€‚")
    sys.exit(1)


# --- 6. åˆ†å¡Šè®€å– CSV ä¸¦å¯«å…¥è³‡æ–™åº« ---
start_time = time.time()
total_rows_processed = 0

print(f"é–‹å§‹å¾ '{CSV_FILE_PATH}' åŒ¯å…¥è³‡æ–™åˆ°è³‡æ–™è¡¨ '{TABLE_NAME}'...")

try:
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(CSV_FILE_PATH):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° CSV æª”æ¡ˆ '{CSV_FILE_PATH}'ã€‚")
        sys.exit(1)

    # ç²å– CSV æª”æ¡ˆçš„è¡¨é ­ï¼Œä»¥ç¢ºä¿ to_sql æ™‚æ¬„ä½åç¨±æ­£ç¢º
    # å¦‚æœCSVæ²’æœ‰è¡¨é ­ï¼Œæˆ–è€…ä½ æƒ³æ‰‹å‹•æŒ‡å®šï¼Œå¯ä»¥èª¿æ•´
    # header_df = pd.read_csv(CSV_FILE_PATH, nrows=0)
    # column_names = header_df.columns.tolist()
    # print(f"CSV æ¬„ä½: {column_names}")

    for i, chunk in enumerate(pd.read_csv(CSV_FILE_PATH, chunksize=CHUNK_SIZE, low_memory=False)):
        chunk_start_time = time.time()
        print(f"æ­£åœ¨è™•ç†ç¬¬ {i+1} å€‹å€å¡Š...")

        # (å¯é¸) è³‡æ–™æ¸…ç†æˆ–è½‰æ›
        # ä¾‹å¦‚ï¼Œå¦‚æœ 'date' æ¬„ä½æ˜¯å­—ä¸²ï¼Œéœ€è¦è½‰æ›æˆ datetime ç‰©ä»¶
        if 'date' in chunk.columns:
            try:
                chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce')
            except Exception as e:
                print(f"è½‰æ›æ—¥æœŸæ¬„ä½æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                print("ç¹¼çºŒè™•ç†ï¼Œä½†æ—¥æœŸæ¬„ä½å¯èƒ½ç„¡æ³•æ­£ç¢ºè½‰æ›...")
        
        # ç¢ºä¿ created_time æ˜¯æ•´æ•¸
        if 'created_time' in chunk.columns and not pd.api.types.is_numeric_dtype(chunk['created_time']):
            try:
                chunk['created_time'] = pd.to_numeric(chunk['created_time'], errors='coerce').fillna(0).astype(int)
            except Exception as e:
                print(f"è½‰æ› created_time æ¬„ä½æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                print("ç¹¼çºŒè™•ç†ï¼Œä½† created_time æ¬„ä½å¯èƒ½ç„¡æ³•æ­£ç¢ºè½‰æ›...")

        # å°‡æ•¸æ“šå¯«å…¥ SQL è³‡æ–™åº«
        try:
            chunk.to_sql(TABLE_NAME, engine, if_exists='append', index=False, method='multi')
            total_rows_processed += len(chunk)
            chunk_time_taken = time.time() - chunk_start_time
            print(f"ç¬¬ {i+1} å€‹å€å¡Š ({len(chunk)} ç­†è³‡æ–™) å·²è™•ç†ä¸¦æ’å…¥ï¼Œè€—æ™‚ {chunk_time_taken:.2f} ç§’ã€‚")
            print(f"ç›®å‰å·²è™•ç†ç¸½è³‡æ–™ç­†æ•¸: {total_rows_processed}")

        except Exception as e:
            print(f"âš ï¸ å°‡ç¬¬ {i+1} å€‹å€å¡Šæ’å…¥è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            print("â¡ï¸ å˜—è©¦é€åˆ—åµéŒ¯ä»¥æ‰¾å‡ºç•°å¸¸è³‡æ–™...")

            for row_idx, row in chunk.iterrows():
                try:
                    row_df = pd.DataFrame([row])
                    row_df.to_sql(TABLE_NAME, engine, if_exists='append', index=False)
                except Exception as row_err:
                    print(f"âŒ ç¬¬ {i+1} å€å¡Šä¸­ç¬¬ {row_idx} åˆ—å¯«å…¥éŒ¯èª¤: {row_err}")
                    print("ğŸ” è©²ç­†è³‡æ–™å¦‚ä¸‹ï¼š")
                    print(row)
                    print("ğŸ”¬ å˜—è©¦é€æ¬„ä½åµéŒ¯ï¼š")
                    for col in row.index:
                        try:
                            test_df = pd.DataFrame([{col: row[col]}])
                            test_df.to_sql(TABLE_NAME, engine, if_exists='append', index=False)
                        except Exception as col_err:
                            print(f"    ğŸ”´ æ¬„ä½ '{col}' ç™¼ç”ŸéŒ¯èª¤: {col_err}")
                    print("-" * 60)

            print("â¡ï¸ å·²å®Œæˆè©²å€å¡Šé€åˆ—åˆ†æï¼Œç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹å€å¡Š...")
            continue

    end_time = time.time()
    print(f"æˆåŠŸåŒ¯å…¥ {total_rows_processed} ç­†è³‡æ–™åˆ° '{TABLE_NAME}'ã€‚")
    print(f"ç¸½è€—æ™‚: {(end_time - start_time):.2f} ç§’ã€‚")

except FileNotFoundError:
    print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ '{CSV_FILE_PATH}'ã€‚")
    sys.exit(1)
except Exception as e:
    print(f"ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
    sys.exit(1)